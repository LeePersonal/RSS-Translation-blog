<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[Huggingface Daily Papers]]></title><link/> https://huggingface.co/papers <atom:link href="https://rsshub-yuee.vercel.app/huggingface/daily-papers?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh-cn</language><lastbuilddate> 2023 年 8 月 15 日，星期二 04:56:00 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[Composable Function-preserving Expansions for Transformer Architectures]]></title><description><![CDATA[Training state-of-the-art neural networks requires a high cost in terms of compute and time. Model scale is recognized to be a critical factor to achieve and improve the state-of-the-art. Increasing the scale of a neural network normally requires restarting from scratch by randomly initializing all the parameters of the model, as this implies a change of architecture's parameters that does not allow for a straightforward transfer of knowledge from smaller size models. In this work, we propose six composable transformations to incrementally increase the size of transformer-based neural networks while preserving functionality, allowing to expand the capacity of the model as needed. We provide proof of exact function preservation under minimal initialization constraints for each transformation. The proposed methods may enable efficient training pipelines for larger and more powerful models by progressively expanding the architecture throughout training.]]></description><pubDate> Mon, 14 Aug 2023 05:13:56 GMT</pubDate><guid ispermalink="false"> https://arxiv.org/abs/2</guid><link/> https://arxiv.org/abs/2 <author><![CDATA[Andrea Gesmundo, Kaitlin Maile]]></author></item><item><title><![CDATA[Improving Joint Speech-Text Representations Without Alignment]]></title><description><![CDATA[The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.]]></description><pubDate> Mon, 14 Aug 2023 04:44:38 GMT</pubDate><guid ispermalink="false"> https://www.archiv.org/abs/2308.06125</guid><link/> https://www.archiv.org/abs/2308.06125 <author><![CDATA[Cal Peyser, Zhong Meng, Ke Hu, Rohit Prabhavalkar, Andrew Rosenberg, Tara N. Sainath, Michael Picheny, Kyunghyun Cho]]></author></item><item><title><![CDATA[Enhancing Network Management Using Code Generated by Large Language Models]]></title><description><![CDATA[Analyzing network topologies and communication graphs plays a crucial role in contemporary network management. However, the absence of a cohesive approach leads to a challenging learning curve, heightened errors, and inefficiencies. In this paper, we introduce a novel approach to facilitate a natural-language-based network management experience, utilizing large language models (LLMs) to generate task-specific code from natural language queries. This method tackles the challenges of explainability, scalability, and privacy by allowing network operators to inspect the generated code, eliminating the need to share network data with LLMs, and concentrating on application-specific requests combined with general program synthesis techniques. We design and evaluate a prototype system using benchmark applications, showcasing high accuracy, cost-effectiveness, and the potential for further enhancements using complementary program synthesis techniques.]]></description><pubDate> Mon, 14 Aug 2023 04:37:43 GMT</pubDate><guid ispermalink="false"> https://www.archiv.org/abs/2308.06261</guid><link/> https://www.archiv.org/abs/2308.06261 <author><![CDATA[Sathiya Kumaran Mani, Yajie Zhou, Kevin Hsieh, Santiago Segarra, Ranveer Chandra, Srikanth Kandula]]></author></item></channel></rss>