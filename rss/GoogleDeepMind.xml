<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[DeepMind Blog]]></title><link/> https://www.deepmind.com/blog <atom:link href="https://rsshub-yuee.vercel.app/deepmind/blog?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[Read the latest articles and stories from DeepMind and find out more about our latest breakthroughs in cutting-edge AI research. - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh</language><image/><url> https://assets-global.website-files.com/621d30e84caf0be3291dbf1c/621d336835a91420c6a8dcf2_webclip.png</url><title><![CDATA[DeepMind Blog]]></title><link/> https://www.deepmind.com/blog<lastbuilddate> 2023 年 8 月 17 日星期四 02:03:18 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[RT-2: New model translates vision and language into action]]></title><description><![CDATA[<p> <strong>Robotic Transformer 2 (RT-2) 是一种新颖的视觉-语言-动作 (VLA) 模型，可以从网络和机器人数据中学习，并将这些知识转化为机器人控制的通用指令。</strong></p><p>高容量视觉语言模型（VLM）在网络规模的数据集上进行训练，使这些系统非常擅长识别视觉或语言模式并跨不同语言进行操作。但要让机器人达到类似的能力水平，他们需要收集每个物体、环境、任务和情况的第一手机器人数据。</p><p>在我们的<a href="https://robotics-transformer2.github.io/assets/rt2.pdf" target="_blank">论文</a>中，我们介绍了 Robotic Transformer 2 (RT-2)，这是一种新颖的视觉-语言-动作 (VLA) 模型，可以从网络和机器人数据中学习，并将这些知识转化为机器人控制的通用指令，同时保留网络-规模能力。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb04fd16049ea01b833_64c1e428094c3322c3991c90_RT-2%2520gif_1.gif" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption>在网络规模数据上预训练的视觉语言模型 (VLM) 正在从 RT-1 机器人数据中学习，成为 RT-2，这是一种可以控制机器人的视觉语言动作 (VLA) 模型。</figcaption></figure><p>这项工作建立在 Robotic Transformer 1<a href="https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html" target="_blank">(RT-1)</a>的基础上，这是一个经过多任务演示训练的模型，它可以学习机器人数据中看到的任务和对象的组合。更具体地说，我们的工作使用了 RT-1 机器人演示数据，这些数据是在办公室厨房环境中使用 13 个机器人在 17 个月内收集的。</p><p> RT-2 显示出超越其所接触的机器人数据的泛化能力以及语义和视觉理解能力。这包括解释新命令并通过执行基本推理（例如关于对象类别或高级描述的推理）来响应用户命令。</p><p>我们还表明，结合思想链推理允许 RT-2 执行多阶段语义推理，例如决定哪个物体可以用作临时锤子（石头），或者哪种类型的饮料最适合疲倦的人（一种能量饮料）。</p><h4>采用 VLM 进行机器人控制</h4><p>RT-2 以 VLM 为基础，将一个或多个图像作为输入，并生成一系列通常代表自然语言文本的标记。此类 VLM 已<a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" target="_blank">成功接受网络规模数据的训练</a>，以执行视觉问答、图像字幕或对象识别等任务。在我们的工作中，我们采用 Pathways Language and Image model ( <a href="https://ai.googleblog.com/2022/09/pali-scaling-language-image-learning-in.html" target="_blank">PaLI-X</a> ) 和 Pathways Language model Embodied ( <a href="https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html" target="_blank">PaLM-E</a> ) 作为 RT-2 的支柱。</p><p>为了控制机器人，必须训练它输出动作。我们通过在模型输出中将操作表示为标记（类似于语言标记）来解决这一挑战，并将操作描述为可以由标准<a href="https://github.com/google/sentencepiece" target="_blank">自然语言标记生成器</a>处理的字符串，如下所示： </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb04858ef9cbf04ccfe_64c25414d77b82ddfede6c9f_Fig%25202.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption> RT-2 训练中使用的动作字符串的表示形式。这种字符串的示例可以是机器人动作标记编号的序列，例如“1 128 91 241 5 101 127 217”。</figcaption></figure><p>该字符串以一个标志开头，指示是继续还是终止当前情节，而不执行后续命令，然后是更改末端执行器的位置和旋转以及机器人夹具所需延伸的命令。</p><p>我们使用与 RT-1 中相同的机器人动作离散版本，并表明将其转换为字符串表示使得可以在机器人数据上训练 VLM 模型 - 因为此类模型的输入和输出空间不需要改变了。</p><p> ‍ </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb0cdbb5f4e62c3615a_64c2549e5be66bd1bd2f7b32_Fig%25201.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption> RT-2 架构和训练：我们针对机器人和网络数据共同微调预先训练的 VLM 模型。生成的模型接收机器人摄像头图像并直接预测机器人要执行的动作。</figcaption></figure><h4>泛化和突发技能</h4><p>我们在 RT-2 模型上进行了一系列定性和定量实验，进行了 6,000 多次机器人试验。在探索 RT-2 的新兴功能时，我们首先搜索需要将网络规模数据的知识与机器人的经验相结合的任务，然后定义三类技能：符号理解、推理和人类识别。</p><p>每项任务都需要理解视觉语义概念以及执行机器人控制以操作这些概念的能力。需要诸如“捡起即将从桌子上掉下来的袋子”或“将香蕉移动到二加一之和”之类的命令，其中要求机器人对机器人数据中从未见过的物体或场景执行操作任务将知识从基于网络的数据转化为可操作的。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb0c2c8236e724dc720_64c2873d3b6f7032a03db6c5_Fig%25203.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption>机器人数据中不存在的新兴机器人技能示例，需要通过网络预训练进行知识转移。</figcaption></figure><p>在所有类别中，我们观察到与之前的基线（例如之前的 RT-1 模型和 Visual Cortex ( <a href="https://eai-vc.github.io/" target="_blank">VC-1</a> ) 等模型）相比，泛化性能有所提高（提高了 3 倍以上），这些模型是在大型视觉数据集上进行预训练的。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb0722d137a039de352_64c255b5541f70c5a255c414_Fig%25204.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption>紧急技能评估的成功率：我们的 RT-2 模型优于之前的机器人变压器 (RT-1) 和视觉预训练 (VC-1) 基线。</figcaption></figure><p>我们还进行了一系列定量评估，从最初的 RT-1 任务开始，我们在机器人数据中提供了示例，然后继续对机器人进行不同程度的以前未见过的物体、背景和环境，要求机器人从 VLM 预训练中学习泛化能力。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb00ad422de67d4d87a_64c2563fef42896c99f648db_Fig%25205.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption>机器人以前未见过的环境示例，其中 RT-2 可以推广到新的情况。</figcaption></figure><p> RT-2 保留了机器人数据中看到的原始任务的性能，并提高了机器人在以前未见过的场景上的性能，从 RT-1 的 32% 提高到了 62%，显示了大规模预训练的巨大好处。</p><p>此外，我们观察到与仅视觉任务预训练的基线相比有显着改进，例如 VC-1 和机器人操作的可重用表示 ( <a href="https://sites.google.com/corp/view/robot-r3m/" target="_blank">R3M</a> )，以及使用 VLM 进行对象识别的算法，例如开放世界对象的操作 (<a href="https://robot-moo.github.io/" target="_blank">哞哞</a>）。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb0a7e46157ca3edb96_64c2566864b91d6bd0590f41_Fig%25206.svg" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption> RT-2 在可见的分布内任务上实现了高性能，并且在分布外未见的任务上优于多个基线。</figcaption></figure><p>在机器人任务的开源<a href="https://github.com/google-research/language-table" target="_blank">语言表</a>套件上评估我们的模型，我们在模拟中实现了 90% 的成功率，比之前的基线（包括<a href="https://sites.google.com/corp/view/bc-z/home" target="_blank">BC-Z</a> (72%)、<a href="https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html" target="_blank">RT-1</a> (74%) 和<a href="https://interactive-language.github.io/" target="_blank">熔岩</a>(77%)。</p><p>然后我们在现实世界中评估相同的模型（因为它是在模拟和真实数据上进行训练的），并展示了其泛化到新对象的能力，如下所示，其中除了蓝色立方体之外，没有任何对象出现在训练中数据集。 </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb1c73e93977fcc3ada_64c256a1aadd37257efbe4cd_Fig%25207.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption> RT-2 在真实机器人语言表任务中表现良好。除了蓝色立方体之外，训练数据中不存在任何对象。</figcaption></figure><p>受到<a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html" target="_blank">法学硕士中使用的思维链提示方法的</a>启发，我们探索了我们的模型，将机器人控制与思维链推理相结合，以便能够在单个模型中学习长期规划和低级技能。</p><p>特别是，我们对 RT-2 的变体进行了几百个梯度步骤的微调，以提高其联合使用语言和动作的能力。然后，我们对数据进行了扩充，添加了一个额外的“计划”步骤，首先用自然语言描述机器人即将采取的动作的目的，然后是“动作”和动作标记。在这里，我们展示了这种推理和机器人的最终行为的示例： </p><figure class="w-richtext-figure-type-image w-richtext-align-center"><div><img src="https://assets-global.website-files.com/621e749a546b7592125f38ed/64c28bb1d047fa442a8c3acb_64c256fd9390fd8ed049a2d3_Fig%25208.png" loading="lazy" alt="" referrerpolicy="no-referrer"></div><figcaption>思想链推理可以学习一个独立的模型，该模型既可以规划长期技能序列，又可以预测机器人的动作。</figcaption></figure><p>通过此过程，RT-2 可以执行更多复杂的命令，这些命令需要推理完成用户指令所需的中间步骤。得益于其 VLM 主干，RT-2 还可以根据图像和文本命令进行规划，从而实现基于视觉的规划，而当前的计划和行动方法（如<a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html" target="_blank">SayCan）</a>无法看到现实世界并完全依赖于语言。</p><h4>推进机器人控制</h4><p>RT-2表明视觉语言模型（VLM）可以转化为强大的视觉语言动作（VLA）模型，通过将VLM预训练与机器人数据相结合，可以直接控制机器人。</p><p>通过基于 PaLM-E 和 PaLI-X 的 VLA 的两个实例化，RT-2 带来了高度改进的机器人策略，更重要的是，带来了显着更好的泛化性能和突发能力，继承自网络规模的视觉语言预-训练。</p><p> RT-2 不仅是对现有 VLM 模型的简单而有效的修改，而且还展示了构建通用物理机器人的前景，该机器人可以推理、解决问题和解释信息，以在现实中执行各种任务。世界。</p><p> ‍ </p><div class="w-embed"><div class="article-gtag-buttons"><svg class="rounded-full bg-yellow-500 fill-current" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.3536 6.64648L17.3536 11.6465C17.5488 11.8417 17.5488 12.1583 17.3536 12.3536L12.3536 17.3536L11.6464 16.6465L15.7929 12.5H0V11.5H15.7929L11.6464 7.35359L12.3536 6.64648Z"></path></svg></div><div>阅读我们的论文</div></div><style type="text/css">
.w-embed { width: 100%; !imporrtant }
</style></div><div class="w-embed"><div class="article-gtag-buttons"><svg class="rounded-full bg-yellow-500 fill-current" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.3536 6.64648L17.3536 11.6465C17.5488 11.8417 17.5488 12.1583 17.3536 12.3536L12.3536 17.3536L11.6464 16.6465L15.7929 12.5H0V11.5H15.7929L11.6464 7.35359L12.3536 6.64648Z"></path></svg></div><div>了解有关关键字的更多信息</div></div><style type="text/css">
.w-embed { width: 100%; !imporrtant }
</style></div>]]>;</description><pubDate> Fri, 28 Jul 2023 00:00:00 GMT</pubDate><guid ispermalink="false"> https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action</guid><link/> https://www.deepmind.com/blog/rt-2-new-model-translates-vision-and-language-into-action </item><item><title><![CDATA[Using AI to fight climate change]]></title><description><![CDATA[<h4>我们如何应用最新的人工智能发展来帮助应对气候变化并建设一个更加可持续的低碳世界</h4><p>人工智能是一项强大的技术，将改变我们的未来，那么我们如何才能最好地应用它来帮助应对气候变化并找到可持续的解决方案呢？</p><p>我们的气候与可持续发展主管 Sims Witherspoon 最近在<a href="https://countdown.ted.com/" target="_blank">TED Countdown</a>上谈到了人工智能如何加速我们向可再生能源的过渡，他解释说：“气候变化是一个多方面的问题，没有单一的解决方案。我们需要超越讨论我们能做<em>什么</em>，而开始关注我们<em>如何</em>做到这一点。”</p><p>气候变化对地球生态系统的影响极其复杂，作为我们利用人工智能解决世界上一些最具挑战性问题的努力的一部分，以下是我们正在努力增进理解、优化现有系统的一些方法，并加速气候及其影响科学的突破。</p><h4>了解天气、气候及其影响</h4><p>更好地了解核心问题及其影响是应对气候变化的关键第一步。我们与英国气象局合作，开发了<a href="https://www.deepmind.com/blog/nowcasting-the-next-hour-of-rain" target="_blank">降水临近预报模型</a>，以更好地了解不断变化的天气。这种临近预报模型比现有技术更准确，并且受到大都会气象专家的青睐。我们的气候和天气研究涵盖短期（两小时内）到中期（十天）的预测，这可以极大地影响我们如何优化基于自然资源的可再生能源系统。</p><p>从对塞伦盖蒂<a href="https://www.deepmind.com/blog/using-machine-learning-to-accelerate-ecological-research" target="_blank">动物物种的行为进行建模</a>，到支持<a href="https://www.deepmind.com/blog/advancing-conservation-with-ai-based-facial-recognition-of-turtles" target="_blank">推动非洲保护项目</a>的机器学习项目，我们一直在帮助科学家跟踪和更好地了解气候变化对生态系统和生物多样性的影响。展望未来，我们的团队还将基于用于 <a href="https://blog.google/intl/en-au/company-news/technology/ai-ecoacoustics/" target="_blank">识别澳大利亚鸟鸣的</a>人工智能系统，帮助改进大规模监测野生动物变化的工具。</p><p>此外，我们正在与非营利组织<a href="https://www.climatechange.ai/" target="_blank">气候变化人工智能</a>合作，以缩小气候相关数据的重要差距。目前，这一合作伙伴关系的重点是建立一个全面的数据集愿望清单，其可用性将推动气候变化的人工智能解决方案。完成后，我们将向更广泛的公众提供此愿望清单。</p><h4>优化现有系统</h4><p>在我们转向更可持续的基础设施的同时，我们需要优化当今世界所依赖的系统。例如，当今的计算基础设施，包括人工智能本身，都是能源密集型的。为了帮助解决其中一些问题，我们一直在开发可以增强现有系统的人工智能，包括<a href="https://www.deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40" target="_blank">优化工业冷却</a>和<a href="https://www.deepmind.com/blog/optimising-computer-systems-with-more-generalised-ai-tools" target="_blank">更高效的计算机系统</a>。</p><p>鉴于我们的能源网尚未依靠清洁能源运行，因此在努力向可再生能源过渡的同时，尽可能有效地利用我们的资源非常重要。加速全球向可再生能源的过渡也可以大大减少碳排放。</p><p> 2019 年，我们的气候与可持续发展团队与 Google 旗下风电场的领域专家合作， <a href="https://www.deepmind.com/blog/machine-learning-can-boost-the-value-of-wind-energy" target="_blank">提高风能的价值</a>，最终旨在支持更广泛行业的增长。通过开发定制人工智能工具来更好地预测风电输出，并开发另一个模型来建议向电网供应这种预期能源的承诺，该工具极大地提高了风能的价值。 Cloud目前正在使用这种模式开发一款软件产品，并由法国电力公司ENGIE进行试点。</p><blockquote> “如果我们不制定广泛适用的解决方案，我们的气候倒计时时间就会耗尽。”<br> ‍<br> – <strong>Sims Witherspoon，气候与可持续发展主管</strong></blockquote><h4>加速科学突破</h4><p>除了优化现有基础设施之外，我们还需要科学突破来帮助我们建设可持续的能源未来。核聚变是一种前景广阔的特殊领域，这是一种极其强大的技术，有潜力提供无限的无碳能源。聚变反应堆由比太阳核心更热的电离氢加压等离子体提供动力。极高的热量意味着这种等离子体只能通过快速调整的磁场来保持——这是一个众所周知的困难的工程挑战。</p><p>掌握等离子体的磁控制是解决控制核聚变过程和利用其提供的丰富绿色能源挑战的基本部分。因此，我们与洛桑联邦理工学院的瑞士等离子体中心合作开发了一种人工智能系统，该系统可以学习如何成功<a href="https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control" target="_blank">预测和控制托卡马克式核聚变反应堆中的等离子体</a>。不仅要容纳等离子体，还要将其“雕刻”成一系列实验形状。</p><h4>给我们带来您的挑战</h4><p>为了构建有效的人工智能解决方案，研究人员需要深入了解世界各地人们面临的挑战。这包括访问代表问题的数据、与领域专家合作以确保我们构建可靠的系统、遵循监管结构的政策指导以及寻找测试这些系统的现实机会。出于这些原因，与受影响社区、科学家、行业专业人士、监管机构和政府的合作是我们可持续发展努力的核心。</p><p>如果您是行业领域专家或气候科学家，需要解决特定的挑战，以帮助世界理解、减轻或适应气候变化，我们的气候和可持续发展团队很乐意听取您的意见。<br><br>联系我们： <a href="mailto:contact-gdm-sustainability@google.com">contact-gdm-sustainability@google.com</a></p> ]]>;</description><pubDate> Fri, 21 Jul 2023 00:00:00 GMT</pubDate><guid ispermalink="false"> https://www.deepmind.com/blog/using-ai-to-fight-climate-change</guid><link/> https://www.deepmind.com/blog/using-ai-to-fight-climate-change </item><item><title><![CDATA[Google DeepMind’s latest research at ICML 2023]]></title><description><![CDATA[<h4>探索现实世界中人工智能的安全性、适应性和效率</h4><p>下周是第 40 届<a href="https://icml.cc/" target="_blank">国际机器学习会议</a>(ICML 2023) 的开幕，该会议将于 7 月 23 日至 29 日在夏威夷檀香山举行。</p><p> ICML 将人工智能 (AI) 社区聚集在一起，分享新的想法、工具和数据集，并建立联系以推动该领域的发展。从计算机视觉到机器人技术，来自世界各地的研究人员将展示他们的最新进展。</p><p>我们的科学、技术和社会总监 Shakir Mohamed 将发表<a href="https://icml.cc/Conferences/2023/InvitedTalks" target="_blank">关于具有社会目的的机器学习、应对医疗保健和气候挑战、采取社会技术观点以及加强全球社区的演讲</a>。</p><p>我们很荣幸能够作为白金赞助商支持本次会议，并继续与我们的长期合作伙伴<a href="https://www.latinxinai.org/" target="_blank">LatinX in AI</a> 、 <a href="https://www.queerinai.com/" target="_blank">Queer in AI</a>和<a href="https://wimlworkshop.org/" target="_blank">Women in Machine Learning</a>合作。</p><p>在会议上，我们还展示了<a href="https://www.deepmind.com/research/highlighted-research/alphafold" target="_blank">AlphaFold</a>的演示、我们在<a href="https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control" target="_blank">融合科学方面</a>的进展，以及用于机器人技术的<a href="https://ai.googleblog.com/2023/03/palm-e-embodied-multimodal-language.html" target="_blank">PaLM-E</a>和用于从文本生成视频的<a href="https://sites.research.google/phenaki/" target="_blank">Phenaki</a>等新模型。</p><p>谷歌 DeepMind 研究人员今年在 ICML 上发表了 80 多篇新论文。由于许多论文是在<a href="https://www.deepmind.com/blog/announcing-google-deepmind" target="_blank">Google Brain 和 DeepMind 联手</a>之前提交的，最初在 Google Brain 附属机构下提交的论文将包含在<a href="https://ai.googleblog.com/2023/07/google-at-icml-2023.html" target="_blank">Google Research 博客</a>中，而该博客则包含在 DeepMind 附属机构下提交的论文。 </p><div class="w-embed"><div class="article-gtag-buttons"><svg class="rounded-full bg-yellow-500 fill-current" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.3536 6.64648L17.3536 11.6465C17.5488 11.8417 17.5488 12.1583 17.3536 12.3536L12.3536 17.3536L11.6464 16.6465L15.7929 12.5H0V11.5H15.7929L11.6464 7.35359L12.3536 6.64648Z"></path></svg></div><div>查看 ICML 2023 上 Google DeepMind 的完整日程</div></div><style type="text/css">
.w-embed { width: 100%; !imporrtant }
</style></div><h4>（模拟）世界中的人工智能</h4><p>能够读取、写入和创造的人工智能的成功是以基础模型为基础的——在大量数据集上训练的人工智能系统可以学习执行许多任务。我们的最新研究探索了如何将这些努力转化为现实世界，并为更通用、更具体的人工智能代理奠定基础，这些人工智能代理可以更好地理解世界的动态，为更有用的人工智能工具开辟新的可能性。</p><p>在口头演示中，我们介绍了<a href="https://arxiv.org/abs/2301.07608" target="_blank">AdA</a> ，这是一种人工智能代理，可以像人类一样在模拟环境中适应解决新问题。在几分钟内，AdA 就可以承担具有挑战性的任务：以新颖的方式组合物体、导航看不见的地形以及与其他玩家合作</p><p>同样，我们展示了如何使用<a href="https://arxiv.org/abs/2301.12507" target="_blank">视觉语言模型来帮助训练实体代理</a>——例如，通过告诉机器人它在做什么。</p><h4>强化学习的未来</h4><p>为了开发负责任且值得信赖的人工智能，我们必须了解这些系统的核心目标。在强化学习中，定义这一点的一种方法是通过奖励。</p><p>在口头报告中，我们的目标是<a href="https://arxiv.org/abs/2212.10420#:~:text=The%20reward%20hypothesis%20posits%20that,to%20fully%20settle%20this%20hypothesis." target="_blank">解决理查德·萨顿（Richard Sutton）首先提出的奖励假设</a>，即所有目标都可以被认为是最大化预期累积奖励。我们解释了它成立的确切条件，并阐明了强化学习问题的一般形式的奖励可以（和不能）捕获的目标类型。</p><p>部署人工智能系统时，它们需要足够强大以适应现实世界。我们研究如何<a href="https://arxiv.org/abs/2302.01275" target="_blank">在约束条件下更好地训练强化学习算法</a>，因为人工智能工具通常必须受到安全性和效率的限制。</p><p>在我们获得<a href="https://icml.cc/Conferences/2023/Awards" target="_blank">ICML 2023 杰出论文奖的</a>研究中，我们探索了如何在<a href="https://arxiv.org/abs/2212.12567" target="_blank">不完美信息博弈</a>的不确定性下教授模型复杂的长期策略。我们分享模型如何在不知道其他玩家的位置和可能的行动的情况下赢得两人游戏。</p><h4>人工智能前沿的挑战</h4><p>人类可以轻松地学习、适应和理解我们周围的世界。开发能够以类似人类的方式进行推广的先进人工智能系统将有助于创建我们可以在日常生活中使用的人工智能工具并应对新的挑战。</p><p>人工智能适应的一种方式是根据新信息快速改变其预测。在口头演讲中，我们研究了<a href="https://arxiv.org/abs/2303.01486#:~:text=We%20find%20that%20loss%20of,units%20or%20divergent%20gradient%20norms." target="_blank">神经网络的可塑性</a>以及它在训练过程中如何丢失，以及防止丢失的方法。</p><p>我们还提出了一些研究，通过研究<a href="https://arxiv.org/abs/2302.03067" target="_blank">在统计数据自发变化的数据源（例如自然语言预测中）上进行元训练的神经网络，</a>可以帮助解释大型语言模型中出现的上下文学习类型。</p><p>在口头演讲中，我们介绍了一个新的循环神经网络<a href="https://arxiv.org/abs/2303.06349" target="_blank">(RNN) 系列，它们在长期推理任务上表现更好，</a>以释放这些模型对未来的希望。</p><p>最后，在“<a href="https://openreview.net/pdf?id=4yoLVter71" target="_blank">分位数信用分配</a>”中，我们提出了一种将运气与技能分开的方法。通过在行动、结果和外部因素之间建立更清晰的关系，人工智能可以更好地理解复杂的现实环境。 </p><div class="w-embed"><div class="article-gtag-buttons"><svg class="rounded-full bg-yellow-500 fill-current" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M12.3536 6.64648L17.3536 11.6465C17.5488 11.8417 17.5488 12.1583 17.3536 12.3536L12.3536 17.3536L11.6464 16.6465L15.7929 12.5H0V11.5H15.7929L11.6464 7.35359L12.3536 6.64648Z"></path></svg></div><div>查看 ICML 2023 上 Google DeepMind 的完整日程</div></div><style type="text/css">
.w-embed { width: 100%; !imporrtant }
</style></div>]]>;</description><pubDate> Thu, 20 Jul 2023 00:00:00 GMT</pubDate><guid ispermalink="false"> https://www.deepmind.com/blog/google-deepmind-research-at-icml-2023</guid><link/> https://www.deepmind.com/blog/google-deepmind-research-at-icml-2023</item></channel></rss>