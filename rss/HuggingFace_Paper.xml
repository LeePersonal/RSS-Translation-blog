<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[Huggingface Daily Papers]]></title><link/> https://huggingface.co/papers <atom:link href="https://rsshub-yuee.vercel.app/huggingface/daily-papers?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh-cn</language><lastbuilddate> 2023 年 8 月 15 日星期二 06:37:35 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[SpeechX: Neural Codec Language Model as a Versatile Speech Transformer]]></title><description><![CDATA[Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.]]></description><pubDate> Tue, 15 Aug 2023 06:04:07 GMT</pubDate><guid ispermalink="false"> https://www.archiv.org/abs/2308.06873</guid><link/> https://www.archiv.org/abs/2308.06873 <author><![CDATA[Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min Tang, Shujie Liu, Jinyu Li, Takuya Yoshioka]]></author></item><item><title><![CDATA[Platypus: Quick, Cheap, and Powerful Refinement of LLMs]]></title><description><![CDATA[We present $\textbf{Platypus}$, a family of fine-tuned and merged Large Language Models (LLMs) that achieves the strongest performance and currently stands at first place in HuggingFace's Open LLM Leaderboard as of the release date of this work. In this work we describe (1) our curated dataset $\textbf{Open-Platypus}$, that is a subset of other open datasets and which $\textit{we release to the public}$ (2) our process of fine-tuning and merging LoRA modules in order to conserve the strong prior of pretrained LLMs, while bringing specific domain knowledge to the surface (3) our efforts in checking for test data leaks and contamination in the training data, which can inform future research. Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs. In particular, a 13B Platypus model can be trained on $\textit{a single}$ A100 GPU using 25k questions in 5 hours. This is a testament of the quality of our Open-Platypus dataset, and opens opportunities for more improvements in the field. Project page: https://platypus-llm.github.io]]></description><pubDate> Tue, 15 Aug 2023 06:01:34 GMT</pubDate><guid ispermalink="false"> https://archiv.org/abs/2308.07317</guid><link/> https://archiv.org/abs/2308.07317<author><![CDATA[Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz]]></author></item></channel></rss>