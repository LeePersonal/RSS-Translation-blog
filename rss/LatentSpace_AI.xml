<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd"><channel><title><![CDATA[Latent Space]]></title><description><![CDATA[The AI Engineer newsletter + Top 10 US Tech podcast. Exploring AI UX, Agents, Devtools, Infra, Open Source Models, and more. See https://latent.space/about for highlights from Andrej Karpathy, George Hotz, Simon Willison, Emad Mostaque, and more!]]></description><link/> https://www.latent.space<image/><url> https://substackcdn.com/image/fetch/w_256,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22fff541- 7037-4d24-8a42-c53bad8ddf76_1280x1280.png</url><title>潜在空间</title><link/>https://www.latent.space<generator>子栈</generator><lastbuilddate>2023 年 8 月 14 日星期一 09:04:45 GMT </lastbuilddate><atom:link href="https://www.latent.space/feed" rel="self" type="application/rss+xml"></atom:link><copyright><![CDATA[Latent.Space]]></copyright><language><![CDATA[en]]></language><webmaster><![CDATA[swyx@substack.com]]></webmaster><itunes:owner><itunes:email><![CDATA[swyx@substack.com]]></itunes:email><itunes:name><![CDATA[swyx]]></itunes:name></itunes:owner><itunes:author><![CDATA[swyx]]></itunes:author><googleplay:owner><![CDATA[swyx@substack.com]]></googleplay:owner><googleplay:email><![CDATA[swyx@substack.com]]></googleplay:email><googleplay:author><![CDATA[swyx]]></googleplay:author><item><title><![CDATA[LLMs Everywhere: Running 70B models in browsers and iPhones using MLC — with Tianqi Chen of CMU / OctoML]]></title><description><![CDATA[Listen now (52 mins) | Compilers are all you need. How XGBoost and TVM were created, and the future of universal model deployments with MLC!]]></description><link/> https://www.latent.space/p/llms-everywhere<guid ispermalink="true"> https://www.latent.space/p/llms-everywhere</guid><dc:creator><![CDATA[Tianqi Chen]]></dc:creator><pubDate> Thu, 10 Aug 2023 16:42:09 GMT</pubDate> <enclosure length="0" type="audio/mpeg" url="https://api.substack.com/feed/podcast/135877261/f9697bd6bc8bd79edb6457270f5ccf77.mp3"></enclosure><content:encoded><![CDATA[<p><em>我们刚刚宣布了<a href="https://www.ai.engineer/summit">人工智能工程师峰会</a>的第一批演讲者！如果您想提供支持，请注册观看直播或发送电子邮件至<a href="http://sponsors@ai.engineer">Sponsors@ai.engineer</a> 。</em></p><div><hr></div><p>我们正面临着<a href="https://twitter.com/tqchenml/status/1689397314576662528">GPU 的严重短缺</a>。随着初创公司和风投都像各国统计核库存一样囤积 Nvidia GPU，<a href="https://twitter.com/abacaj/status/1687946803377782784?s=20">有关 GPU 短缺的推文</a>变得越来越普遍。</p><p><em>但是，如果我们可以使用 AMD 卡运行法学硕士，或者根本不需要 GPU，会怎样呢？</em>只有一个奇怪的技巧：<strong>编译。</strong>并且有一个人具有独特的资格来做到这一点。</p><p>我们很高兴与卡耐基梅隆大学助理教授<a href="https://www.linkedin.com/in/tianqi-chen-679a9856/">Tianqi Chen</a>坐下来，他教授<a href="https://mlc.ai/summer22/">MLC 课程</a>并负责 MLC 小组。您可能还知道他是<a href="https://www.nvidia.com/en-us/glossary/data-science/xgboost/">XGBoost</a> 、 <a href="https://tvm.apache.org/">Apache TVM</a>和<a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a>的创建者，以及<a href="http://octoml.ai">OctoML</a>的联合创始人。</p><p> MLC（<a href="https://mlc.ai/">机器学习编译</a>的缩写）小组发布了很多有趣的项目：</p><ul><li><p> <strong><a href="https://testflight.apple.com/join/57zd7oxa">MLC Chat</a> ：</strong>一款 iPhone 应用程序，可让您在设备上运行 RedPajama-3B 和 Vicuna-7B 等模型。速度高达 30 tok/s！</p></li><li><p> <strong><a href="https://webllm.mlc.ai/">Web LLM</a> ：</strong>在浏览器中运行 LLaMA-70B 等模型（！！）以在您的产品中提供本地推理。</p></li><li><p> <strong><a href="https://github.com/mlc-ai/mlc-llm">MLC LLM：</a></strong>一个允许任何语言模型本地部署在不同硬件和软件堆栈上的框架。 </p></li></ul><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg" width="1456" height="623" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:623,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Architecture Diagram&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="Architecture Diagram" title="架构图" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5b036d6f-1f54-4610-b1d7-aac1ed6f3ba5_794x340.svg 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p> MLC 组<a href="https://twitter.com/junrushao/status/1689321439625883648?s=20">刚刚宣布了</a>对 AMD 卡的新支持；我们之前谈到了ROCm的缺点，但是使用MLC你可以获得非常接近NVIDIA同类产品的性能。这对于创始人和开发者来说是个好消息，因为 AMD 卡更容易获得。以下是他们在 AMD 7900s 与一些顶级 NVIDIA 消费卡上的最新结果。 </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png" width="1456" height="683" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:683,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="Image" title="图像" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe4a3ebe1-930c-48b9-accc-ea4599b04cb2_1798x844.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p>如果您根本无法获得 GPU，MLC LLM 还可以利用 LLVM 支持 ARM 和 x86 CPU 架构作为目标。虽然速度性能不具有可比性，但它允许在商用硬件上运行非时间敏感的推理。</p><p>我们也很高兴了解 TQ 的流程，其中<a href="https://twitter.com/FanaHOVA/status/1689680222726283264">涉及大量草图</a>： </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg" width="1456" height="1092" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="Image" title="图像" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F82d6660d-dfaa-44ca-82b6-4dd0cb059d7b_4032x3024.jpeg 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p>随着该领域的所有其他工作（如<a href="https://github.com/ggerganov/ggml">ggml</a>和<a href="https://ollama.ai/">Ollama</a>等项目）的进行，我们很高兴看到 GPU 变得越来越不成为更多人手中模型的问题，以及针对硬件问题的创新软件解决方案！</p><h2>显示注释</h2><ul><li><p><strong>TQ的项目：</strong></p><ul><li><p> <a href="https://www.nvidia.com/en-us/glossary/data-science/xgboost/">XGBoost</a></p></li><li><p><a href="https://tvm.apache.org/">阿帕奇TVM</a></p></li><li><p> <a href="https://mxnet.apache.org/versions/1.9.1/">MXNet</a></p></li><li><p><a href="https://mlc.ai/">多层电容</a></p></li><li><p><a href="https://octoml.ai/">奥克托ML</a></p></li><li><p><a href="https://catalyst.cs.cmu.edu/">卡耐基梅隆大学催化剂</a></p></li></ul></li><li><p><a href="https://github.com/onnx/onnx">奥恩克斯</a></p></li><li><p><a href="https://github.com/ggerganov/ggml">GGML</a></p></li><li><p><a href="https://www.modular.com/mojo">莫乔</a></p></li><li><p><a href="https://webllm.mlc.ai/">网络法学硕士</a></p></li><li><p><a href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></li><li><p><a href="https://arxiv.org/abs/2206.12037">河马</a></p></li><li><p><a href="https://www.latent.space/p/flashattention#details">三道的剧情简介· · · · · ·</a></p></li><li><p><a href="https://www.latent.space/p/geohot#details">乔治·霍兹剧集</a></p></li></ul><p><strong>人员</strong>：</p><ul><li><p><a href="https://guestrin.su.domains/">卡洛斯·格斯特林</a></p></li><li><p><a href="https://twitter.com/_albertgu">顾艾伯特</a></p></li></ul><h2>时间戳</h2><ul><li><p>[00:00:00] 简介</p></li><li><p>[00:03:41] XGBoost 的创建及其令人惊讶的受欢迎程度</p></li><li><p>[00:06:01] 比较基于树的模型与深度学习</p></li><li><p>[00:10:33] TVM 概述及其如何与 ONNX 配合使用</p></li><li><p>[00:17:18] MLC 深入研究</p></li><li><p>[00:28:10] 使用 int4 量化进行语言模型推理</p></li><li><p>[00:30:32] MLC与其他模型优化项目的比较</p></li><li><p>[00:35:02] 使用 WebLLM 在浏览器中运行大型语言模型</p></li><li><p>[00:37:47] 将浏览器模型集成到应用程序中</p></li><li><p>[00:41:15] OctoAI 和自优化计算</p></li><li><p>[00:45:45] 闪电轮</p></li></ul><h2>成绩单</h2><p><strong>阿莱西奥</strong>：大家好，欢迎来到潜在空间播客。我是<a href="https://decibel.vc/">Decibel Partners</a>的常驻合伙人兼首席技术官 Alessio，和我一起的还有我的共同主持人 Swyx，他是 Latent Space 的作家和编辑。 [00:00:20]</p><p> <strong>Swyx</strong> ：好的，我们这里有陈天奇（Tianqi Chen），人们称呼他为 TQ，他是卡内基梅隆大学 CMU ML 计算机科学助理教授，同时还帮助运营 Catalyst Group，同时也是 OctoML 的首席技术专家。你身兼数职。你知道，这些是你现在的主要身份吗？当然，当然。 [00:00:42]</p><p><strong>天齐</strong>：你知道，我也非常热衷于开源。所以我也是 Apache TVM 项目的 VP 和 PRC 成员等等。但是，是的，这些是我到目前为止所做的事情。 [00:00:53]</p><p><strong>斯维克斯</strong>：是的。因此，您使用了 Apache TVM、XGBoost 和 MXNet，我们可以详细介绍其中任何一个。但也许在你的个人方面，人们可能无法从你的官方简历或 LinkedIn 中了解到你的哪一件事？ [00:01:08]</p><p> <strong>Tianqi</strong> ：让我说，是的，所以通常当我这样做时，我真的很喜欢编码，尽管我试图运行所有这些东西。所以我保持的一个习惯就是尝试画素描本。我有一本书，就像真正的素描本一样，可以用来画设计图，还有多年来我一直在画草图的素描本，现在我大约有三四本。这通常是一种有趣的经历，可以通过思考设计、了解开源项目如何发展以及回顾我们过去的草图来说，你知道，现在所有这些想法都真正变成了代码。 [00:01:43]</p><p> <strong>Alessio</strong> ：你用了多少本素描本来制作这些东西？我的意思是，如果一个人独自建造了其中一个项目，他将成为一名非常有成就的工程师。就像你建造了其中三个一样。对你来说这个过程是什么样的？就像它是素描本一样，就像开始一样，然后你思考代码之类的。 [00:01:59]</p><p><strong>斯维克斯</strong>：是的。 [00:02:00]</p><p><strong>天齐</strong>：所以，通常我会开始绘制高层架构的草图，并且在一个运行多年的项目中，我们也会开始思考，你知道，新的方向，当然就像生成式人工智能语言模型的出现，它是如何进行的进化。所以通常我会说每年大约需要一本书，大致按照这个速度。这通常很有趣，我发现勾画出草图然后为未来的一些项目提供更像是高级架构指南要容易得多。是的。 [00:02:28]</p><p> <strong>Swyx</strong> ：你出版过这本素描本吗？因为我认为人们会非常感兴趣，至少从历史的角度来看。这就是 XGBoost 诞生的时间，你知道吗？是的，不是真的。 [00:02:37]</p><p><strong>天齐</strong>：我是从XGBoost之后开始画素描的。所以这是一种缺失的部分，但 TVM 中的许多设计细节实际上是我试图记录的书籍的一部分。 [00:02:48]</p><p> <strong>Swyx</strong> ：是的，我们会尝试发表它们并在期刊上发表一些内容。也许您可以抓一些快照来获得视觉帮助。听起来不错。 [00:02:57]</p><p><strong>阿莱西奥</strong>：是的。是的，说到 XGBoost，很多观众可能都知道它是一个梯度提升库，可能是最受欢迎的。它变得非常受欢迎，因为许多人开始像机器学习比赛一样使用它们。我认为就像所有最先进模型的整个维基百科页面一样。他们使用 XGBoost 之类的，这是一个非常长的列表。当你在做这个的时候，我们只有 Tri Dao，他是播客上 FlashAttention 的创建者。我问了他这个问题，就像，当你构建 FlashAttention 时，你知道几乎所有变换竞赛模型都会使用它吗？因此，当您提出 XGBoost 时，我向您提出了同样的问题，例如，您能预测它会如此受欢迎吗？或者创建过程是怎样的？当你发布它时，你期望什么？我们不知道。 [00:03:41]</p><p><strong>天齐</strong>：其实我们建这个库的初衷是因为当时深度学习刚刚出来。就像那时 AlexNet 刚刚问世一样。我和我的顾问卡洛斯·盖斯特林 (Carlos Guestrin) 的雄心勃勃的使命之一是我们想要思考，你知道，尝试检验这个假设。我们能找到深度学习模型的替代方案吗？因为那时，还有其他替代方案，例如支持向量机、线性模型，当然还有基于树的模型。我们的问题是，如果你构建这些模型并为它们提供足够大的数据，因为深度学习的关键特征之一通常是它需要大量数据[00:04:22]</p><p> <strong>Swyx</strong> ：数据，对吗？ [00:04:23]</p><p><strong>天齐</strong>：这样我们就能获得同等的性能。这是我们要测试的假设。当然，如果你现在看看，对，这是一个错误的假设，但作为副产品，我们发现，你知道，大多数梯度增强库的效率不足以让我们测试该假设。所以我过去在构建梯度增强树及其变体方面碰巧有相当多的经验。因此，有效行动推动有点像假设检验的副产品。当时，我也在数据科学挑战中竞争，就像我在 KDDCup 上工作，然后 Kaggle 变得更大，对吧？所以我想也许它对其他人来说变得有用。我的一位朋友说服我尝试对其进行 Python 绑定。这往往是一个非常好的决定，对吧，而且是有效的。通常当我构建它时，我们觉得命令行界面可能还可以。现在我们有一个 Python 绑定，我们有 R 绑定。然后它意识到，你知道，它开始变得有趣。人们开始贡献不同的观点，例如可视化等。因此，我们开始更加努力地构建分布式支持，以确保它可以在任何平台上运行等等。即使在那个时候，当我后来与我的顾问卡洛斯交谈时，他说他从未预料到我们会取得如此程度的成功。实际上，为什么我推动梯度增强树，有趣的是，当时他也不同意。他认为也许我们应该选择内核机器。事实证明，你知道，实际上，从某种意义上说，我们都错了，深度神经网络才是王者。但至少梯度提升方向取得了丰硕的成果。 [00:06:01]</p><p><strong>斯维克斯</strong>：有趣。 [00:06:02]</p><p> <strong>Alessio</strong> ：当谈到这些改进时，我总是很好奇，比如，设计过程是怎样的？其中有多少是与与你一起工作的其他人一样的合作，而不是试图成为，你知道，显然，在学术界，这就像非常论文驱动的研究驱动。 [00:06:19]</p><p> <strong>Tianqi</strong> ：我想说，那个时间点的额外提升改进更多的是，你知道，我正在试图弄清楚，对吧。但它正在结合教训。在此之前，我曾研究过其他一些有关矩阵分解的库。这就像我第一次开源经历。没有人知道这一点，因为你可能会发现，如果你尝试搜索软件包 SVD 功能，你会在某个地方找到一些 SVN 存储库。但它实际上被用于一些推荐系统包。因此，我尝试应用之前的一些经验教训并将它们结合起来。后来的项目，如 MXNet 和 TVM，从某种意义上说，协作性要强得多……但是，当然，额外的推动力变得更大了，对吧？因此，当我们自己启动这个项目时，看到人们加入真是太棒了。迈克尔是一名律师，现在也在人工智能领域工作，致力于贡献可视化。现在，我们社区的人们贡献了不同的东西。因此，即使在今天，仍然有额外的推动力，对吧，这是一个推动该项目的提交者社区。因此，这绝对是一种协作和进步，为我们的社区不断改进一些东西。 [00:07:37]</p><p> <strong>Alessio</strong> ：我们也谈谈 TVM，因为我们在这一集中有很多事情要讲。 [00:07:42]</p><p> <strong>Swyx</strong> ：我想说，在某些时候，我很乐意谈论额外提升或基于树的人工智能或机器学习与深度学习之间的比较，因为我认为周围有很多人感兴趣，我想，合并两个学科，对吗？我们可以更多地讨论这一点。顺便说一句，我不知道该在哪里插入，所以我们稍后再回来讨论。是的。 [00:08:04]</p><p><strong>天齐</strong>：其实我说的，当我们检验假设的时候，假设是一种，我会说它是部分错误的，因为我们现在要检验的假设是，你能在图像分类任务上运行基于树的模型吗？深度学习当然是理所当然的事情 [00:08:17]</p><p> <strong>Swyx</strong> ：现在就是今天，对吧？ [00:08:18]</p><p><strong>天齐</strong>：但是如果你尝试在表格数据上运行它，你仍然会发现大多数人选择基于树的模型。这是有原因的，从某种意义上说，当您查看基于树的模型时，决策边界自然就是您正在查看的规则，对吗？它们还具有很好的特性，例如能够与输入规模无关并且能够自动将功能组合在一起。我知道有人尝试构建适用于表格数据的神经网络模型，有时我也会关注它们。我确实觉得建模空间具有一定的多样性是件好事。实际上，当我们构建 TVM 时，我们为程序构建成本模型，实际上我们也在使用 XGBoost。我仍然认为基于树的模型将非常相关，因为首先，它确实要让它开箱即用。而且，您将能够获得一些互操作性和控制单调性 [00:09:18]</p><p> <strong>Swyx</strong> ：等等。 [00:09:19]</p><p><strong>天齐</strong>：所以是的，它仍然具有相关性。有时我也会不断地回想，我们是否可以在这些模型的基础上进行改进？当然，我觉得这是一个未来有潜力的空间。 [00:09:34]</p><p> <strong>Swyx</strong> ：您认为目前有哪些项目在合并这两个方向方面有希望吗？ [00:09:41]</p><p><strong>天齐</strong>：我认为有些项目试图为表格数据带来变压器类型的模型。我不记得它们的具体细节，但我认为即使在今天，如果你看看人们使用的东西，基于树的模型仍然是他们的工具包之一。所以我认为也许最终它甚至不是一个替代品，它只是一个你可以调用的模型集合。完美的。 [00:10:07]</p><p> <strong>Alessio</strong> ：接下来，大约在 XGBoost 三年后，您构建了一个名为 TVM 的东西，它现在是一个非常流行的模型编译器框架。我们来谈谈，所以这个是和ONNX同时出来的。因此，我认为如果您能稍微概述一下这两者如何协同工作，那就太好了。因为它有点像模型，然后转到ONNX，然后转到TVM。但我认为很多人不明白其中的细微差别。我可以了解一些相关的背景故事。 [00:10:33]</p><p><strong>天齐</strong>：实际上，这是一段古老的历史。在 XGBoost 之前，我从事深度学习工作了两三年。在开始攻读博士学位之前，我获得了硕士学位。在我攻读硕士学位期间，我的论文重点是应用卷积受限玻尔兹曼机进行 ImageNet 分类。这就是我正在做的事情。那是在 AlexNet 时刻之前。因此，我必须在 GTX 2070 卡上手工制作 NVIDIA CUDA 内核。我有一张22070卡。我花了大约六个月的时间才让一个模型正常工作。最终，这个模型并不是那么好，我们应该选择一个更好的模型。但这就像一段古老的历史，真正让我进入了这个深度学习领域。当然，最终我们发现它没有成功。所以在我的硕士阶段，我最终研究了推荐系统，这让我得到了一篇论文，我申请并获得了博士学位。但我一直想回来从事深度学习领域的工作。因此，在 XGBoost 之后，我想我开始与一些人一起研究这个特定的 MXNet。那时候好像CAFE、Ciano、PyTorch的框架还没有出来。我们正在努力优化 GPU 的性能。当时我发现这真的很难，即使对于 NVIDIA GPU 来说也是如此。我花了六个月的时间。然后令人惊讶的是，在不同的硬件上，为感兴趣的平台优化代码是多么困难。所以这让我思考，我们可以构建一些更通用和自动化的东西吗？这样我就不需要一个由这么多人组成的整个团队来构建这些框架。这就是开始从事 TVM 工作的动机。在我们感兴趣的平台上支持深度学习模型所需的机器学习工程确实太少了。我认为它比 ONNX 开始得早一点，但一旦宣布，我认为它处于相似的时间段那时。总的来说，TVM 的工作原理是，您将能够采用我们所说的计算图表示的机器学习程序的子集。如今，我们还可以表示从机器学习模型中摄取的循环级程序。通常，您有 ONNX 模型格式，或者在 PyTorch 中，它们有 FX Tracer，可让您跟踪 FX 图表。然后通过TVM。我们还意识到，是的，它需要更加可定制，因此它将能够执行一些编译优化，例如融合运算符，进行智能内存规划，更重要的是，生成低级代码。因此，这适用于 NVIDIA，并且还可以移植到其他 GPU 后端，甚至是非 GPU 后端 [00:13:36]</p><p> <strong>Swyx</strong> ：就在那里。 [00:13:37]</p><p><strong>天齐</strong>：所以这实际上是我过去几年主要关注的一个项目。很高兴看到它是如何开始的，我认为我们是机器学习编译的早期发起者。我记得有一天有一次拜访，一位学生问我，你还在做深度学习框架吗？我告诉他们我正在从事机器学习编译工作。他们说，好吧，汇编，这听起来很古老。这听起来像是一个非常古老的领域。你为什么要做这个？现在它开始受到更多关注，就像你说 Torch Compile 和其他东西一样。我真的很高兴看到这个领域开始复苏。我们还必须在这里继续创新。 [00:14:17]</p><p> <strong>Alessio</strong> ：我认为我注意到的另一件事是，从 XGBoost 到 TVM 的关注领域有点像一个巨大的飞跃，它有点像堆栈的不同部分。你为什么决定这样做？我认为关于编译到不同的 GPU 以及最终的 CPU 的另一件事是，您是否已经看到模型可能只专注于一个运行时、仅关注 CUDA 等的一些压力，以及其中有多少投入其中？ [00:14:50]</p><p><strong>天齐</strong>：我觉得与其说是想产生影响，不如说是想玩得开心。我喜欢破解代码，破解 CUDA 代码非常有趣。当然，能够生成 CUDA 代码很酷，对吧？但现在，在能够生成 CUDA 代码之后，好吧，顺便说一句，你也可以在其他平台上做到这一点，这不是很神奇吗？所以更多的是这种态度让我开始做这件事。而且，我认为当我们观察不同的研究人员时，我自己更像是一个问题解决者类型。所以我喜欢看到一个问题然后说，好吧，我们需要什么样的工具来解决这个问题？因此无论如何，它都可以构建更好的模型。例如，当我们制造额外的靴子时，我们会对其进行某些正则化，以使其更加稳健。它还意味着构建系统优化、编写低级代码、可能尝试编写汇编和构建编译器等等。所以只要他们解决了问题，一定要去尝试一起去做。我也看到这是现在的一个普遍趋势。就像如果你想能够解决机器学习问题，它不再是在攻击者层，对吧？您需要从攻击者数据和系统角度来解决它。我认为整个机器学习系统领域正在兴起。现在有一个围绕它的会议。很高兴看到越来越多的人开始研究这个问题。 [00:16:10]</p><p><strong>斯维克斯</strong>：是的。你是在谈论 ICML 还是其他什么？ [00:16:13]</p><p><strong>天齐</strong>：所以机器学习和系统，对吗？所以不仅仅是机器学习，还有机器学习和系统。因此，有一个名为 MLsys 的会议。它绝对是一个比 ICML 更小的社区，但我认为它也是一个新兴且不断发展的社区，人们在其中讨论构建机器学习系统的影响，对吗？您如何围绕此进行优化并共同设计模型和系统？ [00:16:37]</p><p><strong>斯维克斯</strong>：是的。您还担任过 ICML 和 NeurIPS 的区域主席。所以你刚刚拥有了很多会议和社区组织的经验。这也是你工作的重要组成部分吗？嗯，这对于学术界来说是值得期待的。 [00:16:48]</p><p><strong>天齐</strong>：如果我从事学术工作，我就需要为社会做服务。好的，太好了。 [00:16:53]</p><p> <strong>Swyx</strong> ：您最近在 MLsys 的投资是通过 MLCLLM 开发手机。你在四月份宣布了这一点。我手机上有。这很棒。我正在运行 Lama 2，Vicuña。我不知道你们还提供什么其他型号。但也许只是描述一下您进入 MLC 的旅程。我不知道这与你在卡耐基梅隆大学的工作有何吻合之处。这是某种产物吗？ [00:17:18]</p><p><strong>天齐</strong>：我认为这更像是我们想要在机器学习编译领域进行的集中努力。所以这与我们在 TVM 中构建的内容有关。那么我们建立 TVM 的时候已经是五年前了，对吧？发生了很多事情。我们构建了可用的端到端机器学习编译器，这是第一个可用的编译器。但后来我们在那里吸取了很多教训。因此，我们正在构建第二个迭代，称为 TVM Unity。这使我们能够让机器学习工程师能够快速捕获新模型以及我们如何要求为它们构建优化。 MLCLLM 有点像 MLC。它更像是一个垂直驱动的组织，我们构建教程，构建像法学硕士这样的项目到解决方案。因此，要真正表明，好吧，您可以采用机器学习编译技术并应用它并带来一些有趣的东西。是的。所以是的，它可以在手机上运行，​​这真的很酷。但这里的目标不仅仅是让它在手机上运行，​​对吧？目标是使其普遍部署。因此，我们确实在 Apple M2 Mac 上运行，即 170 亿台型号。实际上，在单批推理中，最近在 CUDA 上，我认为我们已经在 4 位推理上获得了最好的性能。事实上，正如我在播客之前提到的，我们刚刚得到了 AMD 的结果。实际上，在单个批次中，我们可以获得最新的 AMD GPU。这是一张消费卡。它可以达到 4019 的 80% 左右，因此是 NVIDIA 最好的消费卡。所以它还没有达到标准，但考虑一下多样性、你可以实现什么以及你可以在该卡上获得的以前的东西，你可以用这种技术做的事情真的很令人惊奇。 [00:19:10]</p><p> <strong>Swyx</strong> ：所以我有点困惑的一件事是，大多数模型都在 PyTorch 中，但你在 TVM 中运行它。我不知道。您是否需要做任何根本性的改变，或者这基本上就是 TVM 的基本设计？ [00:19:25]</p><p><strong>天齐</strong>：所以我们的想法是，当然，这又回到了程序表示，对吧？因此，TVM 具有称为 TVM 脚本的程序表示，其中包含更多类似计算图和操作表示的内容。所以，是的，最初，我们确实需要付出一些努力将这些模型引入 TVM 支持的程序表示中。通常，有多种方法，具体取决于您正在查看的模型类型。例如，对于视觉模型和稳定扩散模型，通常我们可以将 PyTorch 模型引入 TVM 进行跟踪。这部分仍在加强中，以便我们可以引入更多模型。在语言模型任务上，实际上我们所做的是直接构建一些模型构造函数，并尝试直接从 Hugging Face 模型进行映射。我们的目标是，如果您有 Hugging Face 配置，我们将能够引入该配置并对其进行优化。因此，模型编译的一个有趣的事情是，您的优化不仅仅作为一种软语言发生，对吧？例如，如果您正在编写 PyTorch 代码，您只需尝试在源代码级别使用更好的融合运算符即可。 Torch 编译可能会帮助你在那里做一些事情。在大多数模型编译中，它不仅发生在开始阶段，而且我们还通过 Python API 在中间应用通用转换。所以你可以调整其中一些。因此，这部分优化有助于提高性能和环境的可移植性。我们拥有的另一件事就是所谓的通用部署。因此，如果将 ML 程序转换成这种 TVM 脚本格式，其中有接受张量和输出张量的函数，我们将能够有一种方法来编译它。因此，他们将能够在 TVM 支持的任何语言运行时加载该函数。因此，如果您可以在 JavaScript 中加载它，那就是一个可以接受张量和输出张量的 JavaScript 函数。当然，如果您要加载 Python，还有 C++ 和 Java。因此，我们的目标实际上是将 ML 模型引入人们关心的语言，并能够在他们喜欢的平台上运行它。 [00:21:37]</p><p> <strong>Swyx</strong> ：令我印象深刻的是，我已经与很多编译器人员交谈过，但你们没有传统的编译器背景。您正在发明自己的学科，称为机器学习编译（MLC）。您认为这会是一个更大的领域吗？ [00:21:52]</p><p><strong>天齐</strong>：首先，我也和从事编译工作的人一起工作。因此，我们也从该领域的许多早期创新中汲取灵感。比如最初的 TVM，我们从 Halide 中得到了很多灵感，它只是一个图像处理编译器。当然，从那时起，我们已经发展了很多，专注于机器学习相关的编译。如果您查看我们的一些会议出版物，您会发现机器学习编译已经是一个子领域。因此，如果你看一下机器学习场馆（当然是 MLC 会议）和系统场馆的论文，每年都会有关于机器学习编译的论文。在名为 CGO 的编译器会议中，有一个 C4ML 研讨会也试图关注这一领域。所以它肯定已经开始受到关注并成为一个领域。我不会声称我发明了这个领域，但我确实帮助了那里的很多人工作。当然，我尝试提出一个观点，尝试从编译器优化中学习很多东西，并尝试将机器学习和系统方面的知识结合起来。 [00:23:07]</p><p> <strong>Alessio</strong> ：乔治·霍茨 (George Hotz) 曾在几集节目中做过播客，他对 AMD 及其软件有很多话要说。那么，当您考虑 TVM 时，您是否仍然在某种程度上受到底层内核性能的限制？因此，如果您的目标类似于 CUDA 运行时，您仍然可以获得更好的性能，无论 TVM 是否可以帮助您实现目标，但您不关心该级别，对吗？ [00:23:34]</p><p> <strong>Swyx</strong> ：这里有两个部分，对吧？ [00:23:35]</p><p><strong>天齐</strong>：所以首先有较低级别的运行时，比如 CUDA 运行时。实际上对于 NVIDIA 来说，很多情绪都来自他们的库，比如 Cutlass、CUDN，对吧？那些库优化。对于专门的工作负载，实际上您可以将它们专门化。因为很多情况下你会发现如果你去做基准测试，那是非常有趣的。例如，就像两年前一样，如果您尝试对 ResNet 进行基准测试，通常是 NVIDIA 库 [00:24:04]</p><p> <strong>Swyx</strong> ：给你最好的表现。 [00:24:06]</p><p><strong>天齐</strong>：打败他们真的很难。但是一旦你开始将模型更改为某种东西，也许是 ResNet 的一些变体，不是用于传统的 ImageNet 检测，而是用于潜在检测等，就会有一些优化的空间，因为人们有时会过度拟合基准。这些人都是去优化事物的人，对吗？所以人们过度拟合了基准。所以这是最大的障碍，就像能够获得低级内核库一样，对吗？从这个意义上说，TVM 的目标实际上是我们尝试拥有一个通用层，当然，在可用时利用库，但也能够自动生成 [00:24:45]</p><p> <strong>Swyx</strong> ：如果可能的话，可以使用库。 [00:24:46]</p><p><strong>天齐</strong>：所以从这个意义上说，我们不受他们提供的库的限制。这就是为什么我们能够在没有可用库的情况下运行 Apple M2 或 WebGPU，因为我们有点像自动生成库。这使得支持不太好的硬件变得更容易，对吗？例如，WebGPU 就是一个例子。从运行时的角度来看，AMD，我认为之前他们的 Vulkan 驱动程序没有得到很好的支持。最近，他们的状态渐入佳境。但即使在此之前，我们也将能够通过名为 Vulkan 的 GPU 图形后端来支持 AMD，它的性能不那么出色，但它为您提供了良好的可移植性 [00:25:29]</p><p> <strong>Swyx</strong> ：硬件。 [00:25:29]</p><p> <strong>Alessio</strong> ：我知道我们还有其他 MLC 内容要讨论，例如 WebLLM，但我想总结一下您正在做的优化。所以有四个核心的东西，对吗？内核融合，我们在 Flash Attention 章节和 Tiny Grab One 内存规划和循环优化中对此进行了一些讨论。我认为这些很漂亮，你知道，不言自明的。我认为人们问得最多的问题是，你能快速解释一下吗[00:25:53]</p><p> <strong>Swyx</strong> ：那些？ [00:25:54]</p><p><strong>天齐</strong>：所以有一些不同的东西，对吧？内核融合意味着，你知道，如果你有一个像 Convolutions 这样的运算符，或者像 MOP 这样的转换器，那么你还有其他遵循它的运算符，对吗？您不想启动两个 GPU 内核。您希望能够以巧妙的方式将它们组合在一起，对吧？作为内存规划，它更多的是，你知道，嘿，如果你像 Python 代码一样运行，每次生成一个新数组时，你都会有效地分配一块新内存，对吗？当然，PyTorch 和其他框架会尝试为您进行优化。所以幕后有一个智能内存分配器。但实际上，在很多情况下，提前静态分配和计划一切会更好。这就是编译器可以发挥作用的地方。首先，我们需要，实际上对于语言模型，它要困难得多，因为动态形状。所以你需要能够进行我们所说的符号形状追踪。所以我们有一个符号变量，它告诉你第一个张量的形状是 n x 12。第三个张量的形状也是 n x 12。或者可能是 n 乘以 2 x 12。尽管你不知道n 是什么，对吧？但你将能够知道这种关系，并能够用它来推理类似的融合和其他决策。所以除此之外，我认为循环改造也是相当重要的。这实际上是非传统的。本来，如果只是简单的写一段代码，想要得到一个性能，那是非常困难的。例如，你知道，如果你编写一个矩阵乘法器，你能做的最简单的事情就是对 i，j，k，c，i，j，加，等于，你知道，a，i，k，乘以 b ， 我知道。 But that code is 100 times slower than the best available code that you can get. So we do a lot of transformation, like being able to take the original code, trying to put things into shared memory, and making use of tensor calls, making use of memory copies, and all this. Actually, all these things, we also realize that, you know, we cannot do all of them. So we also make the ML compilation framework as a Python package, so that people will be able to continuously improve that part of engineering in a more transparent way. So we find that&#39;s very useful, actually, for us to be able to get good performance very quickly on some of the new models. Like when Lamato came out, we&#39;ll be able to go and look at the whole, here&#39;s the bottleneck, and we can go and optimize those. [00:28:10]</p><p> <strong>Alessio</strong> : And then the fourth one being weight quantization. So everybody wants to know about that. And just to give people an idea of the memory saving, if you&#39;re doing FB32, it&#39;s like four bytes per parameter. Int8 is like one byte per parameter. So you can really shrink down the memory footprint. What are some of the trade-offs there? How do you figure out what the right target is? And what are the precision trade-offs, too? [00:28:37]</p><p> <strong>Tianqi</strong> : Right now, a lot of people also mostly use int4 now for language models. So that really shrinks things down a lot. And more recently, actually, we started to think that, at least in MOC, we don&#39;t want to have a strong opinion on what kind of quantization we want to bring, because there are so many researchers in the field. So what we can do is we can allow developers to customize the quantization they want, but we still bring the optimum code for them. So we are working on this item called bring your own quantization. In fact, hopefully MOC will be able to support more quantization formats. And definitely, I think there&#39;s an open field that&#39;s being explored. Can you bring more sparsities? Can you quantize activations as much as possible, and so on? And it&#39;s going to be something that&#39;s going to be relevant for quite a while. [00:29:27]</p><p> <strong>Swyx</strong> : You mentioned something I wanted to double back on, which is most people use int4 for language models. This is actually not obvious to me. Are you talking about the GGML type people, or even the researchers who are training the models also using int4? [00:29:40]</p><p> <strong>Tianqi</strong> : Sorry, so I&#39;m mainly talking about inference, not training, right? So when you&#39;re doing training, of course, int4 is harder, right? Maybe you could do some form of mixed type precision for inference. I think int4 is kind of like, in a lot of cases, you will be able to get away with int4. And actually, that does bring a lot of savings in terms of the memory overhead, and so on. [00:30:09]</p><p> <strong>Alessio</strong> : Yeah, that&#39;s great. Let&#39;s talk a bit about maybe the GGML, then there&#39;s Mojo. How should people think about MLC? How do all these things play together? I think GGML is focused on model level re-implementation and improvements. Mojo is a language, super sad. You&#39;re more at the compiler level. Do you all work together? Do people choose between them? [00:30:32]</p><p> <strong>Tianqi</strong> : So I think in this case, I think it&#39;s great to say the ecosystem becomes so rich with so many different ways. So in our case, GGML is more like you&#39;re implementing something from scratch in C, right? So that gives you the ability to go and customize each of a particular hardware backend. But then you will need to write from CUDA kernels, and you write optimally from AMD, and so on. So the kind of engineering effort is a bit more broadened in that sense. Mojo, I have not looked at specific details yet. I think it&#39;s good to start to say, it&#39;s a language, right? I believe there will also be machine learning compilation technologies behind it. So it&#39;s good to say, interesting place in there. In the case of MLC, our case is that we do not want to have an opinion on how, where, which language people want to develop, deploy, and so on. And we also realize that actually there are two phases. We want to be able to develop and optimize your model. By optimization, I mean, really bring in the best CUDA kernels and do some of the machine learning engineering in there. And then there&#39;s a phase where you want to deploy it as a part of the app. So if you look at the space, you&#39;ll find that GGML is more like, I&#39;m going to develop and optimize in the C language, right? And then most of the low-level languages they have. And Mojo is that you want to develop and optimize in Mojo, right? And you deploy in Mojo. In fact, that&#39;s the philosophy they want to push for. In the ML case, we find that actually if you want to develop models, the machine learning community likes Python. Python is a language that you should focus on. So in the case of MLC, we really want to be able to enable, not only be able to just define your model in Python, that&#39;s very common, right? But also do ML optimization, like engineering optimization, CUDA kernel optimization, memory planning, all those things in Python that makes you customizable and so on. But when you do deployment, we realize that people want a bit of a universal flavor. If you are a web developer, you want JavaScript, right? If you&#39;re maybe an embedded system person, maybe you would prefer C++ or C or Rust. And people sometimes do like Python in a lot of cases. So in the case of MLC, we really want to have this vision of, you optimize, build a generic optimization in Python, then you deploy that universally onto the environments that people like. [00:32:54]</p><p> <strong>Swyx</strong> : That&#39;s a great perspective and comparison, I guess. One thing I wanted to make sure that we cover is that I think you are one of these emerging set of academics that also very much focus on your artifacts of delivery.当然。 Something we talked about for three years, that he was very focused on his GitHub. And obviously you treated XGBoost like a product, you know? And then now you&#39;re publishing an iPhone app. Okay. Yeah. Yeah. What is his thinking about academics getting involved in shipping products? [00:33:24]</p><p> <strong>Tianqi</strong> : I think there are different ways of making impact, right? Definitely, you know, there are academics that are writing papers and building insights for people so that people can build product on top of them. In my case, I think the particular field I&#39;m working on, machine learning systems, I feel like really we need to be able to get it to the hand of people so that really we see the problem, right? And we show that we can solve a problem. And it&#39;s a different way of making impact. And there are academics that are doing similar things. Like, you know, if you look at some of the people from Berkeley, right? A few years, they will come up with big open source projects. Certainly, I think it&#39;s just a healthy ecosystem to have different ways of making impacts. And I feel like really be able to do open source and work with open source community is really rewarding because we have a real problem to work on when we build our research. Actually, those research bring together and people will be able to make use of them. And we also start to see interesting research challenges that we wouldn&#39;t otherwise say, right, if you&#39;re just trying to do a prototype and so on. So I feel like it&#39;s something that is one interesting way of making impact, making contributions. [00:34:40]</p><p> <strong>Swyx</strong> : Yeah, you definitely have a lot of impact there. And having experience publishing Mac stuff before, the Apple App Store is no joke. It is the hardest compilation, human compilation effort. So one thing that we definitely wanted to cover is running in the browser. You have a 70 billion parameter model running in the browser.这是正确的。 Can you just talk about how? Yeah, of course. [00:35:02]</p><p> <strong>Tianqi</strong> : So I think that there are a few elements that need to come in, right? First of all, you know, we do need a MacBook, the latest one, like M2 Max, because you need the memory to be big enough to cover that. So for a 70 million model, it takes you about, I think, 50 gigahertz of RAM. So the M2 Max, the upper version, will be able to run it, right? And it also leverages machine learning compilation. Again, what we are doing is the same, whether it&#39;s running on iPhone, on server cloud GPUs, on AMDs, or on MacBook, we all go through that same MOC pipeline. Of course, in certain cases, maybe we&#39;ll do a bit of customization iteration for either ones. And then it runs on the browser runtime, this package of WebLM. So that will effectively... So what we do is we will take that original model and compile to what we call WebGPU. And then the WebLM will be to pick it up. And the WebGPU is this latest GPU technology that major browsers are shipping right now. So you can get it in Chrome for them already. It allows you to be able to access your native GPUs from a browser. And then effectively, that language model is just invoking the WebGPU kernels through there. So actually, when the LATMAR2 came out, initially, we asked the question about, can you run 17 billion on a MacBook? That was the question we&#39;re asking. So first, we actually... Jin Lu, who is the engineer pushing this, he got 17 billion on a MacBook. We had a CLI version. So in MLC, you will be able to... That runs through a metal accelerator. So effectively, you use the metal programming language to get the GPU acceleration. So we find, okay, it works for the MacBook. Then we asked, we had a WebGPU backend. Why not try it there? So we just tried it out. And it&#39;s really amazing to see everything up and running. And actually, it runs smoothly in that case. So I do think there are some kind of interesting use cases already in this, because everybody has a browser. You don&#39;t need to install anything. I think it doesn&#39;t make sense yet to really run a 17 billion model on a browser, because you kind of need to be able to download the weight and so on. But I think we&#39;re getting there. Effectively, the most powerful models you will be able to run on a consumer device. It&#39;s kind of really amazing. And also, in a lot of cases, there might be use cases. For example, if I&#39;m going to build a chatbot that I talk to it and answer questions, maybe some of the components, like the voice to text, could run on the client side. And so there are a lot of possibilities of being able to have something hybrid that contains the edge component or something that runs on a server. [00:37:47]</p><p> <strong>Alessio</strong> : Do these browser models have a way for applications to hook into them? So if I&#39;m using, say, you can use OpenAI or you can use the local model.当然。 [00:37:56]</p><p> <strong>Tianqi</strong> : Right now, actually, we are building... So there&#39;s an NPM package called WebILM, right? So that you will be able to, if you want to embed it onto your web app, you will be able to directly depend on WebILM and you will be able to use it. We are also having a REST API that&#39;s OpenAI compatible. So that REST API, I think, right now, it&#39;s actually running on native backend. So that if a CUDA server is faster to run on native backend. But also we have a WebGPU version of it that you can go and run. So yeah, we do want to be able to have easier integrations with existing applications. And OpenAI API is certainly one way to do that. Yeah, this is great. [00:38:37]</p><p> <strong>Swyx</strong> : I actually did not know there&#39;s an NPM package that makes it very, very easy to try out and use. I want to actually... One thing I&#39;m unclear about is the chronology. Because as far as I know, Chrome shipped WebGPU the same time that you shipped WebILM. Okay, yeah. So did you have some kind of secret chat with Chrome? [00:38:57]</p><p> <strong>Tianqi</strong> : The good news is that Chrome is doing a very good job of trying to have early release. So although the official shipment of the Chrome WebGPU is the same time as WebILM, actually, you will be able to try out WebGPU technology in Chrome. There is an unstable version called Canary. I think as early as two years ago, there was a WebGPU version. Of course, it&#39;s getting better. So we had a TVM-based WebGPU backhand two years ago. Of course, at that time, there were no language models. It was running on less interesting, well, still quite interesting models. And then this year, we really started to see it getting matured and performance keeping up. So we have a more serious push of bringing the language model compatible runtime onto the WebGPU. [00:39:45]</p><p> <strong>Swyx</strong> : I think you agree that the hardest part is the model download. Has there been conversations about a one-time model download and sharing between all the apps that might use this API? That is a great point. [00:39:58]</p><p> <strong>Tianqi</strong> : I think it&#39;s already supported in some sense. When we download the model, WebILM will cache it onto a special Chrome cache. So if a different web app uses the same WebILM JavaScript package, you don&#39;t need to redownload the model again. So there is already something there. But of course, you have to download the model once at least to be able to use it. [00:40:19]</p><p> <strong>Swyx</strong> : Okay. One more thing just in general before we&#39;re about to zoom out to OctoAI. Just the last question is, you&#39;re not the only project working on, I guess, local models.这是正确的。 Alternative models. There&#39;s gpt4all, there&#39;s olama that just recently came out, and there&#39;s a bunch of these. What would be your advice to them on what&#39;s a valuable problem to work on? And what is just thin wrappers around ggml? Like, what are the interesting problems in this space, basically? [00:40:45]</p><p> <strong>Tianqi</strong> : I think making API better is certainly something useful, right? In general, one thing that we do try to push very hard on is this idea of easier universal deployment. So we are also looking forward to actually have more integration with MOC. That&#39;s why we&#39;re trying to build API like WebILM and other things. So we&#39;re also looking forward to collaborate with all those ecosystems and working support to bring in models more universally and be able to also keep up the best performance when possible in a more push-button way. [00:41:15]</p><p> <strong>Alessio</strong> : So as we mentioned in the beginning, you&#39;re also the co-founder of Octomel. Recently, Octomel released OctoAI, which is a compute service, basically focuses on optimizing model runtimes and acceleration and compilation. What has been the evolution there? So Octo started as kind of like a traditional MLOps tool, where people were building their own models and you help them on that side. And then it seems like now most of the market is shifting to starting from pre-trained generative models. Yeah, what has been that experience for you and what you&#39;ve seen the market evolve? And how did you decide to release OctoAI? [00:41:52]</p><p> <strong>Tianqi</strong> : One thing that we found out is that on one hand, it&#39;s really easy to go and get something up and running, right? So if you start to consider there&#39;s so many possible availabilities and scalability issues and even integration issues since becoming kind of interesting and complicated. So we really want to make sure to help people to get that part easy, right? And now a lot of things, if we look at the customers we talk to and the market, certainly generative AI is something that is very interesting. So that is something that we really hope to help elevate. And also building on top of technology we build to enable things like portability across hardwares. And you will be able to not worry about the specific details, right? Just focus on getting the model out. We&#39;ll try to work on infrastructure and other things that helps on the other end. [00:42:45]</p><p> <strong>Alessio</strong> : And when it comes to getting optimization on the runtime, I see when we run an early adopters community and most enterprises issue is how to actually run these models. Do you see that as one of the big bottlenecks now? I think a few years ago it was like, well, we don&#39;t have a lot of machine learning talent. We cannot develop our own models. Versus now it&#39;s like, there&#39;s these great models you can use, but I don&#39;t know how to run them efficiently. [00:43:12]</p><p> <strong>Tianqi</strong> : That depends on how you define by running, right? On one hand, it&#39;s easy to download your MLC, like you download it, you run on a laptop, but then there&#39;s also different decisions, right? What if you are trying to serve a larger user request? What if that request changes? What if the availability of hardware changes? Right now it&#39;s really hard to get the latest hardware on media, unfortunately, because everybody&#39;s trying to work on the things using the hardware that&#39;s out there. So I think when the definition of run changes, there are a lot more questions around things. And also in a lot of cases, it&#39;s not only about running models, it&#39;s also about being able to solve problems around them. How do you manage your model locations and how do you make sure that you get your model close to your execution environment more efficiently? So definitely a lot of engineering challenges out there. That we hope to elevate, yeah. And also, if you think about our future, definitely I feel like right now the technology, given the technology and the kind of hardware availability we have today, we will need to make use of all the possible hardware available out there. That will include a mechanism for cutting down costs, bringing something to the edge and cloud in a more natural way. So I feel like still this is a very early stage of where we are, but it&#39;s already good to see a lot of interesting progress. [00:44:35]</p><p> <strong>Alessio</strong> : Yeah, that&#39;s awesome. I would love, I don&#39;t know how much we&#39;re going to go in depth into it, but what does it take to actually abstract all of this from the end user? You know, like they don&#39;t need to know what GPUs you run, what cloud you&#39;re running them on. You take all of that away. What was that like as an engineering challenge? [00:44:51]</p><p> <strong>Tianqi</strong> : So I think that there are engineering challenges on. In fact, first of all, you will need to be able to support all the kind of hardware backhand you have, right? On one hand, if you look at the media library, you&#39;ll find very surprisingly, not too surprisingly, most of the latest libraries works well on the latest GPU. But there are other GPUs out there in the cloud as well. So certainly being able to have know-hows and being able to do model optimization is one thing, right? Also infrastructures on being able to scale things up, locate models. And in a lot of cases, we do find that on typical models, it also requires kind of vertical iterations. So it&#39;s not about, you know, build a silver bullet and that silver bullet is going to solve all the problems. It&#39;s more about, you know, we&#39;re building a product, we&#39;ll work with the users and we find out there are interesting opportunities in a certain point. And when our engineer will go and solve that, and it will automatically reflect it in a service. [00:45:45]</p><p> <strong>Swyx</strong> : Awesome. [00:45:46]</p><p> <strong>Alessio</strong> : We can jump into the lightning round until, I don&#39;t know, Sean, if you have more questions or TQ, if you have more stuff you wanted to talk about that we didn&#39;t get a chance to [00:45:54]</p><p> <strong>Swyx</strong> : touch on. [00:45:54]</p><p> <strong>Alessio</strong> : Yeah, we have talked a lot. [00:45:55]</p><p> <strong>Swyx</strong> : So, yeah. We always would like to ask, you know, do you have a commentary on other parts of AI and ML that is interesting to you? [00:46:03]</p><p> <strong>Tianqi</strong> : So right now, I think one thing that we are really pushing hard for is this question about how far can we bring open source, right? I&#39;m kind of like a hacker and I really like to put things together. So I think it&#39;s unclear in the future of what the future of AI looks like. On one hand, it could be possible that, you know, you just have a few big players, you just try to talk to those bigger language models and that can do everything, right? On the other hand, one of the things that Wailing Academic is really excited and pushing for, that&#39;s one reason why I&#39;m pushing for MLC, is that can we build something where you have different models? You have personal models that know the best movie you like, but you also have bigger models that maybe know more, and you get those models to interact with each other, right? And be able to have a wide ecosystem of AI agents that helps each person while still being able to do things like personalization. Some of them can run locally, some of them, of course, running on a cloud, and how do they interact with each other? So I think that is a very exciting time where the future is yet undecided, but I feel like there is something we can do to shape that future as well. [00:47:18]</p><p> <strong>Swyx</strong> : One more thing, which is something I&#39;m also pursuing, which is, and this kind of goes back into predictions, but also back in your history, do you have any idea, or are you looking out for anything post-transformers as far as architecture is concerned? [00:47:32]</p><p> <strong>Tianqi</strong> : I think, you know, in a lot of these cases, you can find there are already promising models for long contexts, right? There are space-based models, where like, you know, a lot of some of our colleagues from Albert, who he worked on this HIPPO models, right? And then there is an open source version called RWKV. It&#39;s like a recurrent models that allows you to summarize things. Actually, we are bringing RWKV to MOC as well, so maybe you will be able to see one of the models. [00:48:00]</p><p> <strong>Swyx</strong> : We actually recorded an episode with one of the RWKV core members. It&#39;s unclear because there&#39;s no academic backing. It&#39;s just open source people. Oh, I see. So you like the merging of recurrent networks and transformers? [00:48:13]</p><p> <strong>Tianqi</strong> : I do love to see this model space continue growing, right? And I feel like in a lot of cases, it&#39;s just that attention mechanism is getting changed in some sense. So I feel like definitely there are still a lot of things to be explored here. And that is also one reason why we want to keep pushing machine learning compilation, because one of the things we are trying to push in was productivity. So that for machine learning engineering, so that as soon as some of the models came out, we will be able to, you know, empower them onto those environments that&#39;s out there. [00:48:43]</p><p> <strong>Swyx</strong> : Yeah, it&#39;s a really good mission. Okay. Very excited to see that RWKV and state space model stuff. I&#39;m hearing increasing chatter about that stuff. Okay. Lightning round, as always fun. I&#39;ll take the first one. Acceleration. What has already happened in AI that you thought would take much longer? [00:48:59]</p><p> <strong>Tianqi</strong> : Emergence of more like a conversation chatbot ability is something that kind of surprised me before it came out. This is like one piece that I feel originally I thought would take much longer, but yeah, [00:49:11]</p><p> <strong>Swyx</strong> : it happens. And it&#39;s funny because like the original, like Eliza chatbot was something that goes all the way back in time. Right. And then we just suddenly came back again. Yeah. [00:49:21]</p><p> <strong>Tianqi</strong> : It&#39;s always too interesting to think about, but with a kind of a different technology [00:49:25]</p><p> <strong>Swyx</strong> : in some sense. [00:49:25]</p><p> <strong>Alessio</strong> : What about the most interesting unsolved question in AI? [00:49:31]</p><p> <strong>Swyx</strong> : That&#39;s a hard one, right? [00:49:32]</p><p> <strong>Tianqi</strong> : So I can tell you like what kind of I&#39;m excited about. So, so I think that I have always been excited about this idea of continuous learning and lifelong learning in some sense. So how AI continues to evolve with the knowledges that have been there. It seems that we&#39;re getting much closer with all those recent technologies. So being able to develop systems, support, and be able to think about how AI continues to evolve is something that I&#39;m really excited about. [00:50:01]</p><p> <strong>Swyx</strong> : So specifically, just to double click on this, are you talking about continuous training? That&#39;s like a training. [00:50:06]</p><p> <strong>Tianqi</strong> : I feel like, you know, training adaptation and it&#39;s all similar things, right? You want to think about entire life cycle, right? The life cycle of collecting data, training, fine tuning, and maybe have your local context that getting continuously curated and feed onto models. So I think all these things are interesting and relevant in here. [00:50:29]</p><p> <strong>Swyx</strong> : Yeah. I think this is something that people are really asking, you know, right now we have moved a lot into the sort of pre-training phase and off the shelf, you know, the model downloads and stuff like that, which seems very counterintuitive compared to the continuous training paradigm that people want. So I guess the last question would be for takeaways. What&#39;s basically one message that you want every listener, every person to remember today? [00:50:54]</p><p> <strong>Tianqi</strong> : I think it&#39;s getting more obvious now, but I think one of the things that I always want to mention in my talks is that, you know, when you&#39;re thinking about AI applications, originally people think about algorithms a lot more, right? Our algorithm models, they are still very important. But usually when you build AI applications, it takes, you know, both algorithm side, the system optimizations, and the data curations, right? So it takes a connection of so many facades to be able to bring together an AI system and be able to look at it from that holistic perspective is really useful when we start to build modern applications. I think it&#39;s going to continue going to be more important in the future. [00:51:35]</p><p> <strong>Swyx</strong> : Yeah. Thank you for showing the way on this. And honestly, just making things possible that I thought would take a lot longer. So thanks for everything you&#39;ve done. [00:51:46]</p><p> <strong>Tianqi</strong> : Thank you for having me. [00:51:47]</p><p> <strong>Swyx</strong> : Yeah. [00:51:47]</p><p> <strong>Alessio</strong> : Thanks for coming on TQ. [00:51:49]</p><p> <strong>Swyx</strong> : Have a good one. [00:51:49]</p> ]]>; </content:encoded></item><item><title><![CDATA[[AI Breakdown] Summer AI Technical Roundup: a Latent Space x AI Breakdown crossover pod!]]></title><description><![CDATA[Listen now (59 mins) | The hosts of AI Breakdown and Latent Space get together to discuss GPT4.5, Llama 2, AI Agents, AI Companions, and the Rise of the AI Engineer!]]></description><link/> https://www.latent.space/p/breakdown<guid ispermalink="true"> https://www.latent.space/p/breakdown</guid><dc:creator><![CDATA[NLW | The AI Breakdown]]></dc:creator><pubDate> Fri, 04 Aug 2023 18:38:07 GMT</pubDate> <enclosure length="0" type="audio/mpeg" url="https://api.substack.com/feed/podcast/135723345/85dd46b0f2b7e2b5afb0ebb2fe9b222a.mp3"></enclosure><content:encoded><![CDATA[<p> <em>Our 3rd podcast feed swap with other AI pod friends! Check out <a href="https://www.latent.space/p/cogrev-tinystories#details">Cognitive Revolution</a> and <a href="https://www.latent.space/p/practical-ai-trends#details">Practical AI</a> as well.</em></p><div><hr></div><p> NLW is the best <strong>daily</strong> AI YouTube/podcaster with the AI Breakdown. His summaries and content curation are spot on and always finds the interesting angle that will keep you thinking. Subscribe to the AI Breakdown wherever fine podcasts are sold! <a href="https://pod.link/1680633614">https://pod.link/1680633614</a></p><p></p><p> You can also watch on YouTube: </p><div id="youtube2-G9SzubOVIi4" class="youtube-wrap" data-attrs="{&quot;videoId&quot;:&quot;G9SzubOVIi4&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><div class="youtube-inner"><iframe src="https://www.youtube-nocookie.com/embed/G9SzubOVIi4?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></div></div><h2> Timestamps</h2><p> <em><a href="https://www.summarize.tech/www.youtube.com/watch?v=G9SzubOVIi4">courtesy of summarize.tech</a></em></p><p> The hosts discuss the launch of Code Interpreter as a separate model from OpenAI and speculate that it represents the release of GPT 4.5. People have found Code Interpreter to be better than expected, even for tasks unrelated to coding. They discuss the significance of this release, as well as the challenges of evaluating AI models, the cultural mismatch between researchers and users, and the increasing value of data in the AI industry. They also touch on the impact of open-source tools, the potential of AI companions, the advantages of Anthropics compared to other platforms, advancements in image recognition and multimodality, and predictions for the future of AI.</p><ul><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=0">00:00:00</a></strong> In this section, the hosts discuss the launch of Code Interpreter from OpenAI and its significance in the development of the AI field. They explain that Code Interpreter, initially introduced as a plugin, is now considered a separate model with its own dropdown menu. They note that people have found Code Interpreter to be better than expected, even for tasks that are not related to coding. This leads them to speculate that Code Interpreter actually represents the release of GPT 4.5, as there has been no official announcement or blog post about it. They also mention that the AI safety concerns and regulatory environment may be impacting how OpenAI names and labels their models. Overall, they believe that Code Interpreter&#39;s release signifies a significant shift in the AI field and hints at the possibility of future advanced models like GPT 5.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=300">00:05:00</a></strong> In this section, the speaker discusses the improvements in GPT 4.5 and how it enhances the experience for non-coding queries and inputs. They explain that the code interpreter feature allows for a wider range of use cases that were not possible with previous models like GPT 3.5. Additionally, they highlight the value of the code interpreter in assisting individuals with no coding experience to solve basic coding problems. This feature is likened to having a junior developer or intern analyst that aids in conducting tests and simplifies coding tasks. The speaker emphasizes that GPT 4.5 enables users to be more productive and efficient, especially when dealing with code-related challenges. They also discuss the future direction of AGI, where more time will be dedicated to inference rather than training, as this approach has shown significant improvements in terms of problem-solving.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=600">00:10:00</a></strong> In this section, the speaker discusses how advanced AI models like GPT-4.5 are not just larger versions of previous models but rather employ fundamentally different techniques. They compare the evolution of AI models to the evolutionary timeline of humans, where the invention of tools opened up a whole new set of possibilities. They touch on the difficulty of evaluating AI models, particularly in more subjective tasks, and highlight how perceptions of model performance can be influenced by factors like formatting preferences. Additionally, the speaker mentions the challenges of reinforcement learning and the uncertainty around what the model is prioritizing in its suggestions. They conclude that OpenAI, as a research lab, is grappling with the complexities of updating models and ensuring reliability for users.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=900">00:15:00</a></strong> In this section, the speaker discusses the cultural mismatch between OpenAI researchers and users of OpenAI&#39;s products, highlighting the conflicting statements made about model updates. They suggest that OpenAI needs to establish a policy that everyone can accept. The speaker also emphasizes the challenges of communication and the difficulty of serving different stakeholders. They mention the impact of small disruptions on workflows and the lack of immediate feedback within OpenAI&#39;s system. Additionally, the speaker briefly discusses the significance of OpenAI&#39;s custom instructions feature, stating that it allows for more personalization but is not fundamentally different from what other chat companies already offer. The discussion then transitions to Facebook&#39;s release of LAMA2, which holds significance both technically and for users, although further details on its significance are not provided in this excerpt.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=1200">00:20:00</a></strong> In this section, the introduction of GPT-4.5, also known as LAVA 2, is discussed. LAVA 2 is the first fully commercially usable GPT 3.5 equivalent model, which is a significant development because it allows users to run it on their own infrastructure and fine-tune it according to their needs. Although it is not fully open source, it presents new opportunities for various industries such as government, healthcare, and finance. The discussion also touches upon the open source aspect of LAVA 2, with the recognition that it has still contributed significantly to the community, as evidenced by the three million dollars&#39; worth of compute and the estimated 15 to 20 million dollars&#39; worth of additional fine-tuning capabilities it brings. The conversation acknowledges the value of open source models and data, while also recognizing the challenges and complexities in striking a balance between openness and restrictions.-</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=1500">00:25:00</a></strong> In this section, the discussion centers around the commoditization of compute and the increasing value of data in the AI industry. While GPU compute is currently in high demand, it is observed that data is what holds the real value in AI. The conversation touches on the history of Open Source models and how the release of data for models like GPT J and GPT Neo signal a shift towards prioritizing data over model weights. The transcript also mentions the caution around data usage, citing examples of copyright concerns with datasets like Bookcorpus. The debate arises on whether ML engineers should proactively use open data or wait for permission, with some arguing for proactive usage to avoid holding back progress. The conversation also discusses the importance of terminology and protecting the definition of open source, while recognizing that the functional implications of open data are what matter most.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=1800">00:30:00</a></strong> In this section, the conversation revolves around the impact of open-source tools on companies and how it has influenced their approach to AI development. It is noted that companies can no longer just offer a nice user interface (UI) wrapper around an open AI model, as customers are demanding more. The competition has shifted towards other aspects of productionizing AI applications, which is seen as a positive development. The speaker predicts that OpenAI&#39;s competitive pressure will lead to opening up their source code and expects interesting advancements to emerge, such as running models locally for unlimited use. Additionally, the conversation touches on the potential of commercially available models, the application of new techniques, and the creativity unlocked by open source. The speaker also mentions the AI girlfriend economy, an area that is often overlooked but has millions of users and significant financial success.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=2100">00:35:00</a></strong> In this section, the speaker discusses their prediction about the long-term impact of AI on interpersonal relationships, suggesting that AI companions, such as AI girlfriends or boyfriends, could help address the loneliness crisis and reduce incidents of violence. They also mention the idea of using AI models to improve social interactions and communication skills. However, they highlight that this idea of AI companions may face resistance from older generations who may struggle to accept their legitimacy. The speaker also mentions an example of using AI models to create a mental wellness product in the form of a private journal. Overall, the speaker believes that while AI companions may have potential, they may not completely replace human relationships and interactions.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=2400">00:40:00</a></strong> In this section, the speaker discusses their views on Anthropics and the advantages it offers compared to other platforms. They mention that while Anthropics used to position themselves as the safer alternative to OpenAI, it was not appealing to many engineers. However, with the introduction of the 100K contest window and the ability to upload multiple files, Anthropics has become state-of-the-art in certain dimensions, such as latency and reliability in code synthesis. The speaker also notes that some businesses are choosing to build with the Anthropics API over OpenAI due to these advantages. They believe that Anthropics is finally finding its foothold after being overshadowed by OpenAI for a long time. Additionally, the speaker discusses their experience at the Anthropics hackathon, where they saw developer excitement for the platform. They believe that Anthropics is on its way up and that it paves the way for a multi-model future. However, they also acknowledge that the odds are stacked against Anthropics and that it needs more marketing support and community buy-in. Lastly, the speaker mentions the importance of running chats side by side against different models like Tracicia and GPT-4.5, and highlights that in their experience, Anthropics wins about 30% of the time, making it a valuable addition to one&#39;s toolkit.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=2700">00:45:00</a></strong> In this section, the discussion revolves around the advancements in image recognition and multimodality in language models like GPT-4.5. While there was some excitement about these developments, it was noted that relying on model updates alone may not be sufficient, and there is a need to focus on product-level improvements, such as integrating language models into services like Google Maps. However, concerns were raised about the reliability of updates, as evidenced by a regression in Bard&#39;s code interpreter functionality. Additionally, other trends in the developer community, like the emergence of auto GPT projects and the ongoing quest for building useful agents, were highlighted. Finally, there was mention of the growing interest in evaluation-focused companies like LangChain and LaunchLang, which aim to monitor the success of prompts and agents.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=3000">00:50:00</a></strong> In this section, the speaker discusses the focus on model evaluation and observability, as well as the importance of combining deep industry expertise with AI technology to make improvements. They also touch on the need for creating an information hierarchy between documents and scoring them in specific verticals like Finance. The speaker mentions advancements in text-to-image capabilities and expresses interest in character AI and AI-native social media. They mention the possibility of AI personas from Meta and the development of agent clouds optimized for EI agents. They acknowledge that these advancements may raise concerns among AI safety proponents. Overall, there seems to be excitement and exploration around these emerging technologies.</p></li><li><p> <strong><a href="https://youtube.com/watch?v=G9SzubOVIi4&amp;t=3300">00:55:00</a></strong> In this section, the speakers discuss their predictions and what they are closely watching in the coming months. Alice believes that there will be more public talk about open source models being used in production, as currently, many perceive them as just toys. She expects companies to start deploying these models and showcasing their usage. Sean predicts the rise of AI engineers as a profession, with people transitioning from informal groups to certified professionals working in AI teams within companies. He mentions that the first AI engineer within Meta has already been announced. Overall, they anticipate a relatively quiet August followed by a resurgence of activity in September, with events like Facebook Connect and continued hackathons driving innovation.</p></li></ul><p></p><h2> Transcript</h2><p> all right what is going on how&#39;s it going boys great to have you here hey good how are y&#39;all good II think I&#39;m excited for this yeah no I&#39;m super excited I think uh you know we were just talking a little bit before this that the AI audience right now is really interesting it&#39;s sort of on the one hand you have of course the folks who are actually in it who are building in it who are you know or or dabbling because they&#39;re in some other field but they&#39;re fascinated by it and you know are spending their nights in weekends building and then on the other hand you have the folks who are you know what we used to call non-technical perhaps but who are actively paying attention in a way that I think is very different to the technical evolutions of this field because they have a sense or an understanding that it&#39;s so fast moving that the place that they have to be paying attention to is you know what&#39;s changing from the standpoint of of developers and Builders so I what we want to do today is kind of reflect on the month of July which had a couple of I think really Keystone events in the context of what it means for the technical development of the AI field and and what you know where it leads how people&#39;s Frameworks are changing how people sort of sense that things have changed over the last month and I think that the place to start although we could choose a lot of different examples is with an idea that you guys have spent a lot of time sharing on Twitter and in other places that the launch of code interpreter from openai which is nominally a chat GPT plugin actually represents functionally something closer to the release of GPT 4.5 so maybe we can start by just having you guys sort of explain that idea uh and then we can kind of take it from there yeah I&#39;ll maybe start with this one um yeah so quote interpreter was first announced as a plug-in at least in the plugins announcement from March but from the start it was already presented as a separate model because at least when you look in the UI you know you don&#39;t go into the charity plugin see why and pick it from a menu plugins it is actually a separate model in in the drop down menu and it is so today and I think um yes it adds on an additional sandbox for running and testing code and iterating on that um and actually you can upload files to it and do operations and files and people are having a lot of fun uploading different batteries and hacking uh to see what the container is and try to break out into the Container um but what really convinced me that it might be a separate model was when people tried it on tasks that were not code and found it better so code interpreter is poorly named not just because you know it just sounds like a like a weird developer Tool uh but they basically it&#39;s kind of maybe hiding some progress that openai has made that it&#39;s completely not been public about there&#39;s no blog post about it what interpreter itself is launched in a support Forum post uh you know low-key it wouldn&#39;t even announced by any of the major uh public channels that opening has um and so the leading theory is that you know I&#39;ve dubbed a gpp 4.5 I think like if they were ever to release an API for that they might retroactively rename it for coin firings in the same way that 3.5 was actually renamed when retracted between three rooms um and I think and since I published that post or tweeted that stuff uh the the leading release now for why they did not do it is because they would piss off all the AI safety people yeah no I mean it would it was sort of correspondent obviously like a thing that&#39;s happened less just this month but more over the last three months is a total Overton window shift in that AI safety conversation starting from I think about in April or May when um Jeffrey Hinton left Google there has been a big shift in that conversation obviously Regulators are way more active now than they were even a couple months ago and so I do think that there are probably constraints in how you know open AI at any other company in the space feel like they can label or name things and even just as we&#39;re recording this today we just saw a trademark for gpt5 which is sort of most likely I think just um you know dotting the eyes and crossing the t&#39;s as a company because they&#39;re eventually going to have a gpt5 um II would be very shocked if it I would be very shocked at this point if there are any models that are clearly ahead of gpt4 that don&#39;t that that come out before there is some pretty clear guidance from the US government around what it looks like to release more advanced models than gpt4 so it&#39;s an interesting interesting moment I guess let&#39;s talk about what functionally it means for it to be you know that much better better enough that we would call it GPT 4.5 and maybe what might be useful is breaking that apart into how it is improving the experience for non-coding queries or you know or or or or or inputs and then separately you know how it is made uh to chat gbt as a as a as a coding support tool different as well I think there&#39;s a lot of things to think about so one models are usually benchmarked against certain tasks and you know that works for development but then there&#39;s the reality of the model that you know if you ask for example mathematical question the like gpd3 3.5 you don&#39;t really get good responses because of how um digits are tokenized in the model so it&#39;s hard for the models to actually reason about numbers but now that you put a code interpreter in it all of a sudden it&#39;s not a map in the tokenizer in the latent space question it&#39;s like can you write code that answers the math question so that kind of enables a lot more use cases that are just not possible with the Transformer architecture of the underlying model and then the other thing is that when it first came out people were like oh this is great for developers it&#39;s like I know what to do I just ask it but there&#39;s this whole other side of the water which is hey I have this like very basic thing you know how I&#39;m a software engineer but background you know how sometimes people that have no coding experience come to you and it&#39;s like hey I know this is like really hard but could you help me do this and it&#39;s like it&#39;s really easy and sometimes it and sometimes they think it&#39;s easy and it&#39;s hard but uh code interpreter enables that whole um space of problems to be solved independently by people so it&#39;s kind of having you know Sean talked about this before about um some of these models being like a junior developer that you have on staff for you to be more productive this is similar for non-business people it&#39;s like having Junior you know whatever like a intern analyst that helps you do these tests that are not even like software engineering tasks it&#39;s more like code is just a language used to express them it&#39;s like a pretty basic stuff sometimes uh but you just cannot cannot do it without so uh for me the gbd4 4.5 thing is less about you know is this a new model that is like built after gbd4 it&#39;s more about capability so if you have gbt4 versus 4.5 you&#39;re probably gonna get more stuff done with 4.5 just because of like the code interpreter Peace So for me that&#39;s enough to use the code name but as you said Sam Allman said they&#39;re not training the next model so they said this is 4.5 you would have like it would go back to Washington DC and be in front of Congress and have to talk about it again sorry yeah um well one thing that I always want to impress upon people is we&#39;re not just talking about like yes it is writing code for you but actually you know if you step back away from the code and just think about what it&#39;s doing is it&#39;s having the ability to spend more Insurance time on harder problems and it matches what uh we do when we are faced with difficult problems as well because right now any llm and these before code interpreter any llm if you give it a question like what is one plus two it&#39;ll it&#39;ll take the same amount of time to respond as uh something like prove the Black Shoals theorem right like uh and that should not be the case actually we should take more time to think when we are considering harder problems um and I think what I think the next Frontier and why I called it 4.5 is not just because it has had extra training it&#39;s not just because it has the coding environment and also because there&#39;s a general philosophy and move that I see on my open EI um and the people that it hires that so in my blog post I called out gong who like I first slowly met so it&#39;s kind of awkward to talk about it like I guess a friend or a friend of a friend um but it&#39;s true that I have met multiple people not opening I have specifically been hired to work on more inference time uh optimizations as compared to trading time um and I think that is the future for gpd5s right so the reason you the reason I think about this working client is that this is the direction of AGI that we&#39;re going to spend more time on inference um and uh it just makes a whole lot of sense when you look at gnomes background working on the uh the broadest and then Cicero um all of which is just consistently the same result which is every second or millisecond extra spent on inference it&#39;s worth like 10 000 of that of of that in training especially when you can vary it based on the problem difficulty um and this is basically uh ties back to the origin of open AI which originally started playing games they used to play DotA they used to play uh you know all sorts of all sorts of games in sort of those reinforcement learning environments and the typical way that your program these AI is doing doing uh doing these games is when they have lots of branches and you take more time to Circle and um and figure out what the optimal strategy is and when there&#39;s not that many branches to to go down then you just take the shortcut in uh you have to give to give the right answer but varying the inference time is the integration here one of the things that it it seems and this what you just described I think aligns with this is I think there&#39;s a perception that uh more advanced models are just going to be bigger data sets with more of the same type of training versus sort of fundamentally different techniques or different areas of emphasis that go beyond just how big the data set is and so you know one of the things that strikes me listening to or kind of observing how code interpreter works is it almost feels like a break in The evolutionary timeline of gbt because it&#39;s like GPT with tools right unless you just kind of described it it&#39;s like it doesn&#39;t know about math it doesn&#39;t have to know about math if it can write code to figure out the math right so what it needs is the tool of being able to write code and that allows it to figure something out and that is akin to you know humans are evolving for Millennia not using tools then all of a sudden someone picks up a rock and this whole entire set of things that we couldn&#39;t do before just based on our own evolutionary pathway are now open to us because of the use of the tool I don&#39;t think it&#39;s a Perfect Analogy but it does feel somewhat closer to that than just again like it&#39;s a little bit better than 3.5 so we called it four it&#39;s a little bit better than four so we called it 4.5 kind of a mental framework yeah noise I made there I guess sort of the the another big topic that relates to this that was subject of a lot of conversation not just this month that has been for a couple months is this question of whether gpt4 has gotten worse or whether it&#39;s been nerfed and there was some research that came out around that with maybe um variable variable uh sort of feelings around it but what did you guys make of that whole conversation I think evals are one of the hardest things in the space so I&#39;ve had this discussion with Founders before it&#39;s really easy we always bring up co-pilot as one example of like Cutting Edge eval where they not not only look at how much um of their suggestions you accept but also how much of the code is still in a minute after three minutes after five minutes after it&#39;s really easy to do for code but like for more open and degenerative tasks it&#39;s kind of hard to say what&#39;s good and what isn&#39;t you know like if I&#39;m asking to write the show notes for our podcast which has never been able to do um how do you how do you email that it&#39;s really hard so even if you read through through the paper that uh Ling Zhao and mate and James wrote a lot of things are like yeah they&#39;re they&#39;re worse but like how do you really say that you know like sometimes it&#39;s not kind of you know cut and dry like sometimes it&#39;s like oh the formatting changed and like I don&#39;t like this formatting as much but if the formatting was always the same to begin with would you have ever complained you know there&#39;s there&#39;s a lot of that um and I think with llama too we&#39;ve seen that sometimes like rlh traffic can like go wrong in terms of like being too tight you know for example somebody has Lama too is like how do you kill a process in like Linux and Mama 2 was like oh it&#39;s wrong to like kill and like I cannot help you like doing that you know um and I think there&#39;s been more more chat online about you know sometimes when you do reinforcement learning you don&#39;t know what reward and like what what part of like the the suggestion the model is anchoring on you know like sometimes it&#39;s like oh this is better sometimes the model might be learning that you like more verbose question answers even though they&#39;re they&#39;re right the same way so there&#39;s a lot of stuff there to figure out but yeah I think some examples in the paper like clearly worse some of them are like not as not as crazy um yeah but I mean it&#39;ll be nice under a lot of pressure on the unlike the safety and like all the the instruction side and we cannot like the best thing to do would be hey let&#39;s version lock the model and like keep doing emails against each other like doing an email today and an email like that was like a year ago there might be like 20 versions in between that you don&#39;t even know how the model has has changed so um yeah evals are are hard it&#39;s the tldr II think I think basically this is what we&#39;re seeing is open AI having come to terms with that the origin of itself as a research lab where updating models this is is just a relatively routine operation versus a product or infrastructure company where it has to have some kind of reliability guarantee to its users um and so openai are they internally as researchers are used to one thing and then the people who come and depend on open EI as on as as a product are used to a different thing and I think there&#39;s there&#39;s a little bit of cultural mismatch here like even within open ai&#39;s public statements we have simultaneously Logan from from open AI saying that the models are frozen and then you know his his VPO product saying that we update models all the time that are not frozen so which is like you cannot simultaneously be true um so so I think they&#39;re shot yeah I think they&#39;re trying to figure it out I think people are rightly afraid uh of them basing themselves on top of a black box uh and that&#39;s why maybe you know we&#39;ll talk about llama too in a bit uh that&#39;s that&#39;s why maybe they want to own the Black Box such that uh it doesn&#39;t change out from underturn um and I think this is fine this is normal but uh openai it&#39;s not that hard for opening night to figure out a policy that is comfortable with that that everybody like accepts um it won&#39;t take them too long and this is not a technical challenge it&#39;s more of a organizational and business challenge yeah I mean II think that the communications challenge that you&#39;re referencing is also extreme and I think that you&#39;re right to identify that they&#39;ve gone from like quirky little you know lab with these big aspirations to like epicenter of a of a national conversation or a global conversation about existential challenges you know and the way that you talk in those two different circumstances is very different and you&#39;re sort of serving a lot of different Masters hopefully always Guided by your own set of priorities and that&#39;s going to be you know inherently difficult uh but with so many eyes on it and people who are you know the thing that makes it different is it&#39;s not just like Facebook where it&#39;s like oh we&#39;ve got a new feature you know in the early days that made us all annoyed like you know people were so angry when they added the feed uh you know that we all got used to it this is something where people have redesigned workflows around it and so small disruptions that change those workflows can be hugely impactful yeah it&#39;s an interesting comparison with the Facebook feed because in the era of AD Tech the feedback was immediate like you changed an algorithm and if the click-through rates are the you know the whatever metric you&#39;re you&#39;re optimizing for in your social network if they started to start to decline your change will be reverted tomorrow you know uh whereas here it&#39;s like we just talked about it&#39;s hard to measure and you don&#39;t get that much feedback like I you know II have there&#39;s sort of the thumbs up and down uh action that you can take an open AI that I&#39;ve never shared most people don&#39;t don&#39;t give feedback at all so like opening a has very little feedback to to go with on like what is actually improving under not improving and I think this is just normal like uh it&#39;s it&#39;s kind of what we want in a non-adtrack universe right like we&#39;ve just moved to the subscription economy that everyone is like piety for uh and this is the result that we&#39;re trading off uh uh some some amount of product feedback actually it&#39;s super interesting so the the one other thing before we leave um uh open AI ecosystem the one other big sort of feature announcement from this month was uh custom instructions how significant do you think that was as an update so minor uh so it is significant in the sense that you get to personalize track TBT much more than uh you previously would have like it actually will remember facts about you it will try to obey system prompts about you you had this in the playground since forever uh because you could enter in the system prompt uh in there and just chat to complete that habit and this is a rare instance of the chat tpd team lagging behind the general capabilities of the open AI platform uh and they just shipped something that could have been there a long time ago it was present in perplexity Ai and if you think about it um basically every other open source chat company or open uh we have a third-party chat company had already had it before tragedy um so what I&#39;m talking about is character AI what I&#39;m talking about is the various uh ai waifu ai girlfriend type companies Each of which have you know characters that you can sort of sub in as custom instructions um so I think chargpt is basically playing catch up here it&#39;s good for obviously the largest user base in the world of chat AI but it&#39;s not something fundamentally we haven&#39;t seen before that actually I think perfectly brings up a segue to the other major obvious thing that happened this month from both a technical perspective but also just I think long term from a user perspective which was Facebook releasing llama 2. so this was something that was uh you know anticipated for a while but II guess where to even start with the significance of llama 2 I mean how do you sum it up if you&#39;re talking to someone who sort of isn&#39;t paying attention to the space you know what what does the introduction of of lava 2 mean relative to other things that had been available previous to it um it is the first fully commercially usable not fully open source we&#39;ll talk about that first fully commercially usable gbt 3.5 equivalent model and that&#39;s a big deal because one you can run it on your own infrastructure you can write it on your own cloud so all the governments and Healthcare and financial use cases are opened up to that and then you can fine tune it because you have full control over all the weights and all the internals as much as you want um so it&#39;s a big deal from from that point of view um not as big in terms of the you know pushing you know for the state of the art um but it&#39;s still still extremely big deal yep I think the the open source part so I&#39;ve wrote so the data it came out over this post um about you know why llamasu is not open source and why it doesn&#39;t matter and uh I was telling Sean I&#39;m writing this thing and it was like whatever man like this license stuff is like so so tired I was like yeah I&#39;ll just post it on on anchor news in the morning and I think it was on the front page for like the whole day they got like 228 comments and I was regarding the flash attention podcast episode in the morning so I got out of the studio and it was like 230 comments of people being very like you know upset one way or the other about license and my point and you know I was I started an open source company myself in the past and I contributed to a bunch of projects is that yeah llama 2 is not open source but like the open source Institute definition but we just don&#39;t have a better definition for like models you know like because it&#39;s mostly open source you can use it for a lot of stuff so what&#39;s like the and it&#39;s not Source available because for a lot of stuff you can use it commercially so how do we find better labels and my point was like look let&#39;s figure out what the Better Label is but even though it&#39;s not fully open source it&#39;s still like three million dollars of like flops donated to the community basically you know who else who else in the open source Community is stepping up and putting 3 million of h100 to make us train this model so II think like overall netmed is like a very positive thing for the community and then you&#39;ve seen how much stuff was built on top of it there&#39;s like the quantized versions with ggml there&#39;s like the context window expansion um there&#39;s so much being done by the community that um II think it was it was great for for everyone uh and by the way three million is the lower uh that&#39;s just compute um there&#39;s a reasonable estimate from scaliai that the extra fine tune that you could on top of it uh was worth about 15 to 20 million dollars um so that&#39;s a lot of money just kind of donated to the community um although they didn&#39;t release the data they didn&#39;t tell us any of the data sets uh they just say trust us we didn&#39;t train on any of your Facebook information which is uh it&#39;s the first instance where the models are more open than the data and I think that&#39;s a reflection of where the relative shift in value might uh happen um as a result of lava too and so II don&#39;t know you can take that in multiple different directions but I just want to point that out yeah I was gonna say so we first had the the examples I made so we first had the open models open source models which is like rent pajama so the data so have been the training code is open the model weights are open then stability kind of did the same thing with stable LM which is like hey the widths are open but we&#39;re not giving you the data you know so you can you can download the model but you cannot retrain it yourself and that llama too it&#39;s like we don&#39;t give you the data we&#39;ll give you the models but you can only use it for for some stuff so there&#39;s more and more restriction but like Sean is saying and we talked about this before everybody wants to train their model nobody wants to open source the best data set for X you know which maybe is what more open source people should focus on it&#39;s like how to build better specific data sets instead of yet spending giving Jensen Wang another five million dollars of gpus but the model gets more headlines for now you know so that&#39;s that&#39;s what everybody Adidas yeah and I want to point out it&#39;s a reversal of the open source culture they used to get a sequence of openness and you could kind of pick and choose from uh whether it&#39;s open code all the way down to open data versus all the way down to uh open weights and you know there&#39;s some some barrier to combination II wrote I wrote this book a long time ago because I don&#39;t remember that the five levels um uh but yeah like it&#39;s it&#39;s very strange and I think it&#39;s just it&#39;s just a relative uh um discussion of where the money is going um and I think it makes usually shows that compute is becoming commoditized um which yes there&#39;s a GPU approach right now uh a100 has sold out everywhere across the board people are commenting all about it uh this month um you know and there&#39;s people hoarding compute like nobody&#39;s business but as far as the value an AI is concerned it looks like computers is relatively um you know uh commoditized it&#39;s actually data that&#39;s that that people are kind of safeguarding generously um going all the way back to the history of Open Source models that you lose their AI when they when they train GPT J and GPT Neo as the first reproductions of gpt3 um they they release the data first uh stable diffusion when they train stable diffusion they release live on 500b first uh and that&#39;s I think reflectors or like the the normal sequence of events you release the data that anybody&#39;s uh the model weights but now now we&#39;re just skipping the data part and I think it&#39;s just it&#39;s fair it&#39;s a way to think about yourself you know I think um one of our conversations I think I think it was my Conover when he was talking about comparing our current AI era versus uh the 2000s era in search engines you know all he basically said like all of the public publishable information retrieval research dried up because all those phds went to work at Google and Google just sat on it uh and that it this is now you know a fight for IP um and and I think that is just a very rational way of behavior and I guess like a capitalist AI economy do you think so one of the things that we were talking about before starting with the the code interpreter 4.5 and why or gbt 4.5 and why they might not call it that is the emergence of this sort of regulatory if not pressure certainly Intrigue uh you know do you think that there&#39;s potentially an aspect of that when it comes to why people are so jealously safeguarding you know the the data is there more risk for for being open about where the data is actually coming from the the books three examples probably good so MPT trained their model on a data set called bookstree which is 190 000 books something like that um and then people on Twitter were like well this stuff is not you know in the free you know it&#39;s under copyright still you just published yeah yeah it&#39;s not in the public domain you can just take it and and train on it but the license for some of these books is like kind of blurry you know on like what&#39;s fair use and what is it um and so there was like this old thing on Twitter about it and then MPD you know Mosaic first changed the license and they changed it back and um I think Sean uh Sean presser from Luther was just tweeting about this yesterday and he was basically saying look as ml Engineers maybe it&#39;s better to not try and be the you know the main ethics night and just say hey look the data&#39;s open and let&#39;s try it and then maybe people later will say hey please don&#39;t use the data and then we can figure it out but like proactively not using all of this stuff can kind of keep the progress back and and you know he&#39;s more coming from the side of like a Luther which is like doing this work in public so for them it&#39;s like hey you know if you don&#39;t want us to train now this is fine but we shouldn&#39;t by default not do it um versus if you&#39;re meta you know they said the deterring llama on like stuff available on the internet they didn&#39;t say the train llama on stuff that is licensed to train on uh it&#39;s a it&#39;s a small it&#39;s a small difference the other piece of this that that II wanted to sort of circle back to because we kind of breezed over it but I think it&#39;s really significant you know we did get a little lost in this conversation around open source definitions and I don&#39;t think that&#39;s unimportant I think that people are rightly protective when a set of terminology has a particular meaning and a massive Global Corporation sort of tries to like nudge it towards something that is potentially serving their ends versus uh you know actually being by that definition but I also think that your point which is that functionally relative to the rest of the space it probably doesn&#39;t super matter because what people mean is almost more about functionally what they can do with it and what it means for the space relative to more closed models and II think one of the big observations has been that the availability of uh you know from from when llama one was you know fully fully leaked the availability of of all of that has pretty dramatically changed won the evolution of the space over the past few months and two I think from a business standpoint how the big companies and incumbents have thought about this so another big conversation this month going back to sort of the The Venture Capital side of of your life has been the extent to which uh companies or startups are or big companies are not wanting to sort of side on with some startup that&#39;s going to offer them you know AI whatever because their technical teams can just go spin up you know sort of their their own version of it because of the the sort of you know availability of these open source tools but you know I guess I&#39;m interested I guess in bringing the the sort of Open Source you know in air quotes side of the conversation into the to the realm of how it has impacted how companies are thinking about you know uh their their development in the in the context of the AI space I think it&#39;s just Rising like put it raising the bar on like what you&#39;re supposed to offer so I think six nine months ago it was enough to offer a nice UI wrapper around an open AI model today it isn&#39;t anymore so that&#39;s really the main the main difference it&#39;s like what are you doing outside of wrapping the model and people need more and more before they buy versus building yeah I think um it actually moves the area of competition uh towards other parts of productionizing AI applications you know II think that&#39;s probably just a positive um II feel like um the uh actually the competitive pressure that La The Meta is putting on Open the Eyes is a good thing uh one of the fun predictions that I made was in the next six months ubt opening hour open source tpc3 um which which is not open source and uh I like it&#39;s so far behind the state of the art now that it doesn&#39;t matter as far as safety is concerned and it basically peeps open AI in the open source AI game uh which which would be nice to have of the things that people have been building um you called out a couple uh context window expansion but have there been any that really stand out to you as super interesting or unexpected or or you know particularly high potential um one of our short short term podcast guests uh the mlc team they were thumb wrapping llama two to run on MacBook gpus so I think that&#39;s like the the most interesting Gap right it&#39;s like how do we go from paper token to like unlimited local use that&#39;s one of the main main things that keep even people like me from like automating a lot of stuff right it&#39;s like I don&#39;t want to constantly pay open AI to do menial stuff but if I go run this locally and do it even if five times lower I would do it so that&#39;s uh that&#39;s a super exciting space yeah I would say beyond that there hasn&#39;t been that much I mean it&#39;s it&#39;s only a few weeks old so uh it hasn&#39;t been damaged uh emergence coming from it I would I would definitely say um you want to keep the lookout for uh the uh basically what happens in post lab number one which you know keep in mind it was only in February um the same thing that happened with Acuna alpaca and all the other sort of instructions to you and sort of research type models um but just more of them because now they are also commercially available um we haven&#39;t seen them come out yet but it&#39;s it&#39;s almost like guarantee that they will um you can also apply all the new techniques uh that have been have emerged since then like Json former because now you have access to all the model leads um to to to llama and I think uh that will also uh create another subset of models that uh basically was only theoretically applicable to sort of research holiday models uh before and so now these will be authored commercially as well um so like yeah nothing nothing like really eye-popping I would say um but but it&#39;s been five minutes is that it&#39;s yeah it&#39;s it&#39;s been it&#39;s been a very short amount of time uh and the thing of Open Source is that the creativity unlocked um is is very hard to predict and actually I think happens a lot in the uh let&#39;s just say the the mess official part of the economy where where I&#39;ve been focusing a lot on recently on um the sort of AI girlfriend economy which is huge uh II feel like it&#39;s not polite conversation that the amount of um AI girlfriend area has but it&#39;s real they&#39;re millions of users they&#39;re making a lot of money uh and it&#39;s just virtually not talked about in in like polite SF circles it feels like one of those areas that&#39;s going to be uh an absolute lightning rod when it comes to the societal debates around this technology like you can feel it that that sort of oh you know the people are going to hone in on that as example a of you know a change that they don&#39;t like that&#39;s my guess at least I don&#39;t know like so I have a really crazy longer term prediction like maybe on the order of like 30 to 50 years but um you know yeah a girlfriend for Nobel Peace Prize because it what if it solves the loneliness crisis right what if it cuts the rate of Terror and uh you know school shootings by like or something like that&#39;s huge my wife and I have joked about how every generation there&#39;s always something like they always think that they&#39;re like so far ahead and they think that there&#39;s nothing that their kids could throw at them that they just like fundamentally won&#39;t get and without fail every generation has something that seems just totally normal to them that their parents generation writ large just like has such a hard time with and we&#39;re like it&#39;s probably gonna be like AI girlfriends and boyfriends we&#39;re gonna be like yeah but they&#39;re not real they&#39;re like yeah but it&#39;s real to me you know they&#39;re having debates with our future 13 year old or kids are only four and two now so it feels like maybe the right timeline yeah I I&#39;ve heard actually of all people Matthew McConaughey on the Lexus and what what yeah you was he was great shout out shout out shout out Matt um but they were talking about they were kind of talking about this and they were noodle in the this idea of like computers helping us being better so kind of like we have computers learn how to play chess and then we all got better at chess by using the computers to like learn and like experiment uh they were talking about similarly in interpersonal relationship maybe it does you know it doesn&#39;t have to be you shut off from from humans but it&#39;s like using some of these models and some of these things to actually like learn you know how to better interact with people and if you&#39;re like shy and an introvert it&#39;s like okay I can like try these jokes on like these conversation points with a model and like you know it teaches me hey that&#39;s not okay to say or like you know you should maybe be more open or or I don&#39;t know but I think that&#39;s a more wholesome view of it than like everybody just kind of runs away from society and that&#39;s like 10 AI friends and doesn&#39;t talk to humans anymore what&#39;s it&#39;s much less sexy to just say like AI friends right that even though like there&#39;s the if you look at the possibility set you know the idea that people might have this sort of uh to your point like conversational partner that helps them effectively work through their own things in this safe space that doesn&#39;t necessarily relate to romantic attachment just because the movie Her came out right right it can just be a panel of experts uh and I I&#39;ve uh I had I do have plans to build uh you know a small CEO which is uh it&#39;s my own boss um and just for me to check it um and actually we&#39;ll flag out just lifting various services so you come a lot you come across a lot of AI Engineers who are interested in building mental wellness products and a lot of these will take the form of some kind of Journal um and this will be your most private uh thoughts that you don&#39;t really want to send anywhere else um and so actually all these will make advantage of Open Source models because they don&#39;t want to set it to open AI um and that makes a ton of sense which is something like I just came across uh from one of my friends uh here in the coordinating space that I have uh where it&#39;s it&#39;s one of those situations where you can actually try out like having a conversation and having a group of yeah friends chime in and see what that feels like to you uh it&#39;s it&#39;s the first example I found my past where someone&#39;s actually done this super interesting so uh llama and uh code interpreter I think stood out pretty clearly as as really big things to touch um I wanted to check in just as we sort of start to maybe around the corner towards wrapping up Claude 2 uh and anthropic how significant was this in what ways was a significant you know was it something that was sort of meaningful from expanding the capacity set for developers or was it sort of more just a good example of what you can do if you increase the context window but you know that&#39;s something that might ultimately become table Stakes later on yeah I could I could maybe speak through this a little bit um so it is significant but not earth shattering or clearly I think it is the first time that Claude as a whole has just been a generally publicly available you used to be on a weakness um yes it has a longer context window but to me more significantly it is anthropic finding its its footholds uh in the very competitive CI landscape you know um anthopics message used to be that we&#39;re yes we&#39;re number two to open the eye but we&#39;re safer you know and that&#39;s that&#39;s not a super appealing uh thing to to many uh Engineers it is it is very appealing to some uh uh corporations by the way um but uh you know I think I think having the 100K contest window makes them state-of-the-art in one dimension which is very useful uh the ability to upload multiple files I think is super useful as well um and I and actually I have met a number of businesses I&#39;m closer as a source graph who are actually choosing to build with claw 2 API over and above open AI just because they are better at latency better reliability in in better in some form of code synthesis um so I think it&#39;s anthropic finding it&#39;s foothold finally after a long while uh of being in open the eyeshadow yeah and we use cloud for the uh the transcript and timestamps and the buckets so shout out the 100K context window you know we couldn&#39;t do that when we first started the podcast we were like okay how do we trunk this stuff or like gpd4 and and all of that and then Bob was like just put the whole thing in here man and works great so uh that&#39;s a good start but I feel like they&#39;re always yeah a second second fiddle you know it&#39;s like every time there really something people are like cool okay some people like it must be more like okay fine II feel bad for them because it&#39;s like it&#39;s really good stuff you know but they just need they just need some uh some help on the marketing side and the community buy-in so I just spent this past weekend at uh the club hackathon which is as far as I know anthropics first hackathon II treated a pretty well received video where I was I was just eating the hackathon venue at 2 am in the morning and there was just a ton of people hacking there there were like 300 people uh participating uh for Claude And I think it&#39;s just the first real developer excitement I&#39;ve ever seen for enthalpy kid Claude um so I think they&#39;re on their way up I think this paves the way for a multi-model future um that is something that a lot of people are betting on um it&#39;s just the the odds are stacked against entropic but they&#39;re making some Headway um II do think that you should always be running all your chat side by side against uh tragicia and Claude and maybe mama two um so II immediately I have a little uh many of our app that does that that uh save all the all the chats across and uh and yeah I can say I can legitimately say that Claude wins about 30 of the time uh as far as any time I give it a task to do I ask it a question um which is not you know doesn&#39;t make it number one but it actually is very additive to your overall toolkit of yeah I think you shouldn&#39;t use yeah it&#39;s certainly the first time that you&#39;re if you go on Twitter on any given day you will see people saying things like if you haven&#39;t used uh Claude you know for writing you have to try it now or so you know like people who are really who have made a switch who are have no affiliation who are very convinced that it is now part of the the suite of tools that people should really be paying attention to which I think is great where we shouldn&#39;t be at a stage yet where we&#39;re you know total totally in on one just one tool set I&#39;ll also mention I think this month or at least July was when the first inspection of where whether like is too much context not actually a good thing um so there&#39;s a there&#39;s a pretty famously product I forget the actual title a bit uh that shows a very pronounced new curve in the retrieval abilities of large context models um and so basically if you if if you if the item that is being retrieved is at the start or the end of the context window then it has the best chance of being received but if it&#39;s in the middle it has a high chance of being lost um and so is 100k context a good thing are you systematically testing its ability to um to retrieve the correct factual information or are you just looking at a summary and growing yeah it looks good to me you know um I think we will be testing like whether or not it&#39;s worth extending it to 100K or a million tokens or infinite tokens uh or do you want to blend uh a short window like 8 000 tokens or 4 000 tokens uh in couple that together with a proper semantic search system uh like the retrieval augmented generation and Vector database companies are doing so I think that that discussion has come up in open source a lot um and basically it I think it matches human memory right like you want to have a short working memory hahaha you know the I was thinking about it the one other obviously big sort of company update that we haven&#39;t spoken about yet was around the middle of the month Google bard had aa big set of updates a lot of it was sort of business focused right so it was available in more languages uh it was you know whatever the the sort of from a feature perspective the biggest thing that they were sort of hanging their hat on was around image recognition and sort of this push towards uh towards multimodality but you know did did you have any guys did you guys have any thoughts about that or was that sort of like you know not sort of on the the high priority list as a as an announcement or development this month II think going back to the point before we&#39;re getting to the maturity level of the industry we&#39;re like doing like model updates and all this stuff like it&#39;s fine but like people need more you know people need more and like that&#39;s why I call it interpreter it&#39;s like so good right it&#39;s not just like oh we made the model A little better like we added this thing it&#39;s like this is like a whole new thing if you&#39;re playing the model game if not you got to go to the product level and I think Google should start thinking about how to make that work because when I search on Google Maps for certain stuff it&#39;s like completely does not work so maybe they should use models to like make that better and then say we&#39;re using Bard in Google Maps search uh but yeah I don&#39;t know I&#39;ve kind of I&#39;m kind of tuning off a lot of the single just model announcements so uh so Bart&#39;s updates I think the the multi-modality they actually beat gpt4 to releasing a generally available multimodal wall right you can upload an image and have Bard describe it and that&#39;s pretty interesting pretty cool um I think uh one of our earliest guests Robo flow uh Brad their CTO was actually doing some comparisons because they have access to a lot of division models and and Bart came up a little bit short but it was pretty good it was it was like close to the state of the art um I would say the problem with Bard is that you can&#39;t rely on them having reliable updates because they had a June update I don&#39;t actually remember of implicit code execution where they started to ship uh the code interpreter type functionality but in a more limited format if you run the same code the same questions that but advertising the June blog post it&#39;s sundarkai advertise in in a video that and tweet it out they no longer worked in the heart so they had a regression that&#39;s that was very embarrassing um obviously unintended but uh it&#39;s and it shows that it&#39;s hard to keep model progress up to date but I think Google has this checkered history riff its products being reliable you know they also killed off Google Adobe rip um and uh and I think that&#39;s something that they have to combat which is like yes they&#39;re they&#39;re trying to ship model progress I&#39;ve met the bar people they&#39;re you know good artist people um but they have struggled to to ship uh products even more than open AI which is frankly embarrassing for a couple of the size of Google outside of the the biggies are there any other sort of key trends or or you know maybe not even key trends but sort of bubbling interest that you guys are noticing in the developer community that aren&#39;t necessarily super widely uh seen outside you know one of the things that I keep an eye on is all the auto GPT like things you know in this month we had gbt engineer and we had multi-on who held a hackathon and you know there&#39;s a few few things like that but you know not necessarily in the agent space but are there any other themes that you guys are are keeping an eye on let&#39;s say uh I I&#39;m sure Alessio can chime in but on on I do keep a relative uh close eye on that agent stuff uh it has not uh died down in terms of the the heat uh even the other GPT team who by the way I work uh on the first floor the building that I work on uh they&#39;re hard at work uh shipping the next version and so I think a lot of people are engaging in the dream of agents and um I think like scoping them down to something usable is still a task that uh has not as it has so far eluded every single team so far and uh and it is what it is I think I think uh all these very ambitious goals we are at the very start of of this journey uh the same Journey that maybe self-driving cars took uh in 2012 when when they started doing the darker challenge um and I think the other thing I&#39;ll point out interest in terms of uh just overall interest uh I am definitely seeing a lot of uh eval type companies being formed and winning hackathons too um so what what at Utah companies they&#39;re they&#39;re basically uh companies in that you uh monitor the uh the success of your prompts or your agents and version them and um and and just share them potentially um III feel like I can&#39;t be more descriptive just because it&#39;s hard to um to really describe what they do it&#39;s just because they are not very clear about what they do yet um Lang chain launch Lang Smith um and I think that is the first commercial product that nine chain probably you know the the top one or two developer oriented AI projects out there um and that&#39;s more observability but also local uh tensorous ebal as well because they Aqua hired in an AI eval projects as well so I was I&#39;ll just call out just the general domain of how to eval models um is a very big focus of the developers here again yep yeah we&#39;ve done um two seats and companies doing agents but they&#39;re both verticalized agents so I think the open source motion has been Auto gbt do anything um and now we&#39;re seeing a lot of Founders is like hey you know if you take that and then you combine it with like deep industry expertise you can get so many improvements to it and then the other piece of it is how do you do information retrieval so you know in general knowledge like documents everything is kind of flat but when you&#39;re in specific vertical say Finance for example um you know if you&#39;re looking at the earnings from this quarter like 10 quarters ago like the latest ones are like much more important so how do you start to create this like information hierarchy between documents and then how do you use that instead of doing simple like retrieval from like an embedding store it&#39;s like how do you also start to score these things that&#39;s another area of of research from from founders oh I&#39;ll call out two more things um one more thing that happened this week this month was sdxl uh you know text to image doesn&#39;t seem as sexy anymore even though like last year with all the raids um I but I do think like it&#39;s it&#39;s coming along um II definitely wish that Google was putting up more of a fight because they actually at the start of the Year released some very interesting Capers that they never followed up on uh that show some really interesting Transformers based uh text image models that I thought was super interesting and then this the other uh element which uh you know I&#39;m just like very fascinated by a lot of the I don&#39;t know like the uh uh III hesitate to say this but it&#39;s actually like the the character and like the um um let&#39;s just call they call it character replica and and all the sort of work versions of that um II do think that a lot of people are hacking on this kind of stuff um the retention metrics on character AI blows away um you know a lot of the uh the metrics that you might see in on traditional social media sites and basically AI native social media is something that is something that that is there&#39;s something there that I think people haven&#39;t really explored yet and and people are exploring it you know like uh is this company and like you know he&#39;s always a few years ahead of it so uh not to keep returning to this theme but II just think like it&#39;s it&#39;s definitely coming for a lot of like a lot of the ways that we we deal with things like right now we think co-pilot and we right now we think um uh we&#39;ve been chat gbt but like uh what what we what we really want to speak to is is uh a way of serializing personality and intelligence um and and potentially that is a that is a leading form of Mind upload um so that Becca is into science fiction but I do see a lot of people working on that yeah I mean we just got a Financial Times report that says that AI personas uh from meta from Facebook could be coming next month they were talking about uh yeah they were talking about airport was there&#39;s one one that&#39;s Abraham Lincoln one that&#39;s like a surfer dude who gives you travel advice so it&#39;s it&#39;s it&#39;s you know the sourcing is three people with knowledge of the project or whatever um and it you know no obviously no confirmation from meta but it&#39;s no secret that Zuckerberg has been interested in this stuff and uh you know the the ftp&#39;s is actually it&#39;s a good overview of why a company like Meadow would care about it in very dollars and cents terms yeah something like and I want to State like the first version of this is very very me like when I first looked at character AI it was like okay I want to talk to Genghis Khan if I&#39;m doing a history class but it&#39;s like not it&#39;s like what if what a 10 year old would enjoy you know um but I think the the various iterations of this professionally would be very interesting so on the developer side of this I have been calling for the development of agent clouds which are clouds that are specifically uh optimized not for uh human use but for uh EI agent teams and that is a form of character right it&#39;s a character is it with the different environments uh with the different dependencies pre-installed uh that can be programmatically controlled can get programmatic feedback to agents um and uh and there&#39;s a protocol for me um that some of the leading figures like Auto gbt and e2b are creating that um lets agents run clouds um this would this would definitely terrify the AI safety people because we have gone from like running them on a single machine towards running you know clusters originally um but it&#39;s happening all right so so let&#39;s talk about what comes next do you guys have any predictions for August or if not predictions just things that you&#39;re watching most closely go ahead Alice uh let me let me think and I think Sean is usually good at like the super long term prediction some more uh pragmatic I don&#39;t know you know yeah he&#39;s more like he he like minimum like 12 to 24 months um II think like for me probably starting to see more public talk about open source models in production with people using that as a differentiator I think right now a lot of it is kind of like oh these models are there but nobody&#39;s really saying oh I moved away from opening I&#39;m using this but in our we run a early adopters Community with about 1500 kind of like a Fortune 500 large companies leaders and some of them were like oh we deployed dolly in production and we&#39;re using it we&#39;re not writing a blog post about it um so I think right now the perception is still everybody&#39;s using open Ai and the open source models are like really toys but I think we&#39;re gonna get into September and you know you&#39;re not going to see a lot of announcements in August proper but I think a lot of people are gonna spend August getting these models ready and then going into end of the year and say hey we&#39;re here too you know we&#39;re using the open models like we don&#39;t need open AI um I think right now there&#39;s still not not a lot of a lot of public talk about that so excited to to see more uh yeah I&#39;m a little bit uh as for myself uh this is very self-interested obviously but we had to edit an agenda you know I wrote about the the rise of the AI engineer I mean I think it&#39;s definitely happening as we speak um II have seen multiple tags like people tag me multiple times a day on like uh how they&#39;re reorienting their careers I think people professionalizing around this and going from essentially like informal groups and slack channels and meetups and stuff towards uh certifications and courses and job titles and actual AI teams in every single company I think is happening um II just got notification like two days ago that the uh you know in meta apparently you can sort of name your name a job site title whatever you want internally uh and so they emerged as the first AI engineer within meta uh has has been announced and uh so I think I think as far as you know the near-term I do see this career this profession come into place um that I&#39;ve been forecasting for uh for a little bit and I&#39;m excited to help it along awesome well guys great conversation tons of interesting stuff happening obviously um I do think it you know ironically I think it&#39;s a relatively more quiet time in some ways than than it even was and you know my my prediction for August is that we&#39;re going to see the extension of that we&#39;re going to see sort of the the biggest breath that we&#39;ve had at least from a from a feeling perspective maybe since Chachi PT but then we&#39;re gonna rage back in in September you got Facebook connects in September you&#39;ve got sort of just the return to business that everyone does after August um but of course I think you know the hackathons aren&#39;t going to stop in the Bay Area so people are going to keep building and it&#39;s entirely possible that something you know hits in the next four weeks that that totally changes that be exciting to see looking forward</p> ]]>; </content:encoded></item><item><title><![CDATA[FlashAttention 2: making Transformers 800% faster w/o approximation - with Tri Dao of Together AI]]></title><description><![CDATA[Listen now (55 mins) | How FlashAttention became the new industry standard architecture, how FlashAttention 2 is 2x faster still, life inside the Stanford Hazy Research lab, and hints of the post-Transformers future]]></description><link/> https://www.latent.space/p/flashattention<guid ispermalink="true"> https://www.latent.space/p/flashattention</guid><dc:creator><![CDATA[Alessio Fanelli]]></dc:creator><pubDate> Wed, 26 Jul 2023 16:46:04 GMT</pubDate> <enclosure length="0" type="audio/mpeg" url="https://api.substack.com/feed/podcast/135410661/04e9cab89c6aeff0f3b8c069a96ad021.mp3"></enclosure><content:encoded><![CDATA[<p> FlashAttention was first published by Tri Dao in May 2022 and it had a deep impact in the large language models space. Most open models you&#39;ve heard of (RedPajama, <a href="https://www.latent.space/p/mosaic-mpt-7b">MPT</a> , <a href="https://www.latent.space/p/llama2#details">LLaMA</a> , Falcon, etc) all leverage it for faster inference. Tri came on the podcast to chat about FlashAttention, the newly released FlashAttention-2, the research process at Hazy Lab, and more.</p><p> This is the first episode of our “Papers Explained” series, which will cover some of the foundational research in this space. Our Discord also hosts a weekly Paper Club, which you can signup for <a href="https://lu.ma/llm-paper-club">here</a> .</p><h3> How does FlashAttention work?</h3><p> The paper is titled “ <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> ”. There are a couple keywords to call out:</p><ul><li><p> <strong>“Memory Efficient”</strong> : standard attention memory usage is quadratic with sequence length (ie O(N^2)). FlashAttention is sub-quadratic at O(N).</p></li><li><p> <strong>“Exact”</strong> : the opposite of “exact” in this case is “sparse”, as in “sparse networks” (see our <a href="https://www.latent.space/p/mosaic-mpt-7b#details">episode with Jonathan Frankle</a> for more). This means that you&#39;re not giving up any precision.</p></li><li><p> The “IO” in <strong>“IO-Awareness”</strong> stands for “Input/Output” and hints at a write/read related bottleneck.</p></li></ul><p> Before we dive in, look at this simple GPU architecture diagram: </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png" width="839" height="279" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:279,&quot;width&quot;:839,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;How Is OpenAI's Triton Different From NVIDIA CUDA?&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="How Is OpenAI's Triton Different From NVIDIA CUDA?" title="How Is OpenAI's Triton Different From NVIDIA CUDA?" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ef61a8c-ed93-4376-8dfc-fd25d421c0ef_839x279.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p> The GPU has access to three memory stores at runtime:</p><ul><li><p> <strong>SRAM</strong> : this is on-chip memory co-located with the actual execution core. It&#39;s limited in size (~20MB on an A100 card) but extremely fast (19TB/s total bandwidth)</p></li><li><p> <strong>HBM:</strong> this is off-chip but on-card memory, meaning it&#39;s in the GPU but not co-located with the core itself. An A100 has 40GB of HBM, but only a 1.5TB/s bandwidth.</p></li><li><p> <strong>DRAM:</strong> this is your traditional CPU RAM. You can have TBs of this, but you can only get ~12.8GB/s bandwidth, which is way too slow. </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png" width="476" height="284.24" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:418,&quot;width&quot;:700,&quot;resizeWidth&quot;:476,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6990320a-792f-4123-bed3-80cd10bb2dc2_700x418.png 1456w" sizes="100vw"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div></li></ul><p> Now that you know what HBM is, look at how the standard Attention algorithm is implemented: </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png" width="1198" height="291" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:291,&quot;width&quot;:1198,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2037e5d0-b318-40ef-8655-396b259b6314_1198x291.png 1456w" sizes="100vw" loading="lazy"></picture><div></div></div></figure></div><p> As you can see, all 3 steps include a “write X to HBM” step and a “read from HBM” step. The core idea behind FlashAttention boils down to this: instead of storing each intermediate result, why don&#39;t we use kernel fusion and run every operation in a single kernel in order to avoid memory read/write overhead? <em>(We also talked about kernel fusion in our episode with <a href="https://www.latent.space/p/geohot#details">George Hotz</a> and how PyTorch / tinygrad take different approaches here)</em></p><p> The result is much faster, but much harder to read: </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png" width="1400" height="820" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1400,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e48aec4-146f-4751-b141-828215f2e56c_1400x820.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png" width="344" height="379.5543624161074" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:745,&quot;resizeWidth&quot;:344,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbd7a6c0d-4987-4afd-bbf2-de44a510bc64_745x822.png 1456w" sizes="100vw" loading="lazy"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p> As you can see, FlashAttention is a very meaningful speed improvement on traditional Attention, and it&#39;s easy to understand why it&#39;s becoming the standard for most models.</p><p> This should be enough of a primer before you dive into our episode! We talked about FlashAttention-2, how Hazy Research Group works, and some of the research being done in Transformer alternatives.</p><h2> <strong>Show Notes:</strong></h2><ul><li><p> <strong><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></strong> (arXiv)</p></li><li><p> <a href="https://crfm.stanford.edu/2023/07/17/flash2.html">FlashAttention-2</a></p></li><li><p> <a href="https://www.together.xyz/">Together AI</a></p></li><li><p> <a href="https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning">From Deep Learning to Long Learning</a></p></li><li><p> <a href="https://hardwarelottery.github.io/">The Hardware Lottery by Sara Hooker</a></p></li><li><p> <a href="https://hazyresearch.stanford.edu/">Hazy Research</a></p></li><li><p> <a href="https://www.isattentionallyouneed.com/">Is Attention All You Need?</a></p></li><li><p> <a href="https://github.com/NVIDIA/cutlass/discussions/787">Nvidia CUTLASS 3</a></p></li><li><p><a href="https://www.tomshardware.com/news/no-sram-scaling-implies-on-more-expensive-cpus-and-gpus">SRAM scaling slows</a></p></li><li><p> Transformer alternatives:</p><ul><li><p> <a href="https://hazyresearch.stanford.edu/blog/2022-06-11-simplifying-s4">S4</a></p></li><li><p> <a href="https://hazyresearch.stanford.edu/blog/2023-03-07-hyena">Hyena</a></p></li><li><p> <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">Recurrent Neural Networks (RNNs)</a></p></li></ul></li></ul><h2> <strong>Timestamps:</strong></h2><ul><li><p> Tri&#39;s background [00:00:00]</p></li><li><p> FlashAttention&#39;s deep dive [00:02:18]</p></li><li><p> How the Hazy Research group collaborates across theory, systems, and applications [00:17:21]</p></li><li><p> Evaluating models beyond raw performance [00:25:00]</p></li><li><p> FlashAttention-2 [00:27:00]</p></li><li><p> CUDA and The Hardware Lottery [00:30:00]</p></li><li><p> Researching in a fast-changing market [00:35:00]</p></li><li><p> Promising transformer alternatives like state space models and RNNs [00:37:30]</p></li><li><p> The spectrum of openness in AI models [00:43:00]</p></li><li><p> Practical impact of models like LLAMA2 despite restrictions [00:47:12]</p></li><li><p> Incentives for releasing open training datasets [00:49:43]</p></li><li><p> Lightning Round [00:53:22]</p></li></ul><h2> <strong>Transcript:</strong></h2><p> <strong>Alessio</strong> : Hey everyone, welcome to the Latent Space podcast. This is Alessio, Partner and CTO-in-Residence at <a href="https://decibel.vc/">Decibel Partners</a> . Today we have no Swyx, because he&#39;s in Singapore, so it&#39;s a one-on-one discussion with Tri Dao. Welcome! [00:00:24]</p><p> <strong>Tri</strong> : Hi everyone. I&#39;m Tri Dao, excited to be here. [00:00:27]</p><p> <strong>Alessio</strong> : Tri just completed his PhD at Stanford a month ago. You might not remember his name, but he&#39;s one of the main authors in the FlashAttention paper, which is one of the seminal work in the Transformers era. He&#39;s got a lot of interest from efficient transformer training and inference, long range sequence model, a lot of interesting stuff. And now you&#39;re going to be an assistant professor in CS at Princeton next year. [00:00:51]</p><p> <strong>Tri</strong> : Yeah, that&#39;s right. [00:00:52]</p><p> <strong>Alessio</strong> : Yeah. And in the meantime, just to get, you know, a low pressure thing, you&#39;re Chief Scientist at Together as well, which is the company behind RedPajama. [00:01:01]</p><p> <strong>Tri</strong> : Yeah. So I just joined this week actually, and it&#39;s been really exciting. [00:01:04]</p><p> <strong>Alessio</strong> : So what&#39;s something that is not on the internet that people should know about you? [00:01:09]</p><p> <strong>Tri</strong> : Let&#39;s see. When I started college, I was going to be an economist, so I was fully on board. I was going to major in economics, but the first week I was at Stanford undergrad, I took a few math classes and I immediately decided that I was going to be a math major. And that kind of changed the course of my career. So now I&#39;m doing math, computer science, AI research. [00:01:32]</p><p> <strong>Alessio</strong> : I had a similar thing. I started with physics and then I took like a programming course and I was like, I got to do computer science. I don&#39;t want to do physics. So FlashAttention is definitely, everybody&#39;s using this. Everybody loves it. You just released FlashAttention 2 last week. [00:01:48]</p><p> <strong>Tri</strong> : Yeah. Early this week on Monday. Yeah. [00:01:53]</p><p> <strong>Alessio</strong> : You know, AI time. Things move fast. So maybe let&#39;s run through some of the FlashAttention highlights, some of the innovation there, and then we can dive into FlashAttention 2. So the core improvement in FlashAttention is that traditional attention is a quadratic sequence length. And to the two, FlashAttention is linear, which obviously helps with scaling some of these models. [00:02:18]</p><p> <strong>Tri</strong> : There are two factors there. So of course the goal has been to make attention go faster or more memory efficient. And ever since attention became popular in 2017 with the Transformer paper, lots and lots of folks have been working on this. And a lot of approaches has been focusing on approximating attention. The goal is you want to scale to longer sequences. There are tons of applications where you want to do that. But scaling to longer sequences is difficult because attention scales quadratically in sequence length on both runtime and memory, as you mentioned. So instead of trying to approximate attention, we were trying to figure out, can we do the same computation and maybe be more memory efficient? So in the end, we ended up being the memory is linear in sequence length. In terms of computation, it&#39;s still quadratic, but we managed to make it much more hardware friendly. And as a result, we do get wall clock speed up on the order of 2 to 4x, which really helps because that just means that you&#39;ll be able to train with 2 to 4x longer sequence length for the same cost without doing any approximations. As a result, lots of folks have been using this. The thing is available in a lot of libraries that do language model training or fine tuning. [00:03:32]</p><p> <strong>Alessio</strong> : And the approximation thing is important because this is an exact thing versus a sparse. So maybe explain a little bit the difference there. [00:03:40]</p><p> <strong>Tri</strong> : For sure. So in addition, essentially you compute pairwise similarity between every single element in a sequence against each other. So there&#39;s been other approaches where instead of doing all that pairwise computation, you only compute similarity for some pairs of elements in the sequence. So you don&#39;t do quadratic number of comparison. And this can be seen as some form of sparsity. Essentially you&#39;re ignoring some of the elements. When you write down the matrix, you essentially say, OK, I&#39;m going to pretend there&#39;s zero. So that has some benefits in terms of runtime and memory. But the trade-off is that it tends to do worse in terms of quality because you&#39;re essentially approximating or ignoring some elements. And I personally have worked on this as well for a few years. But when we talk to practitioners who actually train models, especially at large scale, they say, tend not to use these approximate attention methods. Because it turns out, this was surprising to me at the time, was that these approximation methods, even though they perform fewer computation, they tend to not be faster in walk-on time. So this was pretty surprising because back then, I think my background was more on the theoretical side. So I was thinking of, oh, how many flops or floating point operations are you performing? And hopefully that correlates well with walk-on time. But I realized that I was missing a bunch of ideas from the system side where flops or floating point operations don&#39;t necessarily correlate with runtime. There are other factors like memory reading and writing, parallelism, and so on. So I learned a ton from just talking to systems people because they kind of figured this stuff out a while ago. So that was really eye-opening. And then we ended up focusing a lot more on memory reading and writing because that turned out to be the majority of the time when you&#39;re doing attention is reading and writing memory. [00:05:34]</p><p> <strong>Alessio</strong> : Yeah, the IO awareness is probably one of the biggest innovations here. And the idea behind it is, like you mentioned, the FLOPS growth of the cards have been going up, but the memory bandwidth, not as much. So I think maybe that was one of the assumptions that the original attention paper had. So talk a bit about how that came to be as an idea. It&#39;s one of those things that like in insight, it&#39;s like, obviously, why are we like rewriting to like HBM every time, you know, and like once you change it, it&#39;s clear. But what was that discovery process? [00:06:08]</p><p> <strong>Tri</strong> : Yeah, in hindsight, a lot of the ideas have already been there in the literature. And I would say is it was somehow at the intersection of both machine learning and systems. And you kind of needed ideas from both sides. So on one hand, on the system side, so lots of systems folks have known that, oh, you know, kernel fusion is great. Kernel fusion just means that instead of performing, you know, loading the same element, instead of performing an operation, write it down, load it back up and perform the second operation, you just load it once, perform two operations and then write it down again. So that saves you kind of memory read and write in the middle there. So kernel fusion has been a classic. There&#39;s been other techniques from the system side, like tiling, where you perform things in the form of computations in block, again, so that you can load it into a really fast memory. Think of it as a cache. And this is, again, classical computer science ideas, right? You want to use the cache. So the system folks have been thinking about these ideas for a long time, and they apply to attention as well. But there were certain things in attention that made it difficult to do a complete kernel fusion. One of which is there is this softmax operation in the middle, which requires you to essentially sum across the row of the attention matrix. So it makes it difficult to kind of break it, because there&#39;s this dependency. So it makes it difficult to break things into a block. So on the system side, people have been thinking about these ideas, but it&#39;s been difficult to kind of do kernel fusion for the entire operation. On the machine learning side, people have been thinking more algorithmically. They say, okay, either we can approximate attention, or there&#39;s this trick called the online softmax trick, which says that because of softmax, the way it&#39;s written mathematically, you can actually break it up into smaller pieces, do some rescaling, and still get the right answer. So this online softmax trick has been around for a while. I think there was a paper from NVIDIA folks back in 2018 about this. And then there was a paper from Google. So Marcus, Rob, and Stats wrote a paper late 2021 on using this online softmax trick to break attention up into smaller pieces. So a lot of the ideas were already there. But it turns out, you kind of need to combine ideas from both sides. So you need to understand that, hey, we want to do kernel fusion to reduce memory written writes. But we also need this online softmax trick to be able to break the softmax into smaller pieces so that a lot of the systems tricks kind of carry through. We saw that, and it was kind of a natural idea that we ended up using ideas from both sides, and it ended up working pretty well. Yeah. [00:08:57]</p><p> <strong>Alessio</strong> : Are there any downsides to kernel fusion? If I think about databases and the reasons why we have atomic operations, you know, it&#39;s like, you have observability and fallback in between them. How does that work with attention? Is there anything that we lose by fusing the operations? [00:09:13]</p><p> <strong>Tri</strong> : Yeah, I think mostly on the practical side is that you lose a little bit of flexibility in the sense that, hey, now you have, for example, faster attention, it&#39;s just a subroutine that you would call to do attention. But as a researcher, let&#39;s say you don&#39;t want that exact thing, right? You don&#39;t want just attention, let&#39;s say you want some modification to attention. You want to do, hey, I&#39;m going to multiply the query and key, but then I&#39;m going to do this extra thing before I carry on. So kernel fusion just means that, okay, we have a subroutine that does the entire thing. But if you want to experiment with things, you won&#39;t be able to use that fused kernel. And the answer is, can we have a compiler that then automatically does a lot of this kernel fusion? Lots of compiler folks are thinking about this, either with a new language or you can embed it in PyTorch. PyTorch folks have been working on this as well. So if you write just your code in PyTorch and they can capture the graph, can they generate code that will fuse everything together? That&#39;s still ongoing, and it works for some cases. But for attention, because of this kind of softmax rewriting stuff, it&#39;s been a little bit more difficult. So maybe in a year or two, we&#39;ll have compilers that are able to do a lot of these optimizations for you. And you don&#39;t have to, for example, spend a couple months writing CUDA to get this stuff to work. Awesome. [00:10:41]</p><p> <strong>Alessio</strong> : And just to make it clear for listeners, when we say we&#39;re not writing it to memory, we are storing it, but just in a faster memory. So instead of the HBM, we&#39;re putting it in the SRAM. Yeah. [00:10:53]</p><p> <strong>Tri</strong> : Yeah. [00:10:54]</p><p> <strong>Alessio</strong> : Maybe explain just a little bit the difference there. [00:10:56]</p><p> <strong>Tri</strong> : Yeah, for sure. This is kind of a caricature of how you think about accelerators or GPUs in particular, is that they have a large pool of memory, usually called HBM, or high bandwidth memory. So this is what you think of as GPU memory. So if you&#39;re using A100 and you list the GPU memory, it&#39;s like 40 gigs or 80 gigs. So that&#39;s the HBM. And then when you perform any operation, you need to move data from the HBM to the compute unit. So the actual hardware unit that does the computation. And next to these compute units, there are on-chip memory or SRAM, which are much, much smaller than HBM, but much faster. So the analogy there is if you&#39;re familiar with, say, CPU and RAM and so on. So you have a large pool of RAM, and then you have the CPU performing the computation. But next to the CPU, you have L1 cache and L2 cache, which are much smaller than DRAM, but much faster. So you can think of SRAM as the small, fast cache that stays close to the compute unit. Physically, it&#39;s closer. There is some kind of asymmetry here. So HBM is much larger, and SRAM is much smaller, but much faster. One way of thinking about it is, how can we design algorithms that take advantage of this asymmetric memory hierarchy? And of course, lots of folks have been thinking about this. These ideas are pretty old. I think back in the 1980s, the primary concerns were sorting. How can we sort numbers as efficiently as possible? And the motivating example was banks were trying to sort their transactions, and that needs to happen overnight so that the next day they can be ready. And so the same idea applies, which is that they have slow memory, which was hard disk, and they have fast memory, which was DRAM. And people had to design sorting algorithms that take advantage of this asymmetry. And it turns out, these same ideas can apply today, which is different kinds of memory. [00:13:00]</p><p> <strong>Alessio</strong> : In your paper, you have the pyramid of memory. Just to give people an idea, when he says smaller, it&#39;s like HBM is like 40 gig, and then SRAM is like 20 megabytes. So it&#39;s not a little smaller, it&#39;s much smaller. But the throughput on card is like 1.5 terabytes a second for HBM and like 19 terabytes a second for SRAM, which is a lot larger. How do you think that evolves? So TSMC said they hit the scaling limits for SRAM, they just cannot grow that much more. HBM keeps growing, HBM3 is going to be 2x faster than HBM2, I think the latest NVIDIA thing has HBM3. How do you think about the future of FlashAttention? Do you think HBM is going to get fast enough when maybe it&#39;s not as useful to use the SRAM? [00:13:49]</p><p> <strong>Tri</strong> : That&#39;s right. I think it comes down to physics. When you design hardware, literally SRAM stays very close to compute units. And so you don&#39;t have that much area to essentially put the transistors. And you can&#39;t shrink these things too much. So just physics, in terms of area, you don&#39;t have that much area for the SRAM. HBM is off-chip, so there is some kind of bus that essentially transfers data from HBM to the compute unit. So you have more area to essentially put these memory units. And so yeah, I think in the future SRAM probably won&#39;t get that much larger, because you don&#39;t have that much area. HBM will get larger and faster. And so I think it becomes more important to design algorithms that take advantage of this memory asymmetry. It&#39;s the same thing in CPU, where the cache is really small, the DRAM is growing larger and larger. DRAM could get to, I don&#39;t know, two terabytes, six terabytes, or something, whereas the cache stays at, I don&#39;t know, 15 megabytes or something like that. I think maybe the algorithm design becomes more and more important. There&#39;s still ways to take advantage of this, I think. So in the future, I think flash attention right now is being used. I don&#39;t know if in the next couple of years, some new architecture will come in and whatnot, but attention seems to be still important. For the next couple of years, I still expect some of these ideas to be useful. Not necessarily the exact code that&#39;s out there, but I think these ideas have kind of stood the test of time. New ideas like IO awareness from back in the 1980s, ideas like kernel fusions, tiling. These are classical ideas that have stood the test of time. So I think in the future, these ideas will become more and more important as we scale models to be larger, as we have more kinds of devices, where performance and efficiency become much, much more important. [00:15:40]</p><p> <strong>Alessio</strong> : Yeah, and we had Jonathan Frankle on the podcast, and if you go to issattentionallyouneed.com, he has an outstanding bet, and he does believe that attention will be the state of the art architecture still in a few years. Did you think flash attention would be this popular? I&#39;m always curious on the research side, you publish a paper, and obviously you know it&#39;s great work, but sometimes it just kind of falls flat in the industry. Could you see everybody just starting to use this, or was that a surprise to you? [00:16:11]</p><p> <strong>Tri</strong> : Certainly, I didn&#39;t anticipate the level of popularity. Of course, we were extremely happy to have people using this stuff and giving us feedback and so on, and help us improve things. I think when we were writing the paper, I remember sending an email to one of my advisors, and like, hey, I&#39;m excited about this paper, but I think the most important thing will be the artifact, which is the code. So I knew that the code will be valuable. So we kind of focus a lot on the code and make sure that the code is usable and as fast as can be. Of course, the idea, the paper presents the ideas and explain it and have experiments that validate the idea, but I knew that the artifact or the code was also pretty important. And that turned out to be the right focus, which is, you know, we put out the paper, we release the code and continue working on the code. So it&#39;s a team effort with my co-authors as well. [00:17:07]</p><p> <strong>Alessio</strong> : We mentioned Hazy Research a bunch of times on the podcast before. I would love for you to spend five minutes just talking about how does the group work? How do people get together? How do you bounce ideas off of each other? Yeah. [00:17:21]</p><p> <strong>Tri</strong> : So Hazy Research is a research group at Stanford led by one of my advisors, Chris Re. I love the people there. It was one of the best experiences I had. They&#39;ve made my PhD so much more enjoyable. And I think there are a couple of ways that the group has been working pretty well. So one is, I think there&#39;s a diverse pool of people who either, you know, some of them focus on algorithms and theory, some of them focus on building systems, some of them focus on applications. And as a result, there is this flow of idea. So as an example, some of us were working on like more algorithms and theory, and then we can talk to the folks building systems and say, hey, let&#39;s try it out and let&#39;s put it in the systems and see how it is. And there you will get feedback from systems folks. They will say, hey, we implemented this, or we tried this and this is where it doesn&#39;t work, something like that. And once we put it in the systems, the application folks can use the algorithm or new methods or new models. And we again get great feedback from them because the application folks, for example, some of my good friends, they focus on medical imaging or seizure detection. And that is the problem they care about. And if your method doesn&#39;t work on the task they care about, they will tell you. Whereas I think a lot of people in machine learning, they&#39;re a little bit more flexible. So they will be like, hey, it doesn&#39;t work on seizure detection. Let&#39;s try some other task, right? But having that direct feedback of like, hey, it doesn&#39;t work there, let&#39;s figure out why. I think that that feedback allows us to do better work. And I think that kind of process of exchanging ideas, validating it in a real system so that applications folks can try it out and give you feedback. That cycle has been very, very useful. And so that&#39;s one, having a diverse group of people. The other one is, and this is something I really appreciate from advice from Chris was try to understand the fundamental, right? And he&#39;s happy letting me go off and read some textbooks and playing with things because I think a lot of research ideas come from understanding the old literature and see how it fits with the new landscape. And so if you just new archive papers every day, that&#39;s great, but you also need to read textbooks. And that&#39;s one advice I got from Chris, which is understand the fundamentals. And I think that allows us to do more impactful work. [00:19:46]</p><p> <strong>Alessio</strong> : How do you think about academia versus industry? I feel like AI / Machine Learning has been an area where up until three, four years ago, most of the cutting edge work was being done in academia. And now there&#39;s all these big industry research labs. You&#39;re obviously going to Princeton, so you&#39;re an academia believer. How should people think about where to go? Say I&#39;m doing my master&#39;s, I have to decide between doing a PhD and going into OpenAI Anthropic. How should I decide? [00:20:15]</p><p> <strong>Tri</strong> : I think they kind of play a complementary role, in my opinion. Of course, I also was considering different paths as well. So I think right now, scaling matters a lot, especially when you talk about language models and AI and so on. Scaling matters a lot. And that means that you need compute resources and you need infrastructure and you need engineers time. And so industry tends to have an advantage when it comes to scaling things. But a lot of the ideas actually came from academia. So let&#39;s take Attention, which got popular with the Transformer in 2017. Attention actually has been around for a while. So I think the first mention was in 2014, a paper from Bernadot and others and Yoshua Bengio, which is coming from academia. A lot of ideas did come from academia. And scaling things up, of course, I think OpenAI has been great at scaling things up. That was the bet that they made after, I think, GPT-2. So they saw that scaling these things up to back then was 1.5 billion parameter seemed to give you amazing capabilities. So they really committed to that. They really committed to scaling things. And that turned out to be, it&#39;s been a pretty successful bet. I think for academia, we&#39;re still trying to figure out exactly what we&#39;re doing in this shifting landscape. And so lots of folks have been focusing on, for example, evaluation. So I know the Stanford Center for Foundation Model led by Percy, they have this benchmark called HELM, which is this holistic benchmark. So trying to figure out, okay, characterizing the landscape of different kinds of models, what people should evaluate, what people should measure, and things like that. So evaluation is one role. The other one is understanding. So this has happened historically where there&#39;s been some development in the industry and academia can play a role in explaining, understanding. They have the luxury to slow down trying to understand stuff, right? So lots of paper on understanding what&#39;s really going on, probing these models, and so on. I think I&#39;m not as familiar with the NLP literature, but my impression is there&#39;s a lot of that going on in the NLP conferences, which is understanding what these models are doing, what capabilities they have, and so on. And the third one I could see is that the academia can take more risky bets in the sense that we can work on stuff that is quite different from industry. I think industry, my impression is you have some objective. You&#39;re trying to say, hey, for this quarter, we want to scale the model in this particular way. Next quarter, we want the model to have these capabilities. You&#39;re trying to get objectives that maybe, I don&#39;t know, 70% that will work out because it&#39;s important for the company&#39;s direction. I think for academia, the way things work is you have many, many researchers or PhD students, and they&#39;re kind of pursuing independent directions. And they have a little bit more flexibility on, hey, I&#39;m going to try out this seemingly crazy idea and see, let&#39;s say there&#39;s a 30% chance of success or something. And however you define success, for academia, a lot of the time, success just means like, hey, we found something interesting. That could eventually go into industry through collaboration and so on. So I do see academia and industry kind of playing complementary roles. And as for someone choosing a career, I think just more and more generally, industry would be probably better in terms of compensation, in terms of probably work-life balance. But my biased perspective is that maybe academia gives you a little bit more freedom to think and understand things. So it probably comes down to personal choice. I end up choosing to be a professor next year at Princeton. But of course, I want to maintain a relationship with industry folks. I think industry folks can provide very valuable feedback to what we&#39;re doing in academia so that we understand where the field is moving because some of the directions are very much influenced by what, for example, OpenAI or Google is doing. So we want to understand where the field is moving. What are some promising applications? And try to anticipate, okay, if the field is moving like this, these applications are going to be popular. What problems will be important in two, three years? And then we try to start thinking about those problems so that hopefully in two, three years, we have some of the answers to some of these problems in two, three years. Sometimes it works out, sometimes it doesn&#39;t. But as long as we do interesting things in academia, that&#39;s the goal. [00:25:03]</p><p> <strong>Alessio</strong> : And you mentioned the eval side. So we did a Benchmarks 101 episode. And one of the things we were seeing is sometimes the benchmarks really influence the model development. Because obviously, if you don&#39;t score well on the benchmarks, you&#39;re not going to get published and you&#39;re not going to get funded. How do you think about that? How do you think that&#39;s going to change now that a lot of the applications of these models, again, is in more narrow industry use cases? Do you think the goal of the academia eval system is to be very broad and then industry can do their own evals? Or what&#39;s the relationship there? [00:25:40]</p><p> <strong>Tri</strong> : Yeah, so I think evaluation is important and often a little bit underrated. So it&#39;s not as flashy as, oh, we have a new model that can do such and such. But I think evaluation, what you don&#39;t measure, you can&#39;t make progress on, essentially. So I think industry folks, of course, they have specific use cases that their models need to do well on. And that&#39;s what they care about. Not just academia, but other groups as well. People do understand what are some of the emerging use cases. So for example, now one of the most popular use cases is Chatbot. And then I think folks from Berkeley, some of them are from Berkeley, call them MLCs. They set up this kind of Chatbot arena to essentially benchmark different models. So people do understand what are some of the emerging use cases. People do contribute to evaluation and measurement. And as a whole, I think people try to contribute to the field and move the field forward, albeit that maybe slightly different directions. But we&#39;re making progress and definitely evaluation and measurement is one of the ways you make progress. So I think going forward, there&#39;s still going to be just more models, more evaluation. We&#39;ll just have better understanding of what these models are doing and what capabilities they have. [00:26:56]</p><p> <strong>Alessio</strong> : I like that your work has been focused on not making benchmarks better, but it&#39;s like, let&#39;s just make everything faster. So it&#39;s very horizontal. So FlashAttention 2, you just released that on Monday. I read in the blog post that a lot of the work was also related to some of the NVIDIA library updates. Yeah, maybe run us through some of those changes and some of the innovations there. Yeah, for sure. [00:27:19]</p><p> <strong>Tri</strong> : So FlashAttention 2 is something I&#39;ve been working on for the past couple of months. So the story is the NVIDIA CUTLASS team, they released a new version of their library, which contains all these primitives to allow you to do matrix multiply or memory loading on GPU efficiently. So it&#39;s a great library and I built on that. So they released their version 3 back in January and I got really excited and I wanted to play with that library. So as an excuse, I was just like, okay, I&#39;m going to refactor my code and use this library. So that was kind of the start of the project. By the end, I just ended up working with the code a whole lot more and I realized that, hey, there are these inefficiencies still in Flash Attention. We could change this way or that way and make it, in the end, twice as fast. But of course, building on the library that the NVIDIA folks released. So that was kind of a really fun exercise. I was starting out, it&#39;s just an excuse for myself to play with the new library. What ended up was several months of improvement, improving Flash Attention, discovering new ideas. And in the end, we managed to make it 2x faster and now it&#39;s pretty close to probably the efficiency of things like matrix multiply, which is probably the most optimized subroutine on the planet. So we&#39;re really happy about it. The NVIDIA Cutlass team has been very supportive and hopefully in the future, we&#39;re going to collaborate more. [00:28:46]</p><p> <strong>Alessio</strong> : And since it&#39;s an NVIDIA library, can you only run this on CUDA runtimes? Or could you use this and then run it on an AMD GPU? [00:28:56]</p><p> <strong>Tri</strong> : Yeah, so it&#39;s an NVIDIA library. So right now, the code we release runs on NVIDIA GPUs, which is what most people are using to train models. Of course, there are emerging other hardware as well. So the AMD folks did implement a version of Flash Attention, I think last year as well, and that&#39;s also available. I think there&#39;s some implementation on CPU as well. For example, there&#39;s this library, ggml, where they implemented the same idea running on Mac and CPU. So I think that kind of broadly, the idea would apply. The current implementation ended up using NVIDIA&#39;s library or primitives, but I expect these ideas to be broadly applicable to different hardware. I think the main idea is you have asymmetry in memory hierarchy, which tends to be everywhere in a lot of accelerators. [00:29:46]</p><p> <strong>Alessio</strong> : Yeah, it kind of reminds me of Sara Hooker&#39;s post, like the hardware lottery. There could be all these things that are much better, like architectures that are better, but they&#39;re not better on NVIDIA. So we&#39;re never going to know if they&#39;re actually improved. How does that play into some of the research that you all do too? [00:30:04]</p><p> <strong>Tri</strong> : Yeah, so absolutely. Yeah, I think Sara Hooker, she wrote this piece on hardware lottery, and I think she captured really well of what a lot of people have been thinking about this. And I certainly think about hardware lottery quite a bit, given that I do some of the work that&#39;s kind of really low level at the level of, hey, we&#39;re optimizing for GPUs or NVIDIA GPUs and optimizing for attention itself. And at the same time, I also work on algorithms and methods and transformer alternatives. And we do see this effect in play, not just hardware lottery, but also kind of software framework lottery. You know, attention has been popular for six years now. And so many kind of engineer hours has been spent on making it as easy and efficient as possible to run transformer, right? And there&#39;s libraries to do all kinds of tensor parallel, pipeline parallel, if you use transformer. Let&#39;s say someone else developed alternatives, or let&#39;s just take recurrent neural nets, like LSTM, GRU. If we want to do that and run that efficiently on current hardware with current software framework, that&#39;s quite a bit harder. So in some sense, there is this feedback loop where somehow the model architectures that take advantage of hardware become popular. And the hardware will also kind of evolve to optimize a little bit for that kind of architecture and software framework will also evolve to optimize for that particular architecture. Right now, transformer is the dominant architecture. So yeah, I&#39;m not sure if there is a good way out of this. Of course, there&#39;s a lot of development. Things like, I think compilers will play a role because compilers allow you to maybe still be much more efficient across different kinds of hardware because essentially you write the same code and compiler will be able to make it run efficiently different kinds of hardware. So for example, there&#39;s this language Mojo, they&#39;re compiler experts, right? And their bet is AI models will be running on different kinds of devices. So let&#39;s make sure that we have really good compilers with a good language that then the compiler can do a good job optimizing for all kinds of devices. So that&#39;s maybe one way that you can get out of this cycle. But yeah, I&#39;m not sure of a good way. In my own research, I have to think about both the algorithm new model and how it maps to hardware. So there are crazy ideas that seem really good, but will be really, really difficult to run efficiently. And so as a result, for example, we can&#39;t really scale some of the architectures up simply because they&#39;re not hardware friendly. I have to think about both sides when I&#39;m working on new models. [00:32:50]</p><p> <strong>Alessio</strong> : Yeah. Have you spent any time looking at some of the new kind of like AI chips companies, so to speak, like the Cerebras of the world? Like one of their innovations is co-locating everything on the chip. So you remove some of this memory bandwidth issue. How do you think about that? [00:33:07]</p><p> <strong>Tri</strong> : Yeah, I think that&#39;s an interesting bet. I think Tesla also has this Dojo supercomputer where they try to have essentially as fast on-chip memory as possible and removing some of these data transfer back and forth. I think that&#39;s a promising direction. The issues I could see, you know, I&#39;m definitely not a hardware expert. One issue is the on-chip memory tends to be really expensive to manufacture, much more expensive per gigabyte compared to off-chip memory. So I talked to, you know, some of my friends at Cerebros and, you know, they have their own stack and compiler and so on, and they can make it work. The other kind of obstacle is, again, with compiler and software framework and so on. For example, if you can run PyTorch on this stuff, lots of people will be using it. But supporting all the operations in PyTorch will take a long time to implement. Of course, people are working on this. So I think, yeah, we kind of need these different bets on the hardware side as well. Hardware has, my understanding is, has a kind of a longer time scale. So you need to design hardware, you need to manufacture it, you know, maybe on the order of three to five years or something like that. So people are taking different bets, but the AI landscape is changing so fast that it&#39;s hard to predict, okay, what kind of models will be dominant in, let&#39;s say, three or five years. Or thinking back five years ago, would we have known that Transformer would have been the dominant architecture? Maybe, maybe not, right? And so different people will make different bets on the hardware side. [00:34:39]</p><p> <strong>Alessio</strong> : Does the pace of the industry and the research also influence the PhD research itself? For example, in your case, you&#39;re working on improving attention. It probably took you quite a while to write the paper and everything, but in the meantime, you could have had a new model architecture come out and then it&#39;s like nobody cares about attention anymore. How do people balance that? [00:35:02]</p><p> <strong>Tri</strong> : Yeah, so I think it&#39;s tough. It&#39;s definitely tough for PhD students, for researchers. Given that the field is moving really, really fast, I think it comes down to understanding fundamental. Because that&#39;s essentially, for example, what the PhD allows you to do. It&#39;s been a couple of years understanding the fundamentals. So for example, when I started my PhD, I was working on understanding matrix vector multiply, which has been a concept that&#39;s been around for hundreds of years. We were trying to characterize what kind of matrices would have theoretically fast multiplication algorithm. That seems to have nothing to do with AI or anything. But I think that was a time when I developed mathematical maturity and research taste and research skill. The research topic at that point didn&#39;t have to be super trendy or anything, as long as I&#39;m developing skills as a researcher, I&#39;m making progress. And eventually, I&#39;ve gotten quite a bit better in terms of research skills. And that allows, for example, PhD students later in their career to quickly develop solutions to whatever problems they&#39;re facing. So I think that&#39;s just the natural arc of how you&#39;re being trained as a researcher. For a lot of PhD students, I think given the pace is so fast, maybe it&#39;s harder to justify spending a lot of time on the fundamental. And it&#39;s tough. What is this kind of explore, exploit kind of dilemma? And I don&#39;t think there&#39;s a universal answer. So I personally spend some time doing this kind of exploration, reading random textbooks or lecture notes. And I spend some time keeping up with the latest architecture or methods and so on. I don&#39;t know if there&#39;s a right balance. It varies from person to person. But if you only spend 100% on one, either you only do exploration or only do exploitation, I think it probably won&#39;t work in the long term. It&#39;s probably going to have to be a mix and you have to just experiment and kind of be introspective and say, hey, I tried this kind of mixture of, I don&#39;t know, one exploration paper and one exploitation paper. How did that work out for me? Should I, you know, having conversation with, for example, my advisor about like, hey, did that work out? You know, should I shift? I focus more on one or the other. I think quickly adjusting and focusing on the process. I think that&#39;s probably the right way. I don&#39;t have like a specific recommendation that, hey, you focus, I don&#39;t know, 60% on lecture notes and 40% on archive papers or anything like that. [00:37:35]</p><p> <strong>Alessio</strong> : Let&#39;s talk about some Transformer alternatives. You know, say Jonathan Franco loses his bet and Transformer is not the state of the art architecture. What are some of the candidates to take over? [00:37:49]</p><p> <strong>Tri</strong> : Yeah, so this bet is quite fun. So my understanding is this bet between Jonathan Franco and Sasha Rush, right? I&#39;ve talked to Sasha a bunch and I think he recently gave an excellent tutorial on Transformer alternatives as well. So I would recommend that. So just to quickly recap, I think there&#39;s been quite a bit of development more recently about Transformer alternatives. So architectures that are not Transformer, right? And the question is, can they do well on, for example, language modeling, which is kind of the application that a lot of people care about these days. So there are methods based on state space methods that came out in 2021 from Albert Gu and Curran and Chris Re that presumably could do much better in terms of capturing long range information while not scaling quadratically. They scale sub-quadratically in terms of sequence length. So potentially you could have a much more efficient architecture when sequence length gets really long. The other ones have been focusing more on recurrent neural nets, which is, again, an old idea, but adapting to the new landscape. So things like RWKV, I&#39;ve also personally worked in this space as well. So there&#39;s been some promising results. So there&#39;s been some results here and there that show that, hey, these alternatives, either RNN or state space methods, can match the performance of Transformer on language modeling. So that&#39;s really exciting. And we&#39;re starting to understand on the academic research side, we want to understand, do we really need attention? I think that&#39;s a valuable kind of intellectual thing to understand. And maybe we do, maybe we don&#39;t. If we want to know, we need to spend serious effort on trying the alternatives. And there&#39;s been folks pushing on this direction. I think RWKV scale up to, they have a model at 14 billion that seems pretty competitive with Transformer. So that&#39;s really exciting. That&#39;s kind of an intellectual thing. We want to figure out if attention is necessary. So that&#39;s one motivation. The other motivation is Transformer Alternative could have an advantage in practice in some of the use cases. So one use case is really long sequences. The other is really high throughput of generation. So for really long sequences, when you train with Transformer, with flash attention and so on, the computation is still quadratic in the sequence length. So if your sequence length is on the order of, I don&#39;t know, 16K, 32K, 100K or something, which some of these models have sequence length 100K, then you do get significantly slower in terms of training, also in terms of inference. So maybe these alternative architectures could scale better in terms of sequence length. I haven&#39;t seen actual validation on this. Let&#39;s say an RNN model release with context length, I don&#39;t know, 100K or something. I haven&#39;t really seen that. But the hope could be that as we scale to long sequences, these alternative architectures could be more well-suited. Not just text, but things like high resolution images, audio, video, and so on, which are emerging applications. So that&#39;s one, long sequences. Number two is a high throughput generation, where I can imagine scenarios where the application isn&#39;t like an interactive chatbot, but let&#39;s say a company wants to batch as many requests as possible on their server, or they&#39;re doing offline processing, they&#39;re generating stuff based on their internal documents, that you need to process in batch. And the issue with Transformer is that during generation, it essentially needs to keep around all the previous history. It&#39;s called the KV cache. And that could take a significant amount of memory, so you can&#39;t really batch too much because you run out of memory. I am personally bullish on RNNs. I think RNNs, they essentially summarize the past into a state vector that has fixed size, so the size doesn&#39;t grow with the history. So that means that you don&#39;t need as much memory to keep around all the previous tokens. And as a result, I think you can scale to much higher batch sizes. And as a result, you can make much more efficient use of the GPUs or the accelerator, and you could have much higher generation throughput. Now, this, I don&#39;t think, has been validated at scale. So as a researcher, I&#39;m bullish on this stuff because I think in the next couple of years, these are use cases where these alternatives could have an advantage. We&#39;ll just kind of have to wait and see to see if these things will happen. I am personally bullish on this stuff. At the same time, I also spend a bunch of time making attention as fast as possible. So maybe hatching and playing both sides. Ultimately, we want to understand, as researchers, we want to understand what works, why do the models have these capabilities? And one way is, let&#39;s push attention to be as efficient as possible. On the other hand, let&#39;s push other alternatives to be as efficient at scale, as big as possible, and so that we can kind of compare them and understand. Yeah, awesome. [00:43:01]</p><p> <strong>Alessio</strong> : And I think as long as all of this work happens and open, it&#39;s a net positive for everybody to explore all the paths. Yeah, let&#39;s talk about open-source AI. Obviously, together, when Red Pajama came out, which was an open clone of the LLAMA1 pre-training dataset, it was a big thing in the industry. LLAMA2 came out on Tuesday, I forget. And this week, there&#39;s been a lot of things going on, which they call open-source, but it&#39;s not really open-source. Actually, we wrote a post about it that was on the front page of Hacker News before this podcast, so I was frantically responding. How do you think about what open-source AI really is? In my mind, in open-source software, we have different levels of open. So there&#39;s free software, that&#39;s like the GPL license. There&#39;s open-source, which is Apache, MIT. And then there&#39;s kind of restricted open-source, which is the SSPL and some of these other licenses. In AI, you have the open models. So Red Pajama is an open model because you have the pre-training dataset, you have the training runs and everything. And then there&#39;s obviously RandomLens that doesn&#39;t make it one-to-one if you retrain it. Then you have the open-weights model that&#39;s kind of like StableLM, where the weights are open, but the dataset is not open. And then you have LLAMA2, which is the dataset is not open, the weights are restricted. It&#39;s kind of like not really open-source, but open enough. I think it&#39;s net positive because it&#39;s like $3 million of flops donated to the public. [00:44:32]</p><p> <strong>Tri</strong> : How do you think about that? [00:44:34]</p><p> <strong>Alessio</strong> : And also, as you work together, what is your philosophy with open-source AI? Right, right. [00:44:40]</p><p> <strong>Tri</strong> : Yeah, I think that&#39;s a great question. And I think about it on maybe more practical terms. So of course, Meta has done an amazing job training LLAMA1, LLAMA2. And for LLAMA2, they make it much less restrictive compared to LLAMA1. Now you can use it for businesses, unless you are a monthly active user or something like that. I think just this change will have a very significant impact in the kind of landscape of open-source AI, where now lots of businesses, lots of companies will be using, I expect will be using things like LLAMA2. They will fine-tune on their own dataset. They will be serving variants or derivatives of LLAMA2. Whereas before, with LLAMA1, it was also a really good model, but your business companies weren&#39;t allowed to do that. So I think on a more practical term, it&#39;s kind of shifting the balance between a closed-source model like OpenAI and Anthropic and Google, where you&#39;re making API calls, right? And maybe you don&#39;t understand as much of what the model is doing, how the model is changing, and so on. Versus now, we have a model with open weight that is pretty competitive from what I&#39;ve seen in terms of benchmarks, pretty competitive with GPT 3.5, right? And if you fine-tune it on your own data, maybe it&#39;s more well-suited for your own data. And I do see that&#39;s going to shift the balance of it. More and more folks are going to be using, let&#39;s say, derivatives of LLAMA2. More and more folks are going to fine-tune and serve their own model instead of calling an API. So that shifting of balance is important because in one way, we don&#39;t want just a concentration of decision-making power in the hands of a few companies. So I think that&#39;s a really positive development from Meta. Of course, training the model takes a couple of millions of dollars, but engineers have and I&#39;m sure they spend tons of time trying many, many different things. So the actual cost is probably way more than that. And they make the weights available and they allow probably a lot of companies are going to be using this. So I think that&#39;s a really positive development. And we&#39;ve also seen amazing progress on the open source community where they would take these models and they either fine-tune on different kinds of data sets or even make changes to the model. So as an example, I think for LLAMA1, the context lane was limited to 2K. Like a bunch of folks figured out some really simple methods to scale up to like 8K. [00:47:12]</p><p> <strong>Alessio</strong> : Like the RoPE. [00:47:13]</p><p> <strong>Tri</strong> : Yes. I think the open source community is very creative, right? And lots of people. LLAMA2 will, again, kind of accelerate this where more people will try it out. More people will make tweaks to it and make a contribution and then so on. So overall, I think I see that as still a very positive development for the field. And there&#39;s been lots of libraries that will allow you to host or fine-tune these models, like even with quantization and so on. Just a couple of hours after LLAMA2 was released, tons of companies announcing that, hey, it&#39;s on our API or hosting and so on and together did the same. So it&#39;s a very fast-paced development and just kind of a model with available weights that businesses are allowed to use. I think that alone is already a very positive development. At the same time, yeah, we can do much better in terms of releasing data sets. Data sets tend to be... Somehow people are not incentivized to release data sets. So philosophically, yeah, you want to be as open as possible. But on a practical term, I think it&#39;s a little bit harder for companies to release data sets. Legal issues. The data sets released tend to be not as eye-catchy as the model release. So maybe people are less incentivized to do that. We&#39;ve seen quite a few companies releasing data sets together. Released a red pajama data set. I think Cerebus then worked on that and deduplicate and clean it up and release slim pajama and so on. So we&#39;re also seeing positive development on that front, kind of on the pre-training data set. So I do expect that to continue. And then on the fine-tuning data set or instruction tuning data set, I think we now have quite a few open data sets on instruction tuning and fine-tuning. But these companies do pay for human labelers to annotate these instruction tuning data set. And that is expensive. And maybe they will see that as their competitive advantage. And so it&#39;s harder to incentivize these companies to release these data sets. So I think on a practical term, we&#39;re still going to make a lot of progress on open source AI, on both the model development, on both model hosting, on pre-training data set and fine-tuning data set. Right now, maybe we don&#39;t have the perfect open source model since all the data sets are available. Maybe we don&#39;t have such a thing yet, but we&#39;ve seen very fast development on the open source side. I think just maybe this time last year, there weren&#39;t as many models that are competitive with, let&#39;s say, ChatGPT. [00:49:43]</p><p> <strong>Alessio</strong> : Yeah, I think the open data sets have so much more impact than open models. If you think about Elusive and the work that they&#39;ve done, GPT-J was great, and the Pythia models are great, but the Pyle and the Stack, everybody uses them. So hopefully we get more people to contribute time to work on data sets instead of doing the 100th open model that performs worse than all the other ones, but they want to say they released the model. [00:50:14]</p><p> <strong>Tri</strong> : Yeah, maybe the question is, how do we figure out an incentive structure so that companies are willing to release open data sets? And for example, it could be like, I think some of the organizations are now doing this where they are asking volunteers to annotate and so on. And maybe the Wikipedia model of data set, especially for instruction tuning, could be interesting where people actually volunteer their time and instead of editing Wikipedia, add annotation. And somehow they acknowledge and feel incentivized to do so. Hopefully we get to that kind of level of, in terms of data, it would be kind of like Wikipedia. And in terms of model development, it&#39;s kind of like Linux where people are contributing patches and improving the model in some way. I don&#39;t know exactly how that&#39;s going to happen, but based on history, I think there is a way to get there. [00:51:05]</p><p> <strong>Alessio</strong> : Yeah, I think the Dolly-15K data set is a good example of a company saying, let&#39;s do this smaller thing, just make sure we make it open. We had Mike Conover from Databricks on the podcast, and he was like, people just bought into it and leadership was bought into it. You have companies out there with 200,000, 300,000 employees. It&#39;s like, just put some of them to label some data. It&#39;s going to be helpful. So I&#39;m curious to see how that evolves. What made you decide to join Together? [00:51:35]</p><p> <strong>Tri</strong> : For Together, the focus has been focusing a lot on open source model. And I think that aligns quite well with what I care about, of course. I also know a bunch of people there that I know and trust, and I&#39;m excited to work with them. Philosophically, the way they&#39;ve been really open with data set and model release, I like that a lot. Personally, for the stuff, for example, the research that I&#39;ve developed, like we also try to make code available, free to use and modify and so on, contributing to the community. That has given us really valuable feedback from the community and improving our work. So philosophically, I like the way Together has been focusing on open source model. And the nice thing is we&#39;re also going to be at the forefront of research and the kind of research areas that I&#39;m really excited about, things like efficient training and inference, aligns quite well with what the company is doing. We&#39;ll try our best to make things open and available to everyone. Yeah, but it&#39;s going to be fun being at the company, leading a team, doing research on the topic that I really care about, and hopefully we&#39;ll make things open to benefit the community. [00:52:45]</p><p> <strong>Alessio</strong> : Awesome. Let&#39;s jump into the lightning round. Usually, I have two questions. So one is on acceleration, one on exploration, and then a takeaway. So the first one is, what&#39;s something that already happened in AI machine learning that you thought would take much longer than it has? [00:53:01]</p><p> <strong>Tri</strong> : I think understanding jokes. I didn&#39;t expect that to happen, but it turns out scaling model up and training lots of data, the model can now understand jokes. Maybe it&#39;s a small thing, but that was amazing to me. [00:53:16]</p><p> <strong>Alessio</strong> : What about the exploration side? What are some of the most interesting unsolved questions in the space? [00:53:22]</p><p> <strong>Tri</strong> : I would say reasoning in the broad term. We don&#39;t really know how these models do. Essentially, they do something that looks like reasoning. We don&#39;t know how they&#39;re doing it. We have some ideas. And in the future, I think we will need to design architecture that explicitly has some kind of reasoning module in it if we want to have much more capable models. [00:53:43]</p><p> <strong>Alessio</strong> : What&#39;s one message you want everyone to remember today? [00:53:47]</p><p> <strong>Tri</strong> : I would say try to understand both the algorithm and the systems that these algorithms run on. I think at the intersection of machine learning system has been really exciting, and there&#39;s been a lot of amazing results at this intersection. And then when you scale models to large scale, both the machine learning side and the system side really matter. [00:54:06]</p><p> <strong>Alessio</strong> : Awesome. Well, thank you so much for coming on 3. [00:54:09]</p><p> <strong>Tri</strong> : This was great. Yeah, this has been really fun. [00:54:11]</p><p></p> ]]>; </content:encoded></item><item><title><![CDATA[Llama 2: The New Open LLM SOTA (ft. Nathan Lambert, Matt Bornstein, Anton Troynikov, Russell Kaplan, Whole Mars Catalog et al.)]]></title><description><![CDATA[Listen now (80 mins) | > 2000 AI Engineers joined our emergency Space with Simon Willison and Alex Volkov. Discussing datasets, benchmarks, scaling laws, kremlinology, and predictions. Come for the Llama, stay for the drama!]]>;</description><link/> https://www.latent.space/p/llama2<guid ispermalink="true"> https://www.latent.space/p/llama2</guid><dc:creator><![CDATA[Nathan Lambert]]></dc:creator><pubDate> Wed, 19 Jul 2023 14:00:36 GMT</pubDate> <enclosure length="0" type="audio/mpeg" url="https://api.substack.com/feed/podcast/135258128/781706f2ff32fb43752bc927d1563061.mp3"></enclosure><content:encoded><![CDATA[<p> As first discussed on our <a href="https://www.latent.space/p/no-moat#details">May Emergency pod</a> and <a href="https://twitter.com/swyx/status/1679925361247911936">leaked 4 days ago</a> , Llama (renamed from LLaMA) was upgraded to Llama 2 (pretraining on <a href="https://twitter.com/swyx/status/1681372546929946624?s=20">2 trillion tokens</a> with 2x the context length - bigger than any dataset discussed in <a href="https://www.latent.space/p/datasets-101#details">Datasets 101</a> , and adding <a href="https://twitter.com/swyx/status/1681382937760239617?s=20">~$20m</a> of RLHF/preference annotation) and released for commercial use on 18 July. </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png" width="522" height="509.2682926829268" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:656,&quot;resizeWidth&quot;:522,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="Image" title="图像" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea2859eb-aa85-45da-9743-93b1d1c8bea4_656x640.png 1456w" sizes="100vw" fetchpriority="high"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div></figure></div><p> It <a href="https://twitter.com/abidlabs/status/1681337230432804866?s=20">immediately displaced Falcon-40B</a> as the leading open LLM <a class="footnote-anchor" data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1" href="#footnote-1">1</a> and was immediately converted/ <a href="https://twitter.com/swyx/status/1681368417457274880?s=20">quantized to GGML</a> and other formats. Llama 2 seems to outperform all other open source models in their equivalent <a href="https://twitter.com/swyx/status/1679241722709311490">weight class</a> : </p><div class="captioned-image-container"><figure><div class="image2-inset"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg" width="1456" height="1190" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1190,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Image&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal" alt="Image" title="图像" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F59d90d56-4b57-405e-ab6e-3aa2fbc2d81f_2060x1684.jpeg 1456w" sizes="100vw"></picture><div class="image-link-expand"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2"><polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line></svg></div></div><figcaption class="image-caption"> We covered many of these in <a href="https://twitter.com/swyx/status/1644411714270793729">Benchmarks 101</a></figcaption></figure></div><p> <strong>Why are open models important</strong> ? <a href="https://www.latent.space/p/open-source-ai">The intersection of Open Source and AI</a> is one of the oldest themes on this publication, and there has been a raging debate on the security and reliability of the OpenAI models and APIs. Users have reported <a href="https://news.ycombinator.com/item?id=36134249">GPT-4&#39;s quality going down</a> , which has been <a href="https://twitter.com/OfficialLoganK/status/1663934947931897857">denied</a> and <a href="https://twitter.com/npew/status/1679538687854661637">denied</a> and as of today, <a href="https://twitter.com/matei_zaharia/status/1681467961905926144?s=20">given some supporting data from Databricks</a> , and complained about the <a href="https://news.ycombinator.com/item?id=36622020">API reliability</a> and <a href="https://twitter.com/OfficialLoganK/status/1677682265311043584?s=20">rapid deprecation schedules</a> . Last and surely the biggest, there are entire classes of businesses and government/healthcare/military organizations that categorically <em>cannot</em> send any of their sensitive data to an external API provider, even if it is OpenAI through Azure. The only way to have total control is to own and serve your own models, which Llama 2 now pushes forward in terms of the state of the art (your own GPT3.5-quality model, though it is nowhere near Claude 2 or GPT-4).</p><p> As we do with breaking news, we got on to <a href="https://twitter.com/latentspacepod/status/1681413158555254785">Twitter Spaces again</a> to chat with two scheduled guests: </p><ul><li><p><span class="mention-wrap" data-attrs="{&quot;name&quot;:&quot;Nathan Lambert&quot;,&quot;id&quot;:10472909,&quot;type&quot;:&quot;user&quot;,&quot;url&quot;:null,&quot;photo_url&quot;:&quot;https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fdda47b96-836a-4b95-99a0-f0ec744d4245_2316x2316.jpeg&quot;,&quot;uuid&quot;:&quot;f85cdadb-d7d8-4f5f-9c6d-2f4c4e222de2&quot;}" data-component-name="MentionToDOM"></span> , ML Researcher at Huggingface and author of <a href="https://open.substack.com/pub/robotic">Interconnects</a> who had <a href="https://www.interconnects.ai/p/llama-2-from-meta">the best summary of the Llama2 paper</a></p></li><li><p> <a href="https://twitter.com/BornsteinMatt">Matt Bornstein</a> , organizer of the a16z infra team that launched <a href="https://www.llama2.ai/">Llama2.ai</a> ( <a href="https://github.com/a16z-infra/llama2-chatbot">source here</a> ) and has been coding up a storm with <a href="https://github.com/a16z-infra/companion-app/">AI demo apps</a> , unusual for VCs</p></li></ul><p> as well as <a href="https://twitter.com/atroyn">Anton Troynikov</a> of Chroma, <a href="https://twitter.com/russelljkaplan">Russell Kaplan</a> of Scale AI, and Omar Qazi of the <a href="https://twitter.com/WholeMarsBlog">Whole Mars Catalog</a> .</p><p>享受！</p><h2> Show Notes</h2><ul><li><p> Official links</p><ul><li><p> <a href="https://ai.meta.com/llama/">Website</a> , <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Paper</a></p></li><li><p> <a href="https://github.com/facebookresearch/llama/">GitHub</a> (<a href="https://github.com/facebookresearch/llama/commit/6d4c0c290aeec1fa4399694fefb864be5a153bb6">Llama 2 commit</a> )</p></li><li><p> <a href="https://blogs.microsoft.com/blog/2023/07/18/microsoft-and-meta-expand-their-ai-partnership-with-llama-2-on-azure-and-windows/">Azure Partnership</a></p></li><li><p> <a href="https://ai.meta.com/llama/use-policy/">Use policy</a> , <a href="https://about.fb.com/news/2023/07/llama-2-statement-of-support/">Statement of Support for Open Approach</a></p></li></ul></li><li><p> Where to try</p><ul><li><p> Llama2.ai ( <a href="https://github.com/a16z-infra/llama2-chatbot">source</a> ), <a href="https://llama.perplexity.ai/">Perplexity Llama Chat</a></p></li><li><p> <a href="https://replicate.com/a16z-infra/llama13b-v2-chat">Live playground/API on Replicate</a> , <a href="https://twitter.com/tuhinone/status/1681442567517511686">deploy all versions on Baseten</a></p></li><li><p> <a href="https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI">https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI</a></p></li><li><p> Dev ports - <a href="https://simonwillison.net/2023/Jul/18/accessing-llama-2/">simonw llm-replicate</a> , ggml using llama.cpp ( <a href="https://huggingface.co/TheBloke/Llama-2-7B-GGML">7B</a> , <a href="https://huggingface.co/TheBloke/Llama-2-13B-GGML">13B</a> ) or <a href="https://twitter.com/ReporterWeather/status/1681454162033389569?s=20">pinokio</a> , <a href="https://github.com/jmorganca/ollama">ollama</a> , <a href="https://huggingface.co/pcuenq/Llama-2-7b-chat-coreml">Core ML port</a></p></li></ul></li><li><p> Timeline</p><ul><li><p> 24 Feb - <a href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/">LLaMA 1 announced</a></p></li><li><p> 6 May - <a href="https://www.latent.space/p/no-moat#details">our No Moats podcast</a> - first mention of Zuck opening up Llama</p></li><li><p> 14 July - <a href="https://news.ycombinator.com/item?id=36724739">Llama 2 leaked</a></p></li><li><p> 18 July - Llama 2 announced</p></li></ul></li><li><p> Community notes</p><ul><li><p> <a href="https://www.interconnects.ai/p/llama-2-from-meta">Nathan&#39;s research paper recap</a></p></li><li><p> <a href="https://twitter.com/swyx/status/1681383751052562434?s=20">638 LOC, 4 dependencies</a></p></li><li><p> Usage restrictions - <a href="https://twitter.com/FanaHOVA/status/1681397083180507136?s=20">MAU restriction</a> , <a href="https://twitter.com/timohear/status/1681338337985019914?s=20">derivative models</a></p></li><li><p> <a href="https://twitter.com/swyx/status/1681360629335220224">Grouped Query Attention</a></p></li><li><p> <a href="https://twitter.com/swyx/status/1681365318227349504">System prompt</a></p></li><li><p> <a href="https://twitter.com/swyx/status/1681372546929946624">2 trillion token dataset</a></p></li><li><p> >;$20m price tag ( <a href="https://twitter.com/swyx/status/1681382937760239617">rlhf</a> , <a href="https://twitter.com/DrJimFan/status/1681372700881854465">jimfan</a> ),</p></li><li><p> Separate models for safety and helpfulness ( <a href="https://twitter.com/DrJimFan/status/1681372700881854465">jimfan</a> )</p></li><li><p> <a href="https://twitter.com/GuillaumeLample/status/1681346701766934543">Mistral AI founders left out of paper</a></p></li><li><p> Interesting fails:</p></li></ul></li></ul><p></p><h2> Timestamps</h2><ul><li><p> [00:02:30] Introducing the speakers</p></li><li><p> [00:03:32] Nathan Lambert intro</p></li><li><p> [00:04:48] General Summary of Llama 2</p></li><li><p> [00:05:57] Sarah Silverman killed Dataset Transparency?</p></li><li><p> [00:08:48] Simon&#39;s Recap of Llama 2</p></li><li><p> [00:11:43] Matt&#39;s Intro</p></li><li><p> [00:12:59] a16z Infra&#39;s new AI team?</p></li><li><p> [00:15:10] Alessio&#39;s recap of Llama 2</p></li><li><p> [00:17:26] Datasets 101 Followup</p></li><li><p> [00:18:14] Context Length 4k</p></li><li><p> [00:20:35] Open-ish Source? Usage Policy and Restrictions</p></li><li><p> [00:23:38] Huggingface Responsible AI License</p></li><li><p> [00:24:57] Pretraining Llama 2 Base Model beyond Chinchilla</p></li><li><p> [00:29:55] Llama 2 is incomplete? Race to publish</p></li><li><p> [00:31:40] Come for the Llama, stay for the (Meta) drama</p></li><li><p> [00:33:22] Language Translation</p></li><li><p> [00:35:10] Llama2&#39;s coding abilities</p></li><li><p> [00:35:59] Why we want to know about the training data</p></li><li><p> [00:37:45] The importance of Meta pushing forward Truly Open AI</p></li><li><p> [00:40:59] Llama 2 as Enabler of Startups</p></li><li><p> [00:43:59] Where you can try Llama 2</p></li><li><p> [00:44:25] Do you need dataset transparency if you have evals?</p></li><li><p> [00:45:56] >;$20m cost of Llama 2 is primarily preference data collection</p></li><li><p> [00:48:59] Do we even need human annotators?</p></li><li><p> [00:49:42] Models Rating Models</p></li><li><p> [00:53:32] How to get Code preference data</p></li><li><p> [00:54:34] Llama 2 Finetuning Ecosystem</p></li><li><p> [00:56:32] Hey Apple: Llama2 on Metal pls</p></li><li><p> [00:57:17] Llama 2 and Chroma</p></li><li><p> [01:00:15] Open Source MoE model?</p></li><li><p> [01:00:51] Llama 2 using tools</p></li><li><p> [01:01:40] Russell Kaplan on Scale AI&#39;s Llama 2 plans</p></li><li><p> [01:03:31] Scale annotating code?</p></li><li><p> [01:04:36] Immortality</p></li><li><p> [01:04:59] Running Llama on your phone</p></li><li><p> [01:06:54] Sama &lt;3 Satya &lt;3 Zuck? &quot;Azure as Launch Partner&quot;</p></li><li><p> [01:10:58] Meta &quot;Open Source&quot; Leadership</p></li><li><p> [01:11:56] Prediction: Finetuning =>; New Use Cases from Internal State</p></li><li><p> [01:13:54] Prediction: Llama Toolformer</p></li><li><p> [01:14:39] Prediction: Finetune-for-everything</p></li><li><p> [01:15:50] Predictions: Llama Agents</p></li><li><p> [01:16:35] dP(Doom)?</p></li><li><p> [01:19:21] Wrapping up</p></li></ul><h2> Transcript</h2><h2> [00:00:00] Introducing the speakers</h2><p> [00:00:00] <strong>Alessio Fanelli:</strong> There&#39;s not a single dull day in this space. I think when we started the podcast in January, a lot of people asked us, how long can you really do this? Just focusing on AI research and, and models. And I think the, the answer is clear now. A long time. So excited for this and excited to have Simon again.</p><p> [00:00:16] You&#39;re basically a honorary guest host of all of our Twitter spaces. Cool. Thank you.</p><p> [00:00:21] <strong>Simon Willison:</strong> No, it&#39;s great to be here again.</p><p> [00:00:23] <strong>Alessio Fanelli:</strong> And Nathan, thanks for joining us. Actually share your your writeup on, on Lama two technical details with Swyx this morning. So it&#39;s great to to have you here to dive into some of the details.</p><p> [00:00:33] <strong>Nathan Lambert:</strong> Yeah, sounds good. As probably clear Huggingface was trying to collaborate on releasing the model on the platform. So we ended up getting some early details, which made it a lot easier for me to cram study before the chaos hit.</p><p> [00:00:48] <strong>Alessio Fanelli:</strong> No, that&#39;s great. It, it&#39;s kind of what happened with the code interpreter episode when Sean and I had access for about five hours and Simon was like, I&#39;ve been playing with this for weeks and add all the, the insights scoops.</p><p> [00:00:59] So I think this will be a, a good episode.</p><h2> [00:01:02] Nathan Lambert intro</h2><p> [00:01:02] <strong>Alessio Fanelli:</strong> Maybe Nathan, you just want to give people a little bit of background on what you do at Hugging and Face and yeah, the, your experience with the LAMA two kinda preview. Yeah. So</p><p> [00:01:12] <strong>Nathan Lambert:</strong> I&#39;ve been a researcher and helping lead reinforcement learning from human feedback efforts at Hugging and face, which really means I do some research and I try to figure out how to fine tune models to do what people want.</p><p> [00:01:26] Generally we&#39;re trying to operate in the scale a little bit smaller than what Meta is doing cuz we obviously don&#39;t have that kind of resources at a startup. So I do a lot of technical research and also try to actually engage and communicate that with the community and specifically, Llama, I think I was most interested on kind of the research side.</p><p> [00:01:48] I think the paper is a phenomenal artifact and it&#39;s clear that the model is really strong in a lot of areas. And then kind of the big picture trends of where open source is going. Like this is a clear step in a direction that a lot of people wanted, but weren&#39;t sure if it was gonna happen.是的。</p><p> [00:02:04] <strong>Alessio Fanelli:</strong> What are some of the things that stood out to you?</p><p> [00:02:06] I think to a lot of the AI engineers audience that we have, they&#39;re not as deep into the details of the papers. We&#39;d love to get a, a read from somebody like you who&#39;s a much deeper at a, you know, model research level.</p><h2> [00:02:18] General Summary of Llama 2</h2><p> [00:02:18] <strong>Nathan Lambert:</strong> Yeah. It&#39;s like, where do I start? So I think as a general summary, the paper includes a lot of details on methodology. So like, what are the things that they did in their stack to build, to actually run this? And it misses a lot of details on. What does a specific data set actually look like? It&#39;s clear that they have a really fine-tuned data set and they paid a lot of money for these data sets.</p><p> [00:02:46] I think may like, it seems like now that both surge and scale are claiming some part in it, which I find hilarious. Cause it&#39;s really unclear, which are two of the probably biggest data labeling firms. So they kind of took the approach, meta took the approach of starting with open source preference data and then added a lot onto it.</p><p> [00:03:04] And the most interesting part to me on this preference data, which is a new technical approach, is they trained two preference models, two reward models, one toward making the model helpful and one for making the model safe. And then in terms of open source models, it&#39;s clearly more performant on kind of ground root benchmarks and then it&#39;s safer.</p><h2> [00:03:27] Sarah Silverman killed Dataset Transparency?</h2><p> [00:03:27] <strong>swyx:</strong> That&#39;s where I was</p><p> [00:03:28] <strong>Simon Willison:</strong> gonna wrap up to clarify, right. This is a big difference from the first LAMA paper. Cause the first LAMA paper was very, was so detailed in terms of how the training data worked, that people were able to essentially replicate it. And so you&#39;re saying that this new paper, there&#39;s, there&#39;s much less transparency as to how the training worked</p><p> [00:03:45] <strong>Nathan Lambert:</strong> on the DIS side.</p><p> [00:03:46] Yeah, I think they, they did a lot of new methodological things to, so taking the time to explain that like is not as much of a data focused paper. There&#39;s no table that is like, this is what the distribution of pre-training data came from. I would guess that it&#39;s a similar data set to the original llama with the kind of, they mentioned like one of the details that&#39;s really interesting is that they mentioned they up weight high factuality content.</p><p> [00:04:14] So things that probably seem like Wikipedia, that seems like they&#39;re doing some sort of up ranking. During base model training, but they don&#39;t de, they did some type of thing they didn&#39;t detail</p><p> [00:04:24] <strong>swyx:</strong> because it&#39;s also</p><p> [00:04:25] <strong>Simon Willison:</strong> worth mentioning, I mean, they&#39;re being sued right now by Sarah Silverman of all people. I mean, it&#39;s one of the many lawsuits flying around, but there&#39;s a lawsuit specifically over the training data involved in the first Lama because one of the things that went into that was this data set called Books three and Books three is like 190,000 pirated eBooks, like the full text of all of the ha Harry bot novels, things like that.</p><p> [00:04:45] Which, yeah, that&#39;s very difficult to say that that&#39;s not extremely copyrighted data. So I wonder if that&#39;s part of the reason they&#39;ve been less transparent this time round is that, you know, it got them in trouble last time.</p><p> [00:04:57] <strong>Nathan Lambert:</strong> Yeah. One of my colleagues on kind of the Ethics and Society time I side immediately pointed out that pub, publicly available data is the phrase often used in the paper, but that does not mean that it&#39;s free from copyright issues and or terms of service issues.</p><p> [00:05:11] It means that I could go on a computer and download it.</p><p> [00:05:13] <strong>Simon Willison:</strong> Right. If you, if you scrape the entire internet, very little of that stuff is actually like public domain.</p><p> [00:05:21] <strong>Nathan Lambert:</strong> Yeah. And, and I, I think without going down kind of social issues, rabbit hole right now, I think the notion of public is extremely being strained by AI and changing communication practices. And it&#39;s just like kind of those things where it&#39;s like, oh, okay, here we go.</p><p> [00:05:36] And they also use words like democratize and they have these sentences in the paper that are extremely value written, which is like the carbon footprint of our model. And releasing this is good because it&#39;ll mean a lot of people don&#39;t have to train models and burn more CO2 in the future. And it&#39;s like, okay, meta, like, like what?</p><p> [00:05:53] Where are you going with</p><p> [00:05:54] <strong>swyx:</strong> this? Yeah. Perhaps before we go too deep into the issues, cuz we, we have lots to talk about. I would also want to get a high level overview from Simon and from Matt who&#39;s also just joined us from a 16 and Z. So maybe Simon, you, you wanna go first with like, just recap for everybody what you think the relevant details are about LAMA two and, I mean, and we&#39;ll talk, we&#39;ll talk about Matt stuff.</p><h2> [00:06:18] Simon&#39;s Recap of Llama 2</h2><p> [00:06:18] <strong>swyx:</strong> Yeah.</p><p> [00:06:19] <strong>Simon Willison:</strong> So, yeah, I mean the, the, the, the headline here is that LAMA two has been released and meta kept their promise of doing a version of llama that is used, usable for commercial purposes, which is so big because so much of the, like, llama itself came out at the end of February, and so many models have been released on top of that.</p><p> [00:06:37] So, LA models like Vicuna, which was a fine tuned llama, all of them with the same, no, not, not usable for commercial purposes. Warning. So now we&#39;ve got a really high quality foundation model that we are allowed to use commercially. I think the the amount of innovation we&#39;re gonna see over the next few weeks is, is just going to explode.</p><p> [00:06:54] You know, I feel like this is, this is monumental on that front in terms of quality. I never know how to interpret these benchmarks. The benchmarks all look good. You know, the claims are, it&#39;s a bit better than, than Lama it&#39;s competitor with the GP chat, GPT 3.5, et cetera, et cetera. I have no reason to disbelieve that, but it always takes quite a while with these new models to get a feel for them.</p><p> [00:07:13] You have to spend time with them to really feel like, is it trustworthy as a summarizer, all of those kinds of things. My, my hunch is that it is gonna do turn out to be extremely good. Like I, I, I doubt that it&#39;ll, it&#39;ll, it&#39;ll, it&#39;ll turn out to be sort of a damp squib on that front. But yeah, so they&#39;ve released it.</p><p> [00:07:30] The It&#39;s available commercially and you are allowed to redistribute it, but the only way to officially get the waits is to fill in a form on their website and wait for them to approve you still, which is kind of stupid because obviously it&#39;s already started leaking. I&#39;ve down, I downloaded a version onto my laptop this afternoon, which, which worked.</p><p> [00:07:47] There&#39;s a GGML and the bloke thing that&#39;s floating around and hugging, hugging face already, so, you know, within. 24 to 48 hours. I think every possible version of this thing will be available to download without going through a waiting list. I&#39;m almost not sure why they, why they even bother with that.</p><p> [00:08:03] Especially since, you know, llama leaked within I within a few days last time and somebody ended up submitting a pull request to the GitHub Readme with a link to the BitTorrent for the LAMA models, which Facebook didn&#39;t delete. You know, they didn&#39;t sort of, They, they kind of like nodded and winked and said, yeah, this is what you can do.</p><p> [00:08:20] And now it&#39;s even legitimately okay to do it because the license says you can. But anyway, it&#39;s out there. You can run it on your computer right now today. The it&#39;s also hosted in a bunch of places. Yeah Andrea Horowitz got that sponsored, the version of it that&#39;s available on Replicate, although you actually do have to pay for that.</p><p> [00:08:37] I noticed that I built up 26 cents in, in replicate charges already playing around with that model. But it&#39;s api, so, so it&#39;s available via API or you can run it on your own machine and, you know, it&#39;s, it&#39;s open season. That&#39;s all start, start poking around with it and seeing what it can do.</p><p> [00:08:52] <strong>swyx:</strong> It&#39;s open season.</p><p> [00:08:53] Speaking of Andreesen, yes, Matt. Hey.</p><p> [00:08:56] <strong>Matt Bornstein:</strong> Hey. Hey everyone. Thank you for having me. And Simon, if you wanna send me a Venmo request for 26 cents, I&#39;ll, I&#39;ll happily reimburse you.</p><p> [00:09:02] <strong>Simon Willison:</strong> Absolutely. Yeah.</p><p> [00:09:04] <strong>Matt Bornstein:</strong> We, we may lose about $3 on the transaction fee, but I think it&#39;d be worth it</p><p> [00:09:09] <strong>swyx:</strong> just to throw in a term sheet in there for a data set.</p><p> [00:09:11] <strong>Nathan Lambert:</strong> You&#39;re good?</p><h2> [00:09:13] Matt&#39;s Intro</h2><p> [00:09:13] <strong>Matt Bornstein:</strong> No, I&#39;m, I&#39;m a huge data set fan. And, and, you know, we&#39;ve, we&#39;ve followed Simon&#39;s work for quite a while, and, and Nathan, it&#39;s, it&#39;s great to have a chance to share a stage with you. I think folks probably saw we you know, released a bunch of sort of, you know, VC version of evaluations. You know, we&#39;re way less smart than, you know, Nathan and Simon and a bunch of folks on the in the, in the space here.</p><p> [00:09:33] But using just sort of the. Does it feel good approach and trying to get a fairly representative sample across different types of prompts? The model seems very good. We were playing a lot with 13 B and we&#39;re playing now with 70 B, and it really does give you kind of very fast gpt 3.5 level responses to some questions.</p><p> [00:09:54] I, I think Simon&#39;s point about benchmarks is very well taken. It&#39;s hard to know how to interpret those. So, so we sort of go for the, for the direct version and for creative tasks. You know, especially it&#39;s, it, it seems very good so far. So, so a lot of what we&#39;re doing is just trying to get it out there as much as possible and, and, and as fast as possible.</p><p> [00:10:11] You know, II think we should all be incredibly, you know, appreciative that Meta is doing this and it, and it&#39;s not, you know, maybe quite perfect, you know, for some of the reasons that folks are are talking about. But you know, I think it&#39;s gonna be a huge unlock in open source LLMs and, and we&#39;re trying to, you know, just sort of support the community as much as possible.</p><h2> [00:10:29] a16z Infra&#39;s new AI team?</h2><p> [00:10:29] <strong>swyx:</strong> Yeah, I have to say, you guys are doing a bang up job recently. What, so what is, is there, this is a big team effort, right? Like I, I, I see that there&#39;s a number of names from your team, just essentially building projects and then collaborating on this this demo. Like maybe could just, could you describe like what it is andreessen&#39;s ACC sort, sort of involvement so far and like yeah.</p><p> [00:10:50] What, what, what is the scope of this? Yeah.</p><p> [00:10:53] <strong>Matt Bornstein:</strong> You know, we all applied for, you know L three engineer jobs and, and got turned down by all the, all the big tech firms. So we thought, hey, you know, we&#39;ll, we&#39;ll just do it our ourselves. Yeah. Look, I think, and this might be a little controversial, your average venture capitalist doesn&#39;t do any real work, and I completely include myself in this category, you know?</p><p> [00:11:14] Allocating resources to support teams is, is important. It&#39;s an important function in the economy, but it&#39;s, it&#39;s what you might call indirect work, which is you&#39;re supporting someone else doing something. You know, we just sort of made the decision when we really saw AI starting to take off that we should start doing real work too.</p><p> [00:11:31] And it&#39;s really just about supporting the ecosystem, especially around open source like Simon. We&#39;re massive believers that the innovation you see in open source is really gonna be a big unlock for AI based applications, right? Not everybody can just use. The Open AI API is good, as good as it is, and not everybody can train a model from scratch, right?</p><p> [00:11:52] Not everybody you know is, is Nome Shazi or, or someone like that. So so we think it&#39;s a really huge unlock and, and again, we&#39;re just trying to support as much as possible. So today we you know, we released a playground to play around with Llama2. We got it up on, on Replicate so people can just sort of try it with an API call and try integrating it into their apps.</p><p> [00:12:10] We released an AI starter kit over the last couple of weeks which people are actually using. We were shocked. We&#39;re, we&#39;re a little nervous cuz our, our code, you know, may or may not be production ready. But, but you&#39;ll see more and more of this from us over time.</p><p> [00:12:23] <strong>swyx:</strong> Yeah, I&#39;ve seen your companion chat bot, and I have to say, it&#39;s actually pretty impressive.</p><p> [00:12:26] It&#39;s got all the, is it the latest features in terms, especially in terms of streaming and lag chain and all the other stuff. So kudos to your team on that. Just to round out the overviews or the, the high level takes, before we go into individual details Alessio has been compiling the show notes, which we were gonna publish when this podcast goes live on lane space.</p><p> [00:12:45] Lessio, maybe you want to go over some of the, the notes that you&#39;ve been taking. Then I&#39;ll, I&#39;ll go over to Alex.</p><h2> [00:12:50] Alessio&#39;s recap of Llama 2</h2><p> [00:12:50] <strong>Nathan Lambert:</strong> Yeah, we</p><p> [00:12:50] <strong>Alessio Fanelli:</strong> got a, we got a lot of stuff to run through here. I think like the most interesting things that I read from the paper. One, there&#39;s a abandoned size model. So the 7 billion, 13 billion and 70 billion made it to release, but there&#39;s a 34 billion size that didn&#39;t make it.</p><p> [00:13:08] And in the safety chart, you can actually see it&#39;s like, Twice as unsafe, quote unquote. And they decided not to publish it because of lack of time to red team it. So I don&#39;t know if anybody had a chance to try the 34 B before the release, but I would love to learn, learn more about that. Outside of that, yeah, as Simon and Nathan were talking about, the data piece is a lot more obscure.</p><p> [00:13:31] So LAMA one was 67% common crop, 15% c4, a bunch of GitHub Vidia books as we mentioned. We don&#39;t have any information about LAMA two, but they did mention they have a 40% larger pre-training corpus. So they&#39;ve obviously been investing a lot in that. Also, yeah, the, the supervised, fine tuning was very interesting.</p><p> [00:13:52] I saw a tweet, somebody asked the laou how to kill a process, and laou was like, you can&#39;t kill things. And I was like, just a process. It&#39;s not a person. So I think in, in some places, the, it might have gone too far with the RLHF but that&#39;s another, that&#39;s another interesting side, right? Like if this is the starting point and like the defacto standard for open source models, are we okay with, you know, not being able to ask how to kill a Linux process?</p><p> [00:14:18] But I&#39;m not, I&#39;m not sure about that</p><p> [00:14:20] <strong>Nathan Lambert:</strong> yet.</p><p> [00:14:21] <strong>Simon Willison:</strong> I ran into that myself. I, I asked it to give me all of the animal emoji and it said that that would be disrespectful if it, if it attempted to do that, which was kind of interesting.</p><p> [00:14:32] <strong>Alessio Fanelli:</strong> Exactly. So that&#39;s a, that&#39;s an open question on open, you know, it&#39;s the Joel safety question.</p><p> [00:14:39] It&#39;s like, how much do we need to do before we release the smartest to the public versus what should that. The public side. The other thing is like, they should have let this GPUs burn for more. Like if you look at the, at the loss graphs, like these models are not saturated, I guess. Like they spent a lot of, a lot of money to try and train these.</p><h2> [00:14:56] Datasets 101 Followup</h2><p> [00:14:56] <strong>Alessio Fanelli:</strong> But it seems like there&#39;s a lot of work left to do there. We just did a data sets 1 0 1 episode that we released yesterday, which is already old news because now LAMA two is out and this is all the rage. But we talked about some of the scaling laws and we thought the 200 x was like the new LAMA ratio.</p><p> [00:15:12] But I think this one is 275 x Sean, I think.</p><p> [00:15:17] <strong>swyx:</strong> Yeah. So that&#39;s five. Yeah, 2 trillion tokens for seven B model. And that&#39;s, you know, that&#39;s up from 1.2 last time. So they, they&#39;ve definitely ramped up the, the, the amount of data and they, they just refuse to tell us any of it because, well, you know, guess what happened last time They, you know, they published the data, infra red pajama went and cloned you know, line for line exactly what was in the LAMA paper.</p><p> [00:15:39] So, you know, then that created, you know, red pa, red pajama model and then open lama as well.</p><h2> [00:15:44] Context Length 4k</h2><p> [00:15:44] <strong>Simon Willison:</strong> So I saw it says that the context length is up from the first lama. Do we know what the new context length is?</p><p> [00:15:50] <strong>Matt Bornstein:</strong> I think it&#39;s,</p><p> [00:15:50] <strong>Nathan Lambert:</strong> yeah, 4k. 4k.</p><p> [00:15:53] <strong>Simon Willison:</strong> Is that likely to be higher for the 70 B model or are they all the same context length?</p><p> [00:15:58] <strong>Matt Bornstein:</strong> I believe they&#39;re all the same and we have tested it a little bit and my intuition is that you can actually get more effective performance, more accuracy out of 4K rather than scaling up the way, say OpenAI have to 32 K or high. Like it&#39;s, I think it&#39;s just hard to find high quality. Training data. So it&#39;s when users actually start to submit longer inputs, performance kind of breaks down.</p><p> [00:16:22] And I&#39;m not talking about open AI specifically, but in general, and that&#39;s, that&#39;s my intuition on why you know, why meta is keeping it relatively small for these models.</p><p> [00:16:31] <strong>Simon Willison:</strong> I&#39;m kind of hoping that somebody, now that it&#39;s open source, somebody finds some clever trick to increase that. I&#39;ve been playing with the Claude 100,000 a lot recently and it&#39;s pretty phenomenal what you can do once you&#39;ve got that extra context length.</p><p> [00:16:43] <strong>swyx:</strong> There</p><p> [00:16:44] <strong>Alex Volkov:</strong> is actually a trick. It&#39;s called rope. We&#39;ve seen this with a two, two line change that you can, you can make Lama forget about the context it was trained on, and there was back and forth about how effective this is and whether or not it suffers from the same dip, you know, in the middle of the context.</p><p> [00:16:59] But this rope scaling trick then was verified by folks from, I think Microsoft, independently from that guy Kaiko, Ken Devrel, and I, I see some folks in the audience here who are participating in this. So apparently this applies to the previous LAMA and would likely apply to this next one as well.</p><p> [00:17:17] <strong>Simon Willison:</strong> That&#39;s pretty exciting. I can&#39;t wait to, this is the thing I&#39;m looking forward to is now that it open source. All of this stuff is go, these experiments are just gonna start happening at such, such, such a fast rate. This happened with Lamba before. You know, once you let every researcher in the world download and start tinkering with your model, people start finding optimizations and, and new tricks at a, at a crazy rate.</p><p> [00:17:37] It&#39;s gonna be really interesting.</p><p> [00:17:39] <strong>Nathan Lambert:</strong> So</p><p> [00:17:39] <strong>Alex Volkov:</strong> I think the interesting piece here is to see whether or not the commercial license will unlock even more, or did the researchers didn&#39;t care and kinda threw the kitchen sink of everything they wanted to hack together on the previous llama. I&#39;m thinking because it&#39;s open source commercially now companies will actually start, you know, doubling down because there will be able to then use the fruits of their labor on commercial purposes.</p><p> [00:18:02] So we&#39;ll likely see</p><p> [00:18:04] <strong>Alessio Fanelli:</strong> more.</p><h2> [00:18:05] Open-ish Source? Usage Policy and Restrictions</h2><p> [00:18:05] <strong>Alessio Fanelli:</strong> I think you guys use the magic word, which is open source, and everybody has a, has a different, different definition. And I know we had Tom Warren in the audience who asked the question about this. So Tom, I&#39;m gonna invite you up to speak if you&#39;re around.</p><p> [00:18:18] <strong>Simon Willison:</strong> Yeah. I&#39;m gonna say, call it, I, I say openly licensed, not open source, because I feel like open source has a definition, this doesn&#39;t quite apply here.</p><p> [00:18:27] <strong>Alessio Fanelli:</strong> Yeah, yeah, exactly. If you go, actually on my website, I wrote like a 10,000 words thing on like the history of open source licensing, and there&#39;s things that are open source, things that are somewhat open source in traditional infra, that&#39;s like the server side public license. Some of these things that like Elastic and Mongo came up with to avoid the awsapi compatible in quotes products that were literally just the same thing.</p><p> [00:18:51] So yeah, it&#39;s, it&#39;s really curious also that the breakpoint for the LAMA license is 700 million monthly active users, which is. A lot of users obviously, but there&#39;s some notable people that go over it. So Snapchat is one company that is obviously a, a close competitor to, to meta TikTok, isn&#39;t there?</p><p> [00:19:10] YouTube, by far exceeds that</p><p> [00:19:13] <strong>Simon Willison:</strong> amount. Yeah. It&#39;s worth noting, but that&#39;s actually, that&#39;s not a rule going forward as of the date of the release. If you have 700 milli monthly users, you can&#39;t, you, you have to get an extra license from, from Meta. If you manage to achieve 700 million million monthly extras next week, you could still use it.</p><p> [00:19:30] Like it&#39;s, it&#39;s, it&#39;s, it&#39;s that point in time that</p><p> [00:19:32] <strong>swyx:</strong> matters. Yeah, at that point they should just name people. But yeah. Just to close the loop on this open source element, you know, there&#39;s one other piece of about the open source or, or the usage policy, which is you can&#39;t use it to train any other model.</p><p> [00:19:44] Thou shalt not have any other models before llama. Llama is your only model that you can fine tune with, with llama data.</p><p> [00:19:52] <strong>Simon Willison:</strong> I think it&#39;s more than that. This is they&#39;re protecting against distilling the model, right? The thing that everyone&#39;s been doing, like Una was trained on Chachi PT data, despite open AI having a thing in their terms, it says you can&#39;t train a competing model.</p><p> [00:20:04] I don&#39;t, I&#39;m really frustrated by this because the, the language says you cannot train a competing large language model. But what does that even mean? Who gets to decide what a large language model is? If in six months time we invent a new architecture is that&#39;s still an ll M that&#39;s covered under those terms.</p><p> [00:20:20] It&#39;s, it&#39;s frustratingly vague.</p><p> [00:20:22] <strong>Nathan Lambert:</strong> Yeah, these clauses are kind of bogus. We talk about them a lot of hugging base. And it seems also from a legal perspective, the things that they&#39;re grounded in, like terms of service are being walked back in kind of this digital domain. And then also it&#39;s just like unclear what is actually using the language model.</p><p> [00:20:40] So all these things where people use language models as a judge, or you can just generate a bunch of interesting prompts to then modify them. It&#39;s so ridiculous to even think of trying to enforce these clauses. It&#39;s surprising to see it show up,</p><p> [00:20:54] <strong>swyx:</strong> which you have to note, like in the LAMA two paper itself, they also use other company models to do their evaluations.</p><p> [00:21:02] Right? Like so and I, and you know, a strict reading of the, of those clauses would not allow them from from that.</p><h2> [00:21:08] Huggingface Responsible AI License</h2><p> [00:21:08] <strong>swyx:</strong> Nathan, actually a quick follow up. Hugging face has its own license, the rail license. I think there was some iteration following this stable diffusion release. Would you, would that be appropriate for something like Alama two?</p><p> [00:21:19] <strong>Nathan Lambert:</strong> Yeah, I think it&#39;s good. I don&#39;t have a hundred percent knowledge of rail. My understanding is that it&#39;s like, generally the goal is to be like commercially available with good intention and then there&#39;s kind of like, it starts to try to give leverage for people to come after bad actors using their models.</p><p> [00:21:37] I, I think the commercial use of this is gonna be off the charts very soon, like at hugging face. A lot of the monetization efforts are around like trying to enable commercial use of open source language models. And the license questions have been a constant discussion for the last six months from things we&#39;re trying to build and from customers.</p><p> [00:21:57] So like this is definitely going to</p><p> [00:21:59] <strong>swyx:</strong> be used. Yeah. Yeah. Okay. So I don&#39;t, it&#39;s, it&#39;s do we have, we have a lot of you know, insightful people here.</p><p> [00:22:07] I feel like the, the best way to organize this space is maybe to just kind of try to stick to as, as many sort of factual elements as we, as we can.</p><p> [00:22:15] I feel like Nathan, since you&#39;ve done the most work you&#39;ve had the most time with the paper, to be honest. What El maybe sort of pick on one other sort of element of, of the paper that you, that you find worth discussing and we can kind of go into that.</p><h2> [00:22:27] Pretraining Llama 2 Base Model beyond Chinchilla</h2><p> [00:22:27] <strong>swyx:</strong> Maybe the, sort of the, the pre-training base model stuff.</p><p> [00:22:30] <strong>Nathan Lambert:</strong> Like, I, I don&#39;t think there&#39;s a lot on the pre-training. The, there&#39;s definitely an important thing that makes it able to be used, which is they use, like, what is cqa? It&#39;s like cross query attention, which will make inference on the bigger models faster. I think there&#39;s kind of a asterisk that is interesting on that code and math and reasoning seems pretty.</p><p> [00:22:49] Not emphasized in the paper, and that&#39;s what their kind of like market for. That&#39;s what ChatGPT is used by a lot of people on this call for. I think at a technical level, the Rh f details are the most fleshed out that we have seen. Sure. And kind of confirm a lot of the capabilities we&#39;ve seen insinuated by anthropic and open ai.</p><p> [00:23:11] So that was like kind of a relief for me as someone that&#39;s trying to be like, I still think this really works. And they dropped this paper is like, we really like this, which was not guaranteed. I, I have one</p><p> [00:23:22] <strong>Matt Bornstein:</strong> pre-training question. And this is for you, Nathan, or, or for the whole group. Like we, we talked about it before.</p><p> [00:23:27] The, the amount of pre-training data here goes far beyond chinchilla optimal and the loss curves were still going down when they cut it off. Like, are we ready to say that chinchilla optimal is just not optimal anymore?</p><p> [00:23:43] <strong>Nathan Lambert:</strong> Oh, I&#39;m ready. I never really cared about it. Like I think data quality is changing that completely.</p><p> [00:23:51] It&#39;s like, I think when Gent came out, data quality standards were so different and given what the practices are now, I, it&#39;s like, what does it mean?</p><p> [00:24:03] <strong>Matt Bornstein:</strong> It was a really big deal at the time though, right? I mean, it was kind of this breathtaking result that if you just ramp up training data much higher than you thought or people had been doing, you just kept getting better performance.</p><p> [00:24:15] May maybe Nathan, since you&#39;re, you know, the most knowledgeable on this space, like can you just like, give us a little intuition, like when you say better data quality, like what exactly is happening under the hood that makes this possible now?</p><p> [00:24:26] <strong>Nathan Lambert:</strong> Oh, they&#39;re removing. Okay. Think about all the tweets and texts that everyone sends, and we have these weird insider jokes and phrasings that we do.</p><p> [00:24:37] They make no sense if you read them and your language model, like half reproduces them. So like, and like I&#39;ll say like you got got, or something that is just very confusing from like a token prediction state point of view, and then also a ton of just errors. It&#39;s like I write a blog post. I used to not take it as seriously, I&#39;ve like published a blog with a half finished sentence in it.</p><p> [00:25:00] It&#39;s like they would just scrape that and take it, but trying to actually get data that is complete is, is consistent, is just extremely hard. I think technical terms are like deduplication, so you don&#39;t wanna pass the model, the same text, even if it came from different websites and there&#39;s tons more that goes into this.</p><p> [00:25:21] I, I don&#39;t think it&#39;s the area of my most expertise, but I think it&#39;s actually pretty simple. You just wanna put good text into the model and understanding what good text is on the internet is really hard.</p><p> [00:25:34] <strong>Matt Bornstein:</strong> So you&#39;re sort of saying the reason people were using not enough data initially is cuz they just weren&#39;t good enough at cleaning it. And now that those methods have advanced so much, we&#39;re moving duplicates better, we can measure quality better, all of that. Like, like do you think we&#39;re gonna keep going up, I guess is the question like this, you know, they trained a seven B model on 2 trillion tokens.</p><p> [00:25:52] Like, do you think that&#39;s like the Maxim or are we gonna keep going?</p><p> [00:25:55] <strong>Nathan Lambert:</strong> I kind of like, I, I think the intuition on like what you&#39;re saying is how getting more higher quality data is making it so using more works better. I like, that&#39;s what everyone in my circles is saying is the trend and given machine learning in the last few years, I think trends tend to be stickier than most people expect them to be.</p><p> [00:26:17] So I would expect it to keep going. I just kind of trust the process to continue for a lot of stuff like this.</p><p> [00:26:22] <strong>swyx:</strong> Yeah. So we on our podcast, we&#39;ve been asking everyone that we can possibly CAGR ask about, you know, went from two x tokens to perran ratio with Kaplan, and then 20 x with chinch, now 200 x with llama, like someone&#39;s gonna try 2000.</p><p> [00:26:37] Right? We did have a response today from one of our previous guests Varun of Codium who said that they did try a thousand to one tokens, to params ratio. And it definitely gone into the range of overfitting. So your loss can continue to go down, but you&#39;re not sort of measuring overfitting in, in, in, in some of that respect.</p><p> [00:26:53] So it&#39;s, it&#39;s very unclear. I would say though, you know, I, I do have visual sources like. Chin. It&#39;s not that chinch was wrong. Chinch was optimizing for a particular set of assumptions, particularly the pre-training compute budget, right? Compute optimal sort of scaling laws. And if you look at the llama paper right on the first page, I have it open right in front of me.</p><p> [00:27:12] They actually criticize that and say like, you know, this, this disregards the inference budget which is critical when you&#39;re actually serving the model instead of just optimizing for a pre-training compute objective. And as things move from research into production, inference starts to become more, more of a concern.</p><p> [00:27:28] Resource constraints starts becoming more of, more of a concern. And so I, I, I think it&#39;s actually quite reasonable to move on from chinchilla, which is a very important result. And, and say that, you know, we are, we are exploring very different objectives as compared to, you know, more than a year ago when Chinchilla was published.</p><h2> [00:27:45] Llama 2 is incomplete? Race to publish</h2><p> [00:27:45] <strong>Nathan Lambert:</strong> Yeah, I agree. I was just gonna say that I feel like the was going down like all of these fa reading the paper, it feels like this is a checkpoint of a much longer term project. They like readily list off things that they didn&#39;t get to but they want to continue and like capabilities or something.</p><p> [00:28:03] Some of the methods seem like kind of hacks to make things work that they didn&#39;t know if didn&#39;t get to work. Like Anthropic came up with context distillation, which is a way of getting a really, the behavior of a really long system prompt into a shorter prompt essentially like, and, and they did something like this in this paper to get the P model to behave like characters for longer conversation turns.</p><p> [00:28:27] And like, there&#39;s all sorts of little things that I just think meta is going to continue this and.</p><p> [00:28:34] <strong>Simon Willison:</strong> So that&#39;s kinda fascinating cuz that that implies that the, the actual story here, it&#39;s the AI arms race, right? It&#39;s, it&#39;s, it&#39;s Zuckerberg saying, no, we need to get something out right now. Get it to a point where it&#39;s good enough and safe enough and then let&#39;s ship it.</p><p> [00:28:46] And it&#39;s not so much that they, they, they didn&#39;t necessarily have time to get to the sort of perfect point that they wanted to get to.</p><p> [00:28:54] <strong>swyx:</strong> Yeah, that is the I have asked people about this offline, and so I was like, okay, so why don&#39;t people throw a lot more compute at this? And they&#39;re like, you know, as long as you have a state-of-the-art model, you should just ship it and get credit and then wait till, like, wait a few months and then get the next version out.</p><p> [00:29:08] That way you have a lot more shots on gold.</p><p> [00:29:11] <strong>Simon Willison:</strong> That totally makes sense. Yeah.</p><p> [00:29:14] <strong>swyx:</strong> And I was like, oh, okay. Like we are in such early stages that honestly, I mean, they spent 3 million G p U hours on this thing. They could spend 30 million in, like, obviously it would be way better. Like we&#39;re in such early stages that even these relatively simple.</p><p> [00:29:27] Like don&#39;t forget Lama one was published in February of this year. We&#39;re in such a easy cycle where it, it&#39;s, it&#39;s still within, you know, the order of months to make and improve one of these things. That it&#39;s not too terrible.</p><h2> [00:29:40] Come for the Llama, stay for the (Meta) drama</h2><p> [00:29:40] <strong>swyx:</strong> I do, I guess I should also mention a shout out that Not every person who worked on LAMA two is on the paper.</p><p> [00:29:48] Guerro Lampel and who&#39;s, who&#39;s one of the co-founders of Misra, the French startup that raised like a hundred million C round. Apparently worked on LAMA two and they left him out because in, they left his team out because they left Meta before this paper was published. So interesting passage.</p><p> [00:30:03] Treat there. If anyone wants to go through that,</p><p> [00:30:05] <strong>Alessio Fanelli:</strong> come for Alama, stay for the drama. Oh, it&#39;s hard. It&#39;s hard to read, you know, into like the, as you know, especially when it comes to like, work that then goes over source. It&#39;s always we did the work. We didn&#39;t I don&#39;t know, since, since nobody here worked at Meta I would rather not go, not go down that path.</p><p> [00:30:23] Yeah,</p><p> [00:30:23] <strong>swyx:</strong> I, I&#39;ll just leave a bookmark there. Okay. Yeah, but exactly.</p><p> [00:30:26] <strong>Nathan Lambert:</strong> We&#39;re not in the room there. I,</p><p> [00:30:28] <strong>Matt Bornstein:</strong> I, I&#39;m for one shocked to hear that there may be drama among researchers. I&#39;ve, I&#39;ve never heard of that happening before.</p><p> [00:30:34] <strong>Nathan Lambert:</strong> Right. Near, especially after three organizational restructures of researchers hopping, playing hopscotch from one org to another, and being in between, in between jobs.</p><p> [00:30:43] I don&#39;t know.</p><p> [00:30:45] <strong>swyx:</strong> All right. Alex, do you have your hand up? And then I wanted to dig more on the the preference data that Nathan mentioned. Mm-hmm.</p><h2> [00:30:52] Language Translation</h2><p> [00:30:52] <strong>Alex Volkov:</strong> Hey guys. Just to introduce myself real quick, I&#39;m Alex. We participant in the spaces is, and my angle and the way I vibe, quote unquote vibe check models is via languages.</p><p> [00:31:03] And to me, it was really surprising that they released kind of the second iteration while also knowing how much meta actually does for translation. They have very famous NLLB models, no language left behind. They released the world models that you can speak in multiple, like a thousand languages that understands, and for some reason, they&#39;re open source models.</p><p> [00:31:23] They are not very strong multilingually. So we&#39;ve seen this with GPT4, which was way better at multilingual speak. Claude highlighted this point with Claude two that is like way better at the blue score. I think for, for languages, and I&#39;ve tried and my go-to like vibe check with these models is to, with the, especially the open source one is the ability to translate, the ability to understand the languages.</p><p> [00:31:46] I&#39;ve tried it with, with Hebrew a little bit. I&#39;ve tried with. Very, very impressed. Now, obviously fine tuning will come and obviously people will fine tune these, these models towards different outcomes, but it&#39;s very interesting considering how much meta does elsewhere for languages and to bring the world together.</p><p> [00:32:02] How much kind of this model did not focus on this, this specific kind of issue. And the, the, the second thing is also code. I know you guys talked about human eval. That&#39;s fairly low in terms of the score out of the box. And obviously fine tuning will, will, will make it better, but fairly, fairly disappointing score on, on human ev, right?</p><p> [00:32:22] Fairly low coding abilities. And we&#39;ve seen previously that there&#39;s some assumption that training on more code in your dataset actually gives you better kinda logic and reasoning abilities. So kind of surprised that that was fairly low. We went to chairman with these two, two examples about Lama.</p><h2> [00:32:40] Llama2&#39;s coding abilities</h2><p> [00:32:40] <strong>swyx:</strong> I&#39;ll say on the human eval piece don&#39;t count it, not just yet. So I&#39;ve, I&#39;ve had some dms with Quinn Slack or of source graph, and he&#39;s is you know, very actively building Cody their, their coding assistant bot. And it&#39;s well known that human eval is not a very good or reflective measure of how we use coding chatbots.</p><p> [00:32:59] And so like, it, it is probably human EV emails is probably overrepresented in terms of being, being like this effectively the sole benchmark by which we value code models. We, we just need new benchmarks for code.</p><p> [00:33:11] <strong>Matt Bornstein:</strong> I do think it&#39;s possible better instruction tuning will improve code performance of the LAMA two models as well, because their reasoning capabilities are actually relatively good. Not perfect, but relatively good, which makes me think there may be more code in the pre-training than it seems.</p><p> [00:33:26] <strong>swyx:</strong> Well it&#39;s difficult to know cuz they don&#39;t talk.</p><p> [00:33:29] We&#39;ll, we&#39;ll see, we&#39;ll see.</p><h2> [00:33:31] Why we want to know about the training data</h2><p> [00:33:31] <strong>Simon Willison:</strong> I mean, this is the thing that&#39;s so infuriating about these opaque models that don&#39;t talk about their training data is as users of the models, we need to know, we need to know how much, like if it&#39;s had code in it, all of those kinds of things in order to make decisions about what we&#39;re going to use it for.</p><p> [00:33:45] So I kind of feel like you know, the, the, the secrecy around these models really hurts me as a consumer of these models, just from a practical point of view of being able to make good judgements about what the model&#39;s gonna like to be able to do.</p><p> [00:33:55] <strong>Matt Bornstein:</strong> I, I do think that&#39;s true, Simon. You know, I wanna make just one defensive of Meadow, which is like, this is pretty amazing what they&#39;ve released and they&#39;ve, you know, given to the world, obviously it may benefit them commercially as well, but you know, it actually carries pretty substantial risks for them and actually think it&#39;s kind of a courageous act to, to release and, you know, so it, and it&#39;s the things like the training data.</p><p> [00:34:20] Safety that like really, you know, when you&#39;re, when you&#39;re meta and you have billions of, of active users, like you, you actually are taking a pretty big risk with these things. And, you know, regulatory bodies have their sights on you. So I, I do think you&#39;re right. I, I just, I, you know, for what it&#39;s worth, wanna I agree with, I agree with, it&#39;s actually a</p><p> [00:34:37] <strong>Simon Willison:</strong> positive thing.</p><p> [00:34:38] I agree with everything you say, but at the same time, right now, I&#39;ve got a whole bunch of models that I&#39;m choosing to be to, to, that I&#39;m trying to choose between, and I don&#39;t have the information I need to make the decision. I feel like at some point it&#39;s going to be a competitive advantage to put out a model with transparency of the data over, over what went into the data.</p><p> [00:34:55] Cause people will be able to use that model more effectively. But yeah, I completely understand these strategic challenges that I&#39;m, I&#39;m astonished that meta went ahead with this release. I never thought they&#39;d, they&#39;d take the risk of releasing something like this and someone use it for something bad and now they&#39;re on the front page, all of the, all of the papers for it.</p><p> [00:35:12] So yeah, I&#39;m, I&#39;m super excited about it on that front. I wanna</p><h2> [00:35:15] The importance of Meta pushing forward Truly Open AI</h2><p> [00:35:15] <strong>Alex Volkov:</strong> ajo. Yeah. I know from the perspective of releasing something as open source as they did previously we didn&#39;t have commercial licensing, obviously. Now the big thing is we have commercial licensing, but the amount of people, I don&#39;t know if you guys noticed, but like the amount of people who signed, quote unquote in support of releasing these models, Paul Graham and Mark Andreesen, and like a bunch of other folks, like in addition to the model, they also released kind of a counterweight to the moratorium papers and all the AI safety stuff.</p><p> [00:35:41] Because there was a, an FTC pro, right? There was like some, some regulatory stuff talking about the previous releases of LAMA from, from a long time ago. And now not only they released like the, the, the, the quote unquote open source. So unless it doesn&#39;t, doesn&#39;t kick me off here. Not fully open source, but definitely we&#39;re able to use this commercially.</p><p> [00:36:00] But they also released kind of a industry leaders selling like the, the, the open source is needed. And I think that. That, like, gives a very strong counterweight to the M and the keep, keep it closed and don&#39;t release kind of thing. We saw, and it&#39;s very interesting. It comes from meta specifically.</p><p> [00:36:16] So in addition to the courageousness that they did, it looks like they&#39;re also kind of leading the industry in terms of like, this is how to do fully commercial again, quote unquote open source, not open source license, but this is how to release models in a, in a, in a safe way. So definitely joining the, the courage and the applauds for meta and the team.</p><p> [00:36:35] <strong>Nathan Lambert:</strong> Yeah, I just don&#39;t think that like, like the cu we&#39;re not the customers of meta with respect to this model. I think they&#39;re trying to build these for their own purposes and then they have very strong, like, I think it&#39;s kind of the principles of like transparency and research that these organizations at Meta have stood by. And I think that&#39;s like the newest representation of it, more than like, and I don&#39;t think they&#39;re trying to make money off releasing this in any way. Like there is an ecosystem perspective of like where AI content proliferates, there&#39;s more creativity for their users and that enables social media and things.</p><p> [00:37:08] But I think we&#39;re still pretty far from that. And it&#39;s more of like a values and internal research and development tool for themselves. Like is there a way for them to make money directly off of this NPCs</p><p> [00:37:19] <strong>Alessio Fanelli:</strong> and the Metaverse. But I mean, I don&#39;t know.</p><p> [00:37:23] <strong>swyx:</strong> Well, so we, we, we last hosted one of these emergency pods, I think maybe two, two pods ago.</p><p> [00:37:28] Which was I think in May where we did our when the No Moats memo came out from Google. And we actually talked a little bit about what an ecosystem around a language model looks like when you have stackable loras customizing and fine tunes that are based on top of an existing base model that is well known.</p><p> [00:37:48] I, I think that might be part of the strategy there. You know Facebook is also well known for releasing, I guess, PyTorch and, and React. And, and those are very well, like, they don&#39;t make money from that directly, but they definitely do benefit from the ecosystem that has sprung around it, that, that essentially represents a lot of free development from, from the open source community.</p><p> [00:38:07] <strong>Simon Willison:</strong> I think there&#39;s a lot to be said. The fact that meta AI are at the very heart of openly licensed language model research, and that&#39;s because of Lama, you know, Lama came out and it kicked off this immense tidal wave of interest and of activity with meta ai right at the very center of that. And in the world that we live in right now, being at the very center of all of the research and innovation happening around language models feels like a really valuable place to be.</p><h2> [00:38:31] Llama 2 as Enabler of Startups</h2><p> [00:38:31] <strong>swyx:</strong> Yeah, it, it, it really is. II, and maybe we can go to a little bit to, to Matt again. One thing I wanted to get your thoughts on that, you know, I don&#39;t know how long you have with, with us, but is the impact on the startup ecosystem, right? Like how, how big of an enabler is this? Or does this, I guess just commoditize everything to a point where, you know, everyone&#39;s just rappers.</p><p> [00:38:50] <strong>Matt Bornstein:</strong> I think it&#39;s a really, really massive deal. You know, we&#39;ve met with. Conservatively hundreds of AI startups now maybe, maybe thousands. We&#39;d have to go back and look and, and, and I sort of alluded to this before, but the really big dilemma is do I train my own model or do I just use something off the shelf?</p><p> [00:39:15] And we&#39;re really, we&#39;re increasingly seeing that the answer for almost everybody is kind of a hybrid approach. We&#39;re seeing increasing number of startups, basically triage. Their AI workloads where if things require, you know, really high levels of accuracy and you know, human like text generation, GBT four is the only answer.</p><p> [00:39:38] But many queries or workloads actually don&#39;t require that, right? So you can kind of scale down and say, you know, for a really simple query, I can use, you know, an open source model off the shelf for something in the middle. I can fine tune for various tasks and then you can get pretty sophisticated about what you route, where all of that is only possible if we have commercially usable, really high quality language models and especially ones that have been efficiently trained such that latency is, is, is low and cost is relatively low.</p><p> [00:40:09] So I think what we&#39;re gonna see happen is there&#39;s gonna be a, a big push for startups to use. Lama two models and, and other open source models that have similar levels of performance. Fine tune it in ways that actually work for specific tasks, right? Not for specific data, like I think that was sort of a head fake, but for, for specific tasks and, and really be able to build more defensible businesses that way.</p><p> [00:40:34] You know, this, there&#39;s nothing wrong with using OpenAI. That&#39;s fantastic, but it&#39;s probably not good to make that a hundred percent of your business. And, and a lot of founders are doing that now. So, so that&#39;s why I think this is, this is such a huge deal and, you know, the, the progress just today has been amazing.</p><p> [00:40:51] Like, there&#39;s gonna be, by the end of today a number of hosts where you can just easily use The Lama two models, like right outta the box, you know, replicates one that we work with, but there there are others as well. You know, you can already run it on your local computer with two bit precision, which is kind of crazy if you stop and think about that for a second, that with two bits you can actually run a super advanced language model on your own computer.</p><p> [00:41:15] So I, I think I, I just think this is a huge, huge deal for startups and I think if you&#39;re a startup founder working in ai, you know, you, you really should be taking a look at, at open source models now and seeing how they, how they can be used to, to kind of deepen your moat and, and, you know, build a really great AI product.</p><h2> [00:41:34] Where you can try Llama 2</h2><p> [00:41:34] <strong>swyx:</strong> Right. So me, I would like to help fill in the blanks. So apart from Replicate, it looks like hugging Face has also launched an inference endpoint for that. And as far as I know, it&#39;s one of the only few ways to try the 70 B model off the shelf. I think Base 10 has also maybe put something up. And then for the, for the two bit quantized model, you can look at the G GML ecosystem.</p><h2> [00:41:55] Do you need dataset transparency if you have evals?</h2><p> [00:41:55] <strong>swyx:</strong> Yeah. And, and then I also wanted to recognize one of the other respondents in our chat, we have a little, little comment window here. ARD Doshi was responding, I think, to Simon. And, and I, I did actually have a pushback, right? Like, we don&#39;t have to know. The full data sets of of Lama as long as we are able to eval for everything that we want to know about.</p><p> [00:42:13] I think we actually have to live with AI becoming more and more of a black box. Even though the, the mo the the weights are open I mean for me it</p><p> [00:42:20] <strong>Simon Willison:</strong> comes down to model competition. If I have two equally capable models and one of them, I know what&#39;s in it, them, I don&#39;t, and I&#39;m gonna use the open, the, the, the more, the more transparent one.</p><p> [00:42:30] And I&#39;m hoping, because there are so many models competing now, I&#39;m hoping this becomes one of the factors that models compete with each other on</p><p> [00:42:38] <strong>swyx:</strong> I&#39;m, you know, dataset non-transparency I guess is like an emerging theme because like, it&#39;s not like we had that for Falcon either. So yeah, we can</p><p> [00:42:47] <strong>Simon Willison:</strong> hope for it and that&#39;s a huge problem, right?</p><p> [00:42:49] Falcon, if you ask Falcon about human rights abuses in, in the Middle East, it has some very different opinions and I want to understand why. I want to know how they got it to, to do those things.</p><p> [00:43:00] <strong>swyx:</strong> Yeah, yeah, exactly. Yeah, we won&#39;t know. And we can, all, we can, all we can do is ask for more transparency there.</p><p> [00:43:06] But I do, I do support the you know, the concepts of building a business on open source models. Because open AI will not randomly deprecate your models on you, you know, every three months. And I do think that for people who want a certain level of stability and are okay with trading off not being state of the art in three months I think that is a perfectly reasonable tradeoff.</p><h2> [00:43:26] >;$20m cost of Llama 2 is primarily preference data collection</h2><p> [00:43:26] <strong>swyx:</strong> Okay. I wanted to go back to Nathan A. Little bit and talk a little bit more about the preference data and the RRLHF data. So you estimated a 25 million cost for LAMA two. And as far as I can tell, That&#39;s, that&#39;s actually primarily data collection, not GPUs.</p><p> [00:43:46] <strong>Nathan Lambert:</strong> Yeah. This is based on kind of our pilot contract to do preference data collection at hug and paste cuz we can give, like we&#39;re collecting a small amount of data in a similar way and if you do a back of the envelope cost calculation and scale it up by whatever, like 10 or a hundred x that what they did, then you get towards this 20 million number and it could be higher depending on how many flags they end up using in their data.</p><p> [00:44:12] So I think what they did was safety is pretty interesting. So they like separated it and collected metadata and that means they could also collect other metadata during the process. And as you kind of add more knobs to the preference data collection because it takes longer for people to do the task and the cost goes up.</p><p> [00:44:29] So I think pretty safe to say order of 10 million, especially given, because that&#39;s what was rumored with open AI around ChatGPT and everything like that. So, It is not a shock at all to me. And, and is the</p><p> [00:44:43] <strong>swyx:</strong> focus on multi turn significantly higher or, you know, comment worthy I guess?</p><p> [00:44:49] <strong>Nathan Lambert:</strong> Not really. So generally when doing on setting this up, it comes down to per pro, like how many tasks the workforce is gonna do.</p><p> [00:44:58] And you could do an instruction prompt, which is one turn, or you could do a four turn chat and that would, you&#39;d generally be able to trade off the number of labels that you get in that respect. So I think the multi turn is more because open source data sets don&#39;t contain a lot of that, which is something that we found in, in our work as well.</p><p> [00:45:16] And they did that because they needed the model capabilities and they needed to train a preference model that can do that. And I agree, I, I think they must have figured that out months ago. Cause this also takes a lot of time how it works generally. You can see this in the paper, how they say they have these RH F versions and generally what happens is, You sign a contract and then these people sit you down and they&#39;re like, we are gonna try to do this over batches and we scale up the amount of data we&#39;re sending over time so that we can do calibration.</p><p> [00:45:43] And each batch you get some data from the vendor and then you look through the samples and you see what you like and you see what you don&#39;t like and then you change it going forwards. And what they did is they took those batches and they trained a model iteratively and then they saw what their model needed and they went back to the vendor to say, okay, we need more data in this regard to improve things.</p><p> [00:46:01] So it was a really hands-on, really involved process. And I would guess it takes weeks to months for them to get all this data from a vendor. It&#39;s definitely not something you can just get fast and honestly, a potential reason why code is not as good is because way harder to get code data in this regard.</p><p> [00:46:20] So all the task companies are extremely limited in people that know a lot about code. So you get way lower throughput for getting preference labels in code and getting that kind of preference data.</p><h2> [00:46:33] Do we even need human annotators?</h2><p> [00:46:33] <strong>swyx:</strong> That makes a ton of sense. Anyone else have any other commentary, I guess, about the additional data collection? Like what I sense now is that they&#39;re, there&#39;re there&#39;s an inc, there&#39;s a shift away from, I guess the pre-training data sets which are more opaque but also equally well understood towards more of this preference in our HF data.</p><p> [00:46:52] <strong>Alessio Fanelli:</strong> Yeah, they, they spent a lot of time in the supervised fine tuning data too. They actually compare human vendors to some of their models and they were like, yes, we should just use the. Human annotators or like reinforcement learning.</p><p> [00:47:05] <strong>Nathan Lambert:</strong> I&#39;ll tell you what, yeah.</p><p> [00:47:07] <strong>swyx:</strong> The annotators are using the models anyway, right?</p><p> [00:47:09] So it&#39;s just Yeah, exactly.</p><p> [00:47:10] <strong>Nathan Lambert:</strong> Models all the way down.</p><h2> [00:47:12] Models Rating Models</h2><p> [00:47:12] <strong>speaker 1:</strong> II</p><p> [00:47:13] <strong>Alessio Fanelli:</strong> think also the other, I mean, to me, some of these things are like chemy, right? They&#39;re like, we stopped annotating super fast and fine tuning data at 27,540 annotations.为什么？ It&#39;s like, it seems like such a arbitrary number, you know, that I feel like that&#39;s gonna be one of the next research areas, you know, figuring out where the, the right limit is.</p><p> [00:47:35] Do we have maybe, do you know if there&#39;s any really good again, like open source? Open source, like datasets for posts, not pre-training, but like a fine tuning then R lhf. Because I think one of the big moments with Uber pajama was like, okay, we can take the LAMA one data mixture, use all the open source data sets and just run GPUs at them.</p><p> [00:47:55] How do we get to do the same with the post-training flow?</p><p> [00:47:58] <strong>Nathan Lambert:</strong> Okay, so you were breaking up a little bit for the question. So I, I&#39;m gonna say what I think it was, and if it wasn&#39;t, you can jump in and clarify. So I think it&#39;s like, how do we recreate this supervised training data set and like, can we do anything else with it after the fact?</p><p> [00:48:14] Yeah. So Gen, this is another thing that we&#39;ve started doing, and I think that what, so the open source equivalents are something like Open Assistant created a really high quality dataset, artifact, and then the recent trend is for this thing that&#39;s like called Uncensored dataset, which I think is this totally silly name.</p><p> [00:48:34] Because really what they&#39;re doing is they&#39;re removing instructions like as a language model, I don&#39;t wanna say this. And therefore when you remove these things, the model gets more helpful. So that&#39;s just gonna be the new type of data, which is just clean response on instructions with really strong distribution control.</p><p> [00:48:50] And the thing is about recreating this is that it&#39;s. Hard to create a diverse set of tasks. So what they are essentially paying money for is someone to make sure that you&#39;re not getting a whole bunch of the same poems or something. It&#39;s like getting 27,000 weird creative tasks that don&#39;t all overlap with each other is why you have to pay a lot of money for it.</p><p> [00:49:11] Rather than saying, oh, we have 250 people on this call, it&#39;s all due, 10 of them. And then that&#39;s a solid start. Like we would just have a totally misshape in distribution and it wouldn&#39;t be that useful. So I think even in, so you can go look at like instruction, BT and other papers like this have breakdowns of what that instruction data, the supervised, fine tuning data actually looks like.</p><p> [00:49:33] But actually creating it is pretty hard. And I do think that the vendors provide a really high quality amount of data, but their point about the models being able to create it is also really true. So it&#39;s, it&#39;s, it&#39;s pretty borderline right now. And anthropic stop using that in their, in their future work.</p><p> [00:49:50] So like, Philanthropics new base models are just good enough at responding to instructions where they don&#39;t need to do supervised, fine tuning. And that&#39;s like in the constitutional AI paper. So it&#39;s like, I don&#39;t think that&#39;s the place to invest time. It&#39;s much more on the preference side to get the RL HF model and to get these preference models going.</p><p> [00:50:09] So then maybe you can even do creative things like constitutional AI and stuff after that.</p><p> [00:50:13] <strong>Alessio Fanelli:</strong> Yep. So if you wanna do work in open source today, you think you&#39;re better off contributing to this site versus like trying to train another yet another model.</p><p> [00:50:24] <strong>Nathan Lambert:</strong> Yeah. There&#39;s no preference models out there and it&#39;s astonishing to me, especially given that meta&#39;s papers like, oh, we use a ensemble of two preference models.</p><p> [00:50:32] The thing that I wanna see is them do or someone do, is like take a base LAMA model and then also train another preference model that&#39;s for code and then try to do RH F where you like have a prompt flag for all the. All the code questions get rated by their own preference model as well and see what that can do because they already broke it down into like instruction helpfulness and safety.</p><p> [00:50:52] Mm-hmm. It&#39;s like, why can&#39;t we add another one? It it, it&#39;s so obvious that I&#39;m surprised it didn&#39;t, it, it just makes a lot of sense. Seeing it in the paper. I was like,</p><h2> [00:51:02] How to get Code preference data</h2><p> [00:51:02] <strong>swyx:</strong> stoked. Yeah. This, this conversation gave me a bit of an idea for essentially llama stack overflow. Like you, you imagine like Stack overflow with with like sort of llama at, its at its base, but then like, it&#39;s not very good at coding, but we can actually do ratings on like, you know, preference ratings on, on answers and, and, and entire conversation chains.</p><p> [00:51:21] And at, at some point, we&#39;ll, we&#39;ll accumulate the, the code DA dataset that we need to find here in lama. That would probably do it.</p><p> [00:51:27] Yeah,</p><p> [00:51:28] <strong>Nathan Lambert:</strong> we, we&#39;ve like, there&#39;s challenges in base models and how to execute code to get feedback and stuff, but, We&#39;ve seen early experiments and like we&#39;ve worked on one, funnily enough that was called Stack Lama. We like did a, like a nice experimentation of that hugging face and it&#39;s, it&#39;s out there, it&#39;s ready for someone to invest more time in it and do it.</p><p> [00:51:48] I think especially now that Llama2, I&#39;m like, Lama two&#39;s gonna be easier to work with. It&#39;s just better language models are a little bit easier to</p><p> [00:51:56] <strong>swyx:</strong> steer. Absolutely. Alex, you have and Mars catalog you, you just joined and II am sure you have a question. Yeah, go ahead Alex.</p><h2> [00:52:04] Llama 2 Finetuning Ecosystem</h2><p> [00:52:04] <strong>Alex Volkov:</strong> I, I, I just want to complete down what Nathan said.</p><p> [00:52:06] It&#39;s going to be easier to work with because the ton of the ecosystem and the different kind of. Things that the first Lama opened up is now there, right? The G GML is there, all the, for all and, and the Pinocchio browsers, like all different things. How to run like Lama on your laptop already kind of existing.</p><p> [00:52:25] And now we&#39;re just gonna see the commercial folk come in. The, the folks for, for whom working on this actually needs like a dollar sign afterwards. And now they&#39;ll be able to also participate in this. And we&#39;ve seen this already. I, I dunno if you guys talked about this or not scale. AI apparently had early access to this and now released aa, I think open source, like full open source toolkit to fine tune mosaic and which is now Databricks also chime in, but it&#39;s now super simple to fine tune LAMA on their you know, infrastructure.</p><p> [00:52:54] Even though they have the, the TT models, et cetera. They still wanna support LAMA and those Yeah, like the ecosystem exists and I think Nathan&#39;s completely right. It&#39;s gonna be easier to</p><p> [00:53:03] <strong>Nathan Lambert:</strong> use. Easier to find tune. Yeah. Like hugging face. I think every. Library, like all these people at Hugging and Face, were working super hard this weekend to make day zero support for Llama2.</p><p> [00:53:14] Like Transformers, pft, T r L, for like all these people put in the hours to make it&#39;s, it&#39;s there like this week it&#39;s. Like people are doing this now instead of talking on a podcast, they&#39;re fine doing this thing. I&#39;m sure that,</p><p> [00:53:28] <strong>swyx:</strong> For, for what it&#39;s worth I did actually look into the scale thing because I thought that was kind of interesting, their announcement.</p><p> [00:53:33] They never said that they were directly used at Llama2. Perhaps there&#39;s, they&#39;re not allowed to say so. They all, they say scaly, I is proud to be a meta launch partner. We&#39;re launching a platform for customizing lms, blah, blah, blah. And, and obviously, you know, you know, that scale does annotation, so I think it&#39;s just heavily implied.</p><p> [00:53:51] But I don&#39;t think they&#39;re allowed to say,</p><p> [00:53:54] <strong>Simon Willison:</strong> I, I&#39;ve got,</p><p> [00:53:56] <strong>Nathan Lambert:</strong> yeah, surge announced they did the surge device data. At least II think they did more of it too. Go ahead.</p><h2> [00:54:02] Hey Apple: Llama2 on Metal pls</h2><p> [00:54:02] <strong>Simon Willison:</strong> Quick hugging face Transformers question, I really want to run LAMA two on my M two Mac using metal. And so it takes advantage of the GPU integration and the M two.</p><p> [00:54:12] Could somebody please figure out how to do that with hugging face transformers, then publish the world&#39;s most straightforward how to do this document because I have not managed it yet. And I think that would be a huge capacity increase for, for all sorts</p><p> [00:54:24] <strong>swyx:</strong> of people.</p><p> [00:54:24] <strong>Nathan Lambert:</strong> Yeah. Pedro&#39;s at hugging face is working on that. At least integrating these models with Apple directly is fantastic.我同意。我同意。 We agree. There&#39;s</p><p> [00:54:38] <strong>Russell Kaplan:</strong> also a project called llama cpp that hardware accelerates for the M two for the llama one. So I&#39;m sure they&#39;re gonna be updating that for the new models as well,</p><p> [00:54:49] <strong>Simon Willison:</strong> working mean on the cpp.</p><p> [00:54:51] But I&#39;ve, I&#39;ve not seen it run metal yet. I need to, evidently I haven&#39;t checked the reading in the past few weeks.</p><p> [00:54:58] <strong>swyx:</strong> Isn&#39;t it, as long as it&#39;s in G gml, it works, right? Yeah. And those are</p><p> [00:55:01] <strong>Alex Volkov:</strong> the converted models in G GML format. We were able to run one. You guys should split it between CPUs and gpu and I don&#39;t know, in the audience, we LAMA two seven B in G gml and</p><p> [00:55:13] <strong>Nathan Lambert:</strong> run really fast.</p><p> [00:55:15] <strong>Simon Willison:</strong> Fantastic. Yeah. Again, if somebody wants to be really useful, publish a nice detailed step-by-step instructions, they&#39;re getting that working and I will benefit from it and so will load of it. I don&#39;t want to do it myself. I want somebody else to, to figure it out</p><p> [00:55:26] <strong>swyx:</strong> for me.是的。 And, and Simon&#39;s, Simon&#39;s very good at this.</p><p> [00:55:31] You can just kind of copy and paste the, the kind of tutorial quality that he does. That&#39;d be great for all of us. Thank you.</p><p> [00:55:36] I wanna recognize Anton, who is joined. Hey,</p><p> [00:55:39] <strong>Nathan Lambert:</strong> stranger.</p><p> [00:55:40] <strong>Anton Troynikov:</strong> Hey, Swick. How&#39;s it going,</p><p> [00:55:41] <strong>swyx:</strong> man? It&#39;s going well. We&#39;re very excited about open source models. What you got?</p><p> [00:55:46] <strong>Anton Troynikov:</strong> Yeah, I mean, it&#39;s an exciting time, right?</p><h2> [00:55:47] Llama 2 and Chroma</h2><p> [00:55:47] <strong>Anton Troynikov:</strong> I got asked almost immediately, what does this mean for chroma and retrieval and all the other things. We&#39;re in the process of benchmarking and evaluating. To see if it&#39;s actually suitable in the sort of retrieval augmented generation use case. Intuitively we have this idea that lighter weight models want to perform well because you don&#39;t need so many weights for all the facts.</p><p> [00:56:08] You just need them to be reasoning machines. So yeah, we&#39;re excited to be trying that out. We&#39;ll ship results as soon as we have them available.</p><p> [00:56:16] <strong>swyx:</strong> What evals do you look at for models as reasoning machines?</p><p> [00:56:21] <strong>Anton Troynikov:</strong> I mean, there&#39;s plenty of retrieval, augmented generation benchmarks out there. The one that I usually run as a quick test is the SciQ data sets, the multiple choice question answering with distractors and supporting paragraphs.</p><p> [00:56:33] Ah, but there&#39;s, you know, there&#39;s entire batteries of these tests. One of the things that we&#39;re actually looking at doing at chroma very soon, and we&#39;ve been speaking to the AI research labs about this, is nobody&#39;s really got benchmarks that are relevant to production data. The benchmarks that exist are very academically oriented and fairly synthetic.</p><p> [00:56:51] So they consist of, you know, crowdsourced exam, answer question answers. They consist of sort of this really document retrieval oriented thing where it&#39;s like, find a document that&#39;s relevant to this query, but production use cases don&#39;t always look like that. So we&#39;re actually looking at, you know, community sourced benchmarks that, that focus much more on the what, what the real data actually looks like.</p><p> [00:57:15] <strong>swyx:</strong> Yeah, totally. The only one I can think of that is, I guess the most prominent one is the open assistance dataset that is gonna free and clear of any usage restrictions stuff. Yeah, I mean do would you, yeah, I think</p><p> [00:57:27] <strong>Nathan Lambert:</strong> so.</p><p> [00:57:28] <strong>Anton Troynikov:</strong> Usage restrictions, I think, I think for evaluating models, there are very few restrictions for use of these data sets.</p><p> [00:57:36] For benchmarking, it&#39;s very few restrictions for training. There is for sort of commercial purposes, there is, but for the case of like, does this model work well in a retrieval context, there are very few usage restrictions.</p><p> [00:57:48] <strong>Nathan Lambert:</strong> Got it.</p><p> [00:57:49] <strong>swyx:</strong> Amazing. Who else has questions or topics that you wanna bring up about LAMA two and generate?</p><h2> [00:57:55] Open Source MoE model?</h2><p> [00:57:55] <strong>Alessio Fanelli:</strong> One thing that I was thinking about is in the benchmarks they compare to GBT for, but if what George Hotz said on the podcast was right and should be D four is like eight attention heads. I wonder when people are gonna get eight, you know, get a LAMA two mixer expert going and benchmarking that.</p><p> [00:58:12] Maybe it will be better.我不知道。</p><p> [00:58:15] <strong>swyx:</strong> Yes, there, there is a little bit of a playbook that has been published out there, so I mean, it, it takes more skill than I, I have, but I&#39;m sure someone else, else out there is currently working on it. I think that the Chinese universities have, have made some interesting progress there.</p><p> [00:58:28] Yeah, Simon, and then we&#39;ll go to Mars.</p><h2> [00:58:31] Llama 2 using tools</h2><p> [00:58:31] <strong>Simon Willison:</strong> So we talked about the we talked about retrieve augmented generation. The other thing I&#39;m excited about is tool format, right? The the thing where it can call functions, essentially Uhhuh and that&#39;s mentioned in the paper. They mentioned they benchmarked along that, but, but I didn&#39;t get a feel for something that was really good at, the thing I want is I want basically exactly the same APIs, open AI functions, but I want it to run off of Llama2.</p><p> [00:58:53] I think that would be, that would open up all sorts of opportunities.</p><p> [00:58:57] <strong>Nathan Lambert:</strong> They, they said that that capability was emergent and they didn&#39;t train on it. There&#39;s a line in the discussion where it&#39;s like, oh yeah, we got some tool performance where we didn&#39;t train on it. So now we can all go fine tune on it and it should be easier.</p><h2> [00:59:10] Russell Kaplan on Scale AI&#39;s Llama 2 plans</h2><p> [00:59:10] <strong>Anton Troynikov:</strong> We got Russell Kaplan in here from the space, from scale ai. I think we wanna bring him up. I think he&#39;s got a few interesting things to say about how scale is thinking about these things. I know that they were mentioned here before.</p><p> [00:59:20] <strong>swyx:</strong> Hey Russell.</p><p> [00:59:21] <strong>Russell Kaplan:</strong> Here you go. Great. Yeah, no thanks. Thanks Anton. Yeah, we were, we were super stoked about the LAMA two release. Yeah, we put out a, an open source library LM engine for folks to fine tune and serve LAMA two and other language models whether hosted by scale or, or on their own infrastructure.</p><p> [00:59:37] And I think generally at scale we&#39;re looking to start doing a lot more open source stuff. So you know, one of the next things we&#39;re gonna be doing is starting to fine tune LAMA two on interesting domain specific data sets that we create, or, or problem domain. So Anton you mentioned not sure how well it&#39;s working for retrieval.</p><p> [00:59:55] You know, we&#39;d love to just like put together a data set that we could use to fine tune these models to be good at retrieval. I think we have one planned out for SQL right now. Potentially other tool use. So yeah, I&#39;d be really curious, you know, hearing from the audience. If there are sort of requests for, for good fine tunes of LAMA two or if anyone, you know, already has that data, you can just clone our repo LM engine and and try it out.</p><p> [01:00:17] <strong>Simon Willison:</strong> So I&#39;ve got one for you. I want a clone of chat GP PT code interpreter built on top of LAMA two, which I imagine would require quite extensive fine tuning. But my good, I mean we&#39;ve talked about this recently, how chapter code interpreter really is a next level AI tool. Being able to run our own version of that against LAMA two would be incredible.</p><p> [01:00:35] Yeah, that would be, that would be great.</p><p> [01:00:36] <strong>Russell Kaplan:</strong> I, yeah, we do, we do, we do a lot of code sort of data acquisition right now, so I think that&#39;s definitely in the wheelhouse. But yeah, that&#39;s a, that&#39;s a good idea to,</p><p> [01:00:45] <strong>Anton Troynikov:</strong> to try out.</p><p> [01:00:45] Code data acquisition sounds so sinister. Russell,</p><p> [01:00:49] <strong>Russell Kaplan:</strong> You know, it takes, you gotta, you gotta write a lot of code. Write a</p><p> [01:00:52] <strong>Matt Bornstein:</strong> lot of code. Yeah.</p><p> [01:00:53] <strong>Russell Kaplan:</strong> I think we have something like 350,000 people all around the world who are sort of helping with this stuff. And within that there&#39;s, you know, a lot of domain specific expertise.</p><h2> [01:01:01] Scale annotating code?</h2><p> [01:01:01] <strong>swyx:</strong> Is there a way that like, so we were talking before you joined about scale acquiring, I guess preference data from developers rather than I guess the, the standard annotators that you have. Is this a, is this a, is this a need or focus that you have? Is there a way that we can help or Yeah. How do we crowdsource this?</p><p> [01:01:18] Yeah, no,</p><p> [01:01:19] <strong>Russell Kaplan:</strong> definitely. No. So, so one of the interesting things has just been for, for our business where, you know, we do a lot of the R LH f labeling for, for all the companies training these foundation models has just been that the level of expertise required has gone up tremendously. Right? So we have a lot of our crowd now it&#39;s, it&#39;s really domain experts in.</p><p> [01:01:38] Specific areas, whether it&#39;s programming in a particular language or people who have, you know, passed the CPA or people who have passed the bar or licensed in some profession. That&#39;s really been where a lot of our sort of growth has been. And so, yeah, I mean, if anyone is a programmer and wants to kind of infuse their knowledge into the AI, that will power the rest of our, of our society increasingly over time.</p><p> [01:02:01] You can, you can just go to scale.com and and sign up to, to start help help</p><p> [01:02:04] <strong>Nathan Lambert:</strong> programming.</p><h2> [01:02:06] Immortality</h2><p> [01:02:06] <strong>Anton Troynikov:</strong> Another, another benefit of this is by the time we have ais strong enough to simulate entire human beings, your data will already be in them. So you&#39;ll be resurrected and</p><p> [01:02:15] <strong>Nathan Lambert:</strong> get to live forever in the afterlife.</p><p> [01:02:18] <strong>swyx:</strong> Indeed, we are the first immorals. It&#39;s the way to achieve immortality. Yeah. You know, immortality take it. It&#39;s yours, but it&#39;s not on the battlefield. It&#39;s editing Wikipedia. That is that is immortality.</p><h2> [01:02:29] Running Llama on your phone</h2><p> [01:02:29] <strong>swyx:</strong> Mars, you had your hand up. Hey, really</p><p> [01:02:31] <strong>Whole Mars Catalog:</strong> been enjoying listening to this conversation. I think it&#39;s such an exciting day with LAMA two and the commercial license.</p><p> [01:02:39] One of the things that I&#39;ve really been excited about, and I think Qualcomm made an announcement with Meta and they said they&#39;re going to be looking at optimizing it for Snapdragon hardware, accelerating it. I think one of the most interesting things about these open source models, especially now that you have a commercial license, is actually running it on your laptop or even your smartphone.</p><p> [01:03:03] You know, maybe the 7 billion parameter model and the kind of use cases that opened up, that opens up that, you know, just weren&#39;t there a few months ago. I was wondering if people had any thoughts on that and what we might see in that area.</p><p> [01:03:17] <strong>Nathan Lambert:</strong> Meta just gave Tipco a huge softball for Apple to fix Siri, and they still hate each other.</p><p> [01:03:26] <strong>Simon Willison:</strong> So I&#39;ve been running the Qna seven B on my iPhone for a couple of months, just as a, mainly as a demo. So I could just shove it people&#39;s face and go Look, my phone&#39;s offline. And it&#39;s still writing me terrible poetry. And I have to admit, it&#39;s fun. I&#39;ve not yet found use cases for that quality of model for, for when I&#39;m offline.</p><p> [01:03:44] And maybe I&#39;m just not being imaginative enough. My, my hunch is that models that are smaller like that, that can run on your phone are much more interesting if you combine them with retrieval, augmented generation or, or tool use. So on. And just as a, a plain sort of chatty PT style language model, I&#39;ve not yet found many practical uses for it.</p><p> [01:04:02] I&#39;d love to hear from people. Oh, that&#39;s not true. I use it for brainstorming occasionally if I want to come up with a name for something that&#39;s like I used to dread naming things. Now I, I&#39;m fine with naming things cause I get a language model to brainstorm for me. But one on my phone is good enough to do that.</p><p> [01:04:16] I&#39;ve had it come up with some names for things for me so far.</p><p> [01:04:18] <strong>Nathan Lambert:</strong> We talked about evaluation a lot. I&#39;ve used it for naming and I&#39;ve also used these models to kind of generate evaluation prompts, which is kind of a different way to do it. It&#39;s like come up with some hard python coding questions where you put a bug in this type of function and like, I&#39;m not gonna come up with that on my own.</p><p> [01:04:36] Yeah, it can be a really</p><p> [01:04:37] <strong>swyx:</strong> useful spot check, I guess, or I dunno, men mental augmentation tool, whatever</p><p> [01:04:43] <strong>Nathan Lambert:</strong> we call that.</p><h2> [01:04:44] Sama &lt;3 Satya &lt;3 Zuck? &quot;Azure as Launch Partner&quot;</h2><p> [01:04:44] <strong>Anton Troynikov:</strong> So can we, can we take a minute to do some kremlinology here? What&#39;s the deal with like, friendship ended with Sam Altman? Now Mark Zuckerberg is my best friend with Satya. I wanna, I wanna get into that</p><p> [01:04:55] <strong>Alessio Fanelli:</strong> side.</p><p> [01:04:56] I was smiling a lot more in this picture with Mark than with Sam. That&#39;s what I noted. But wait, there&#39;s</p><p> [01:05:01] <strong>swyx:</strong> the picture.什么？</p><p> [01:05:03] <strong>Alessio Fanelli:</strong> Satya posted a photo with, with Mark and he was like just laughing away. And then I looked back at the one that, remember the one you posted, Satya and Sam together, and I think the bill conference or something with</p><p> [01:05:15] <strong>Anton Troynikov:</strong> Satya, Satya, Sam, and Sam&#39;s nipples.</p><p> [01:05:17] <strong>Simon Willison:</strong> Yes.</p><p> [01:05:19] <strong>Alessio Fanelli:</strong> And say Satya was not smiling as much.我不知道。 But I, I really wonder what that does to the, you know, open AI does have to pay back a lot of money to Microsoft stuff. It&#39;s</p><p> [01:05:29] <strong>Anton Troynikov:</strong> kinda, it&#39;s kinda crazy that that a Azure is the launch partner cuz Open AI is exclusively running on Azure, Azure hardware.</p><p> [01:05:36] This is a very, very curious move. Right. And I, I can&#39;t really disentangle it. Given sort of the scope of Microsoft&#39;s investment in OpenAI is entirely in Azure credits. Like one interpretation of this move is that they&#39;ve already got OpenAI locked in. Right. They&#39;re not going anywhere. So might as well get the other, you know, contending models, right?</p><p> [01:06:02] If, if you&#39;re, if you&#39;re Satya, how are you thinking? The only thing that we know for sure at cruise value in this environment is owning compute, and that&#39;s what Microsoft</p><p> [01:06:11] <strong>swyx:</strong> has.是的。 But AWS is also a launch partner, right? What does it mean to be a launch partner of an open source model? Like if you can run compute, you can, you can run it.</p><p> [01:06:20] <strong>Alessio Fanelli:</strong> I think that&#39;s the, that&#39;s the main, the main question. Yeah. But I think like Microsoft is clearly, you know, happy to be involved. To them, it&#39;s like a yes. Their first equals exclusivity just one way, you know, it&#39;s not a two way exclusivity, so they don&#39;t, that&#39;s whatever. The other thing is</p><p> [01:06:35] <strong>speaker 1:</strong> this, this will probably increase the demand, the compute demand on Azure from all of their enterprise customers, right?</p><p> [01:06:41] So, you know, whether they&#39;re selling compute to OpenAI or all of the other enterprises they work with. Having more models available that, that everyone&#39;s using should, should just kinda</p><p> [01:06:50] <strong>Matt Bornstein:</strong> keep growing that business. Not to mention, I</p><p> [01:06:52] <strong>Russell Kaplan:</strong> think a lot of their Azure customers probably have significant concerns about privacy, about putting sensitive business data through this and being able to just run inference on your own hardware that you control probably is more appealing to them in some cases than running REST API and calling out to open AI&#39;s infrastructure.</p><p> [01:07:11] Azure?</p><p> [01:07:12] <strong>Anton Troynikov:</strong> Well, they&#39;ve got, they&#39;ve got Azure endpoints for the open AI models. I&#39;m, I&#39;m not that, I&#39;m actually not quite up to speed with the privacy model there, but my understanding is there&#39;s not really much difference.</p><p> [01:07:25] <strong>Simon Willison:</strong> My hunch is that it doesn&#39;t matter if it is what? What matters is, is what people feel.</p><p> [01:07:29] It&#39;s the vibes. And you see so many of these, so many people, so many companies saying, no, absolutely no way we would pipe pump any of our private data through somebody else&#39;s model. Even if they say they won&#39;t use it for training, which they all do, but whereas I guess maybe they&#39;re okay with pumping it through as through Microsoft as you, but at least it&#39;s on our own, like GPU reserved instances.</p><p> [01:07:51] Maybe that&#39;s what&#39;s going on here. There&#39;s so much paranoia around this space at the moment. Yeah, a lot of the</p><p> [01:07:55] <strong>Russell Kaplan:</strong> details come down to can you run it within your own virtual private cloud? I, I wish, I wish we could close enterprise customer security requirements on the vibes, but at least in my experience at at scale people do, you know, there there&#39;s some compliance function somewhere in the organization that has to sort of check the boxes that you&#39;re not, you know, gonna get screwed on later.</p><p> [01:08:15] And so that&#39;s definitely been one of the big drivers of people looking to self-host their own open source LMS more and more.</p><p> [01:08:25] <strong>Alessio Fanelli:</strong> Yeah. And the other thing is that they did not use any Azure compute to actually train the model. So if you go in the paper it mentions they only use their super cluster and their internal production cluster.</p><p> [01:08:35] So no Azure we use to train it. I guess it&#39;s just the inference partner. Yeah, so I mean, going back to the point of they just want GPUs to run. It&#39;s not about this is the best GPUs that we use. They didn&#39;t even use it.</p><h2> [01:08:48] Meta &quot;Open Source&quot; Leadership</h2><p> [01:08:48] <strong>Matt Bornstein:</strong> I think what&#39;s really interesting</p><p> [01:08:49] <strong>speaker 1:</strong> about, about this release is that, you know, for, for a while people have been talking about how oh, is meta behind in, in ai, generative AI and language models. And, and you know, I think Roone had a tweet that was like, the best open source model sounds a lot better than the fifth best language model.</p><p> [01:09:06] And it&#39;s actually totally true. And, and I actually think that that companies, you know, if you are behind, if you&#39;re not in first place, if you, if you open source stuff and you just sort of get the community using it you can, you can get a lot of goodwill,</p><p> [01:09:18] <strong>Nathan Lambert:</strong> get a lot of adoption and actually really move</p><p> [01:09:20] <strong>speaker 1:</strong> the industry forward.</p><p> [01:09:21] So yeah, really cool to see Meta sort of put this out and I think, I think it will also spur a lot more open source from a lot</p><p> [01:09:28] <strong>swyx:</strong> of other companies.</p><p> [01:09:28] I fully agree. I think, I think this is something that we&#39;ve been very excited about. We heard, we heard some bes about it a couple months ago and then you know earlier this week or I guess last week and now, now it&#39;s fully out. Okay. Maybe I&#39;ll do just a round for predictions.</p><p> [01:09:43] What happens next in open source models over with Lama.</p><h2> [01:09:46] Prediction: Finetuning =>; New Use Cases from Internal State</h2><p> [01:09:46] <strong>Nathan Lambert:</strong> I&#39;ll go first. I&#39;ll</p><p> [01:09:47] go</p><p> [01:09:47] <strong>Anton Troynikov:</strong> first. I think the first thing that needs to happen here is the community will actually get the model into its hands and find out its true capabilities. Benchmarks only take us so far. Once that has happened, we&#39;re gonna see an extensive sort of period of fine tuning where people are going to apply it to their particular applications and, you know, keep, keep pushing the envelope here and then if it is sufficiently capable, I actually think that we might find new uses for these models that we don&#39;t find in rest APIs served ones because you can get at the internal state.</p><p> [01:10:16] Right. The thing that I&#39;m always thinking about obviously is embeddings and internal states and, and like modifications here. And I think that there&#39;s actually a great deal of interesting research and engineering to be done by looking into what&#39;s happening in these models live, especially a sufficiently capable one, which we can do reasoning.</p><p> [01:10:32] And so I&#39;m particularly excited about that. I&#39;m particularly excited about having something at least sufficiently capable that we can start to reason about because the entire research community has access to it rather than, you know, behind a closed wall inside some of the</p><p> [01:10:45] <strong>Nathan Lambert:</strong> bigger AI labs.</p><p> [01:10:47] <strong>swyx:</strong> Anyone else? Simon Nathan?</p><p> [01:10:48] <strong>Nathan Lambert:</strong> Yeah, I, I would mostly just double down on that and I could comment on how remarkable the collapse of kind of NLP research as it was, has been onto open AI APIs.</p><p> [01:11:01] And this is an opportunity to reset some of that dynamic where so much academic work, which is fine tuning open AI models. And I was like, oh, sorry, we nuked all your fine tuned models and things like that. Like from a values perspective, this is huge for research to kind of proceed as it was meant to be in a way.</p><p> [01:11:23] And that is wonderful.</p><h2> [01:11:24] Prediction: Llama Toolformer</h2><p> [01:11:24] <strong>Simon Willison:</strong> I&#39;m looking forward to the first fine tunes. I think like alpaca is what unlocked llama. I can&#39;t wait to see what people do, especially since everyone&#39;s already amped up and ready to go. So I think it&#39;ll be fascinating to see what the, how those start shaping up the next few days, few weeks.</p><p> [01:11:38] And yeah, I want to see people, I want to see the applications. I want to see people figure out retrieve augmented generation. I want to see people figure out if it can do to tool format, all of those things, especially the tricks which make the sort of smaller the seven B models able to do, solve interesting problems.</p><p> [01:11:53] And I think this is gonna happen really quickly. You know, we&#39;ve got so many more people who know how to work with these models today than we did when Lama came out back at the end of February. So I&#39;m expecting that to just be a whirlwind of activity starting about four hours ago. And yeah, I can&#39;t wait to see what happens.</p><h2> [01:12:09] Prediction: Finetune-for-everything</h2><p> [01:12:09] <strong>Simon Willison:</strong> I, I totally</p><p> [01:12:10] <strong>Russell Kaplan:</strong> agree. I think, I think there&#39;s gonna be an explosion. Of domain specific and use case specific fine tunes. And I think that, you know, the sort of first order effects are gonna be pretty clear on, you know, this different industry, this different domain. Everyone is gonna start putting out these domain specific fine tunes, not just the companies themselves doing it for their own use case, but you know, they&#39;re like, as someone said, like alpaca sort of made llama to or made llama accessible.</p><p> [01:12:36] We&#39;ll have something really similar, but for each category of application. And then I think the second order effect that&#39;s really interesting to me is I think tool use and agents are gonna get really good. Right now. People are using, you know, sort of off the shelf untuned language models to try to build agents have them use tools.</p><p> [01:12:57] But if you, if you&#39;re building a, you know, an application and you just need to use that one tool really, really well, Now you have suddenly a GPT 3.5 class model that you can fine tune exclusively to that tool. It&#39;s gonna work really well. And I think that the, you know, the barrier to, to utility is so high for these tool use real world applications because of this sort of problem of exponential compounding of errors over long chains.</p><p> [01:13:24] But if fine tuning works well for that, I think it&#39;s gonna be a really big game changer.</p><h2> [01:13:30] Predictions: Llama Agents</h2><p> [01:13:30] <strong>Anton Troynikov:</strong> I am so bullish on agents, like I&#39;m well aware that they&#39;re nothing but toys today. Although I can think of a couple of practical use cases, including in the fine tuning context. Russell, we ought to talk about this actually later, but that&#39;s a really good point to my mind that sort of having an easy to find train model for your particular agent use case is maybe going to make these things more useful than they are today.</p><p> [01:13:51] I&#39;m, I&#39;m very bullish on that. I&#39;m hopeful and of course cuz Koma builds memory for agents. It would be great for us to.</p><p> [01:13:57] <strong>swyx:</strong> All right. I think unless you dunno if you have any predictions. I, I, I think I&#39;m kind of out. You guys are definitely taking all the ones that I was gonna say. Yeah.</p><h2> [01:14:05] dP(Doom)?</h2><p> [01:14:05] <strong>Nathan Lambert:</strong> Wait, wait, wait,</p><p> [01:14:05] <strong>Anton Troynikov:</strong> wait, wait. Before, before we sign off here, let&#39;s go around the, let&#39;s go around the room. Probability of AI doom improved or made worse by the release of LA material.</p><p> [01:14:14] <strong>Nathan Lambert:</strong> Let&#39;s go.</p><p> [01:14:15] <strong>Simon Willison:</strong> I couldn&#39;t care less. I don&#39;t care about the doom scenarios. I care about building stuff with, with what we&#39;ve got.</p><p> [01:14:22] <strong>Nathan Lambert:</strong> So,</p><p> [01:14:22] <strong>Anton Troynikov:</strong> so none, it has not moved</p><p> [01:14:24] <strong>Nathan Lambert:</strong> your needle. No.</p><p> [01:14:25] <strong>Simon Willison:</strong> My, my needle is, is stuck on the sort of metal, maybe 5%, but, but not worth thinking about. Too hard.</p><p> [01:14:31] <strong>Anton Troynikov:</strong> All right. Five, 5% doom. I&#39;m, I&#39;m willing to accept 5% doom.</p><p> [01:14:36] We&#39;ve, we&#39;ve, we&#39;ve accepted way more percent doom than other technologies.</p><p> [01:14:39] <strong>Alessio Fanelli:</strong> I&#39;m an old DOM, so it&#39;s we&#39;re, we&#39;re gonna use it for more good than bad. We&#39;ll be done with it.</p><p> [01:14:45] <strong>Speaker 2:</strong> I would like to believe that having a model that we can actually understand and like go deep and develop on top of it, will not only advert the DOMA scenarios, but will allow us to prepare better in case any crazy person wants to make doom on their own. A sufficient enough community of builders of LLMs and ais</p><p> [01:15:10] <strong>Matt Bornstein:</strong> can stop that.</p><p> [01:15:12] Yeah, I think that&#39;s a really</p><p> [01:15:13] <strong>Anton Troynikov:</strong> great point actually. The safety story gets better when we have more opportunities to work with the core internals of the models as they actually exist instead of hypothetical abstract objects that we reason about.</p><p> [01:15:27] <strong>swyx:</strong> Yeah, I was</p><p> [01:15:27] <strong>speaker 1:</strong> gonna say</p><p> [01:15:28] <strong>swyx:</strong> like, I&#39;m a pretty high P doom person, but it, it&#39;s moved down because we can have, you know, GC five or LAMA three, you know, explain the weights of LAMA two.</p><p> [01:15:37] And I, I do think that that improves interpretability quite a bit. How</p><p> [01:15:42] <strong>Nathan Lambert:</strong> are you going to know if it&#39;s telling the</p><p> [01:15:43] <strong>Anton Troynikov:</strong> truth? I like, I, I know that you, I know about these, just ask the model approaches, but I&#39;m pretty skeptical.</p><p> [01:15:49] <strong>Nathan Lambert:</strong> I&#39;ve gotta tell ya.</p><p> [01:15:51] <strong>swyx:</strong> Give it a GoBoard you know, swap out one of the positions, see what happens, you know, that kinda stuff.</p><p> [01:15:55] You know, we, we&#39;ve done small versions of this. We&#39;ve done, we&#39;ve done very, very small skills version of this already, right. Like, so, I dunno,</p><p> [01:16:01] <strong>Nathan Lambert:</strong> this</p><p> [01:16:01] <strong>swyx:</strong> is hand wavy. I mean, you</p><p> [01:16:02] <strong>Nathan Lambert:</strong> know. No, I&#39;m,</p><p> [01:16:03] <strong>Anton Troynikov:</strong> I&#39;m just, I&#39;m just genuinely curious about the ideas here, but that&#39;s, that&#39;s a different discussion. Exactly. Yeah. Yeah.</p><p> [01:16:09] <strong>Russell Kaplan:</strong> Yeah, I just think it&#39;s amazing how these language model capabilities that just a few months ago felt cutting edge when people used them for the first time in chat. GBT have now progressed to a state where it&#39;s almost becoming commodified and everybody&#39;s having these models.</p><p> [01:16:27] There&#39;s more and more of them popping up, people starting things and open source models exploding. I don&#39;t think necessarily we can fully understand the significance of what&#39;s happening here today, but going into the future, it&#39;s probably going to be really common for pretty much every computer to be running large language models natively on the device.</p><h2> [01:16:51] Wrapping up</h2><p> [01:16:51] <strong>swyx:</strong> All right. Well, that&#39;s a very positive view of the future. I think we&#39;re all very encouraged by that. Yeah. I would just want to thank everyone for joining and sharing your thoughts on LAMA two. Alessio. Did you have parting</p><p> [01:17:01] <strong>Alessio Fanelli:</strong> thoughts? No, that was it. Thank you everyone.</p><p> [01:17:05] <strong>swyx:</strong> Thank you so much. We&#39;ll clean up the audio of this thing and post it tomorrow on the in space, but otherwise, I think we should follow what Russell and, and Nathan and the others have been saying, which is go play with Llama2.</p><p> [01:17:14] So I guess we&#39;ll all go do that. Have a wonderful day everyone. Thanks everyone. Thank you sir. Alex. Thanks everyone. Bye bye. Have a</p><p> [01:17:23] <strong>Speaker 2:</strong> great time.</p><div class="footnote" data-component-name="FootnoteToDOM"> <a id="footnote-1" href="#footnote-anchor-1" class="footnote-number" contenteditable="false">1</a><div class="footnote-content"><p> And <a href="https://twitter.com/tszzl/status/1681392737969672192?s=20">5th best model overall</a> , which is still impressive?</p></div></div> ]]>;</content:encoded></item></channel></rss>