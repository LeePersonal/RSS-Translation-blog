<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[Huggingface Daily Papers]]></title><link/> https://huggingface.co/papers <atom:link href="https://rsshub-yuee.vercel.app/huggingface/daily-papers?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh-cn</language><lastbuilddate> 2023 年 8 月 15 日星期二 09:33:39 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs]]></title><description><![CDATA[Blind face restoration aims at recovering high-quality face images from those with unknown degradations. Current algorithms mainly introduce priors to complement high-quality details and achieve impressive progress. However, most of these algorithms ignore abundant contextual information in the face and its interplay with the priors, leading to sub-optimal performance. Moreover, they pay less attention to the gap between the synthetic and real-world scenarios, limiting the robustness and generalization to real-world applications. In this work, we propose RestoreFormer++, which on the one hand introduces fully-spatial attention mechanisms to model the contextual information and the interplay with the priors, and on the other hand, explores an extending degrading model to help generate more realistic degraded face images to alleviate the synthetic-to-real-world gap. Compared with current algorithms, RestoreFormer++ has several crucial benefits. First, instead of using a multi-head self-attention mechanism like the traditional visual transformer, we introduce multi-head cross-attention over multi-scale features to fully explore spatial interactions between corrupted information and high-quality priors. In this way, it can facilitate RestoreFormer++ to restore face images with higher realness and fidelity. Second, in contrast to the recognition-oriented dictionary, we learn a reconstruction-oriented dictionary as priors, which contains more diverse high-quality facial details and better accords with the restoration target. Third, we introduce an extending degrading model that contains more realistic degraded scenarios for training data synthesizing, and thus helps to enhance the robustness and generalization of our RestoreFormer++ model. Extensive experiments show that RestoreFormer++ outperforms state-of-the-art algorithms on both synthetic and real-world datasets.]]></description><pubDate> Tue, 15 Aug 2023 07:36:18 GMT</pubDate><guid ispermalink="false"> https://www.archiv.org/abs/2308.07228</guid><link/> https://www.archiv.org/abs/2308.07228 <author><![CDATA[Zhouxia Wang, Jiawei Zhang, Tianshui Chen, Wenping Wang, Ping Luo]]></author></item><item><title><![CDATA[Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation]]></title><description><![CDATA[With a strong understanding of the target domain from natural language, we produce promising results in translating across large domain gaps and bringing skeletons back to life. In this work, we use text-guided latent diffusion models for zero-shot image-to-image translation (I2I) across large domain gaps (longI2I), where large amounts of new visual features and new geometry need to be generated to enter the target domain. Being able to perform translations across large domain gaps has a wide variety of real-world applications in criminology, astrology, environmental conservation, and paleontology. In this work, we introduce a new task Skull2Animal for translating between skulls and living animals. On this task, we find that unguided Generative Adversarial Networks (GANs) are not capable of translating across large domain gaps. Instead of these traditional I2I methods, we explore the use of guided diffusion and image editing models and provide a new benchmark model, Revive-2I, capable of performing zero-shot I2I via text-prompting latent diffusion models. We find that guidance is necessary for longI2I because, to bridge the large domain gap, prior knowledge about the target domain is needed. In addition, we find that prompting provides the best and most scalable information about the target domain as classifier-guided diffusion models require retraining for specific use cases and lack stronger constraints on the target domain because of the wide variety of images they are trained on.]]></description><pubDate> Tue, 15 Aug 2023 07:05:43 GMT</pubDate><guid ispermalink="false"> https://archiv.org/abs/2308.07316</guid><link/> https://archiv.org/abs/2308.07316 <author><![CDATA[Alexander Martin, Haitian Zheng, Jie An, Jiebo Luo]]></author></item></channel></rss>