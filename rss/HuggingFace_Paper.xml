<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title><![CDATA[Huggingface Daily Papers]]></title><link/> https://huggingface.co/papers <atom:link href="https://rsshub-yuee.vercel.app/huggingface/daily-papers?key=yuezhu" rel="self" type="application/rss+xml"></atom:link><description><![CDATA[Huggingface Daily Papers - Made with love by RSSHub(https://github.com/DIYgod/RSSHub)]]></description><generator> RS集线器</generator><webmaster>i@diygod.me (DIYgod)</webmaster><language> zh-cn</language><lastbuilddate> 2023 年 8 月 15 日星期二 07:22:31 GMT</lastbuilddate><ttl> 10 </ttl><item><title><![CDATA[Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation]]></title><description><![CDATA[With a strong understanding of the target domain from natural language, we produce promising results in translating across large domain gaps and bringing skeletons back to life. In this work, we use text-guided latent diffusion models for zero-shot image-to-image translation (I2I) across large domain gaps (longI2I), where large amounts of new visual features and new geometry need to be generated to enter the target domain. Being able to perform translations across large domain gaps has a wide variety of real-world applications in criminology, astrology, environmental conservation, and paleontology. In this work, we introduce a new task Skull2Animal for translating between skulls and living animals. On this task, we find that unguided Generative Adversarial Networks (GANs) are not capable of translating across large domain gaps. Instead of these traditional I2I methods, we explore the use of guided diffusion and image editing models and provide a new benchmark model, Revive-2I, capable of performing zero-shot I2I via text-prompting latent diffusion models. We find that guidance is necessary for longI2I because, to bridge the large domain gap, prior knowledge about the target domain is needed. In addition, we find that prompting provides the best and most scalable information about the target domain as classifier-guided diffusion models require retraining for specific use cases and lack stronger constraints on the target domain because of the wide variety of images they are trained on.]]></description><pubDate> Tue, 15 Aug 2023 07:05:43 GMT</pubDate><guid ispermalink="false"> https://archiv.org/abs/2308.07316</guid><link/> https://archiv.org/abs/2308.07316<author><![CDATA[Alexander Martin, Haitian Zheng, Jie An, Jiebo Luo]]></author></item><item><title><![CDATA[IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models]]></title><description><![CDATA[Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: "an image is worth a thousand words". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at \url{https://ip-adapter.github.io}.]]></description><pubDate> Tue, 15 Aug 2023 06:48:59 GMT</pubDate><guid ispermalink="false"> https://www.archiv.org/abs/2308.06721</guid><link/> https://www.archiv.org/abs/2308.06721<author><![CDATA[Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang]]></author></item></channel></rss>